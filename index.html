<!DOCTYPE html>
<html>
<head>
<title>Paper collected by Wang</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<style type="text/css">


/* BODY
=============================================================================*/

body {
  font-family: Helvetica, arial, freesans, clean, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  color: #333;
  background-color: #fff;
  padding: 20px;
  max-width: 960px;
  margin: 0 auto;
}

span#pid {
  color:red;
  
}
span#filename{
  font-style: oblique;
}

span#title{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: italic;
  font-size: 20px;
  border:1px solid #B50;
}
span#abs{
  font-family: Times New Roman, freesans, clean, sans-serif;
  font-style: oblique;
  font-size: 18px;
}
</style>
</head>
<body>

</p></br></br><div id='section'>Paperid: <span id='pid'>1, <a href='https://arxiv.org/pdf/2512.16904.pdf' target='_blank'>https://arxiv.org/pdf/2512.16904.pdf</a></span>   <span><a href='https://github.com/facebookresearch/textseal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Fernandez, Tom Sander, Hady Elsahar, Hongyan Chang, Tomáš Souček, Valeriu Lacatusu, Tuan Tran, Sylvestre-Alvise Rebuffi, Alexandre Mourachko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16904">How Good is Post-Hoc Watermarking With Language Model Rephrasing?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generation-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore *post-hoc watermarking* where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.
<div id='section'>Paperid: <span id='pid'>2, <a href='https://arxiv.org/pdf/2512.16874.pdf' target='_blank'>https://arxiv.org/pdf/2512.16874.pdf</a></span>   <span><a href='https://github.com/facebookresearch/videoseal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomáš Souček, Pierre Fernandez, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Tom Sander, Alexandre Mourachko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16874">Pixel Seal: Adversarial-only training for invisible image and video watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.
<div id='section'>Paperid: <span id='pid'>3, <a href='https://arxiv.org/pdf/2512.10600.pdf' target='_blank'>https://arxiv.org/pdf/2512.10600.pdf</a></span>   <span><a href='https://github.com/PlayerYangh/Authority-Trigger' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Han Yang, Shaofeng Li, Tian Dong, Xiangyu Xu, Guangchi Liu, Zhen Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10600">Authority Backdoor: A Certifiable Backdoor Mechanism for Authoring DNNs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Neural Networks (DNNs), as valuable intellectual property, face unauthorized use. Existing protections, such as digital watermarking, are largely passive; they provide only post-hoc ownership verification and cannot actively prevent the illicit use of a stolen model. This work proposes a proactive protection scheme, dubbed ``Authority Backdoor," which embeds access constraints directly into the model. In particular, the scheme utilizes a backdoor learning framework to intrinsically lock a model's utility, such that it performs normally only in the presence of a specific trigger (e.g., a hardware fingerprint). But in its absence, the DNN's performance degrades to be useless. To further enhance the security of the proposed authority scheme, the certifiable robustness is integrated to prevent an adaptive attacker from removing the implanted backdoor. The resulting framework establishes a secure authority mechanism for DNNs, combining access control with certifiable robustness against adversarial attacks. Extensive experiments on diverse architectures and datasets validate the effectiveness and certifiable robustness of the proposed framework.
<div id='section'>Paperid: <span id='pid'>4, <a href='https://arxiv.org/pdf/2512.00837.pdf' target='_blank'>https://arxiv.org/pdf/2512.00837.pdf</a></span>   <span><a href='https://github.com/Yukang-Lin/WaterSearch' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/Yukang-Lin/WaterSearch' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yukang Lin, Jiahao Shao, Shuoran Jiang, Wentao Zhu, Bingjie Lu, Xiangping Wu, Joanna Siebert, Qingcai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00837">WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking acts as a critical safeguard in text generated by Large Language Models (LLMs). By embedding identifiable signals into model outputs, watermarking enables reliable attribution and enhances the security of machine-generated content. Existing approaches typically embed signals by manipulating token generation probabilities. Despite their effectiveness, these methods inherently face a trade-off between detectability and text quality: the signal strength and randomness required for robust watermarking tend to degrade the performance of downstream tasks. In this paper, we design a novel embedding scheme that controls seed pools to facilitate diverse parallel generation of watermarked text. Based on that scheme, we propose WaterSearch, a sentence-level, search-based watermarking framework adaptable to a wide range of existing methods. WaterSearch enhances text quality by jointly optimizing two key aspects: 1) distribution fidelity and 2) watermark signal characteristics. Furthermore, WaterSearch is complemented by a sentence-level detection method with strong attack robustness. We evaluate our method on three popular LLMs across ten diverse tasks. Extensive experiments demonstrate that our method achieves an average performance improvement of 51.01\% over state-of-the-art baselines at a watermark detectability strength of 95\%. In challenging scenarios such as short text generation and low-entropy output generation, our method yields performance gains of 47.78\% and 36.47\%, respectively. Moreover, under different attack senarios including insertion, synonym substitution and paraphrase attasks, WaterSearch maintains high detectability, further validating its robust anti-attack capabilities. Our code is available at \href{https://github.com/Yukang-Lin/WaterSearch}{https://github.com/Yukang-Lin/WaterSearch}.
<div id='section'>Paperid: <span id='pid'>5, <a href='https://arxiv.org/pdf/2511.22119.pdf' target='_blank'>https://arxiv.org/pdf/2511.22119.pdf</a></span>   <span><a href='https://github.com/aaFrostnova/PromptMiner' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingzhe Li, Renhao Zhang, Zhiyang Wen, Siqi Pan, Bruno Castro da Silva, Juan Zhai, Shiqing Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22119">PROMPTMINER: Black-Box Prompt Stealing against Text-to-Image Generative Models via Reinforcement Learning and Fuzz Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image (T2I) generative models such as Stable Diffusion and FLUX can synthesize realistic, high-quality images directly from textual prompts. The resulting image quality depends critically on well-crafted prompts that specify both subjects and stylistic modifiers, which have become valuable digital assets. However, the rising value and ubiquity of high-quality prompts expose them to security and intellectual-property risks. One key threat is the prompt stealing attack, i.e., the task of recovering the textual prompt that generated a given image. Prompt stealing enables unauthorized extraction and reuse of carefully engineered prompts, yet it can also support beneficial applications such as data attribution, model provenance analysis, and watermarking validation. Existing approaches often assume white-box gradient access, require large-scale labeled datasets for supervised training, or rely solely on captioning without explicit optimization, limiting their practicality and adaptability. To address these challenges, we propose PROMPTMINER, a black-box prompt stealing framework that decouples the task into two phases: (1) a reinforcement learning-based optimization phase to reconstruct the primary subject, and (2) a fuzzing-driven search phase to recover stylistic modifiers. Experiments across multiple datasets and diffusion backbones demonstrate that PROMPTMINER achieves superior results, with CLIP similarity up to 0.958 and textual alignment with SBERT up to 0.751, surpassing all baselines. Even when applied to in-the-wild images with unknown generators, it outperforms the strongest baseline by 7.5 percent in CLIP similarity, demonstrating better generalization. Finally, PROMPTMINER maintains strong performance under defensive perturbations, highlighting remarkable robustness. Code: https://github.com/aaFrostnova/PromptMiner
<div id='section'>Paperid: <span id='pid'>6, <a href='https://arxiv.org/pdf/2511.21216.pdf' target='_blank'>https://arxiv.org/pdf/2511.21216.pdf</a></span>   <span><a href='https://github.com/ShiFangming0823/AuthenLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangming Shi, Li Li, Kejiang Chen, Guorui Feng, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21216">AuthenLoRA: Entangling Stylization with Imperceptible Watermarks for Copyright-Secure LoRA Adapters</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low-Rank Adaptation (LoRA) offers an efficient paradigm for customizing diffusion models, but its ease of redistribution raises concerns over unauthorized use and the generation of untraceable content. Existing watermarking techniques either target base models or verify LoRA modules themselves, yet they fail to propagate watermarks to generated images, leaving a critical gap in traceability. Moreover, traceability watermarking designed for base models is not tightly coupled with stylization and often introduces visual degradation or high false-positive detection rates. To address these limitations, we propose AuthenLoRA, a unified watermarking framework that embeds imperceptible, traceable watermarks directly into the LoRA training process while preserving stylization quality. AuthenLoRA employs a dual-objective optimization strategy that jointly learns the target style distribution and the watermark-induced distribution shift, ensuring that any image generated with the watermarked LoRA reliably carries the watermark. We further design an expanded LoRA architecture for enhanced multi-scale adaptation and introduce a zero-message regularization mechanism that substantially reduces false positives during watermark verification. Extensive experiments demonstrate that AuthenLoRA achieves high-fidelity stylization, robust watermark propagation, and significantly lower false-positive rates compared with existing approaches. Open-source implementation is available at: https://github.com/ShiFangming0823/AuthenLoRA
<div id='section'>Paperid: <span id='pid'>7, <a href='https://arxiv.org/pdf/2511.07863.pdf' target='_blank'>https://arxiv.org/pdf/2511.07863.pdf</a></span>   <span><a href='https://github.com/Shinwoo-Park/WaterMod' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shinwoo Park, Hyejin Park, Hyeseon Ahn, Yo-Sub Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07863">WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models now draft news, legal analyses, and software code with human-level fluency. At the same time, regulations such as the EU AI Act mandate that each synthetic passage carry an imperceptible, machine-verifiable mark for provenance. Conventional logit-based watermarks satisfy this requirement by selecting a pseudorandom green vocabulary at every decoding step and boosting its logits, yet the random split can exclude the highest-probability token and thus erode fluency. WaterMod mitigates this limitation through a probability-aware modular rule. The vocabulary is first sorted in descending model probability; the resulting ranks are then partitioned by the residue rank mod k, which distributes adjacent-and therefore semantically similar-tokens across different classes. A fixed bias of small magnitude is applied to one selected class. In the zero-bit setting (k=2), an entropy-adaptive gate selects either the even or the odd parity as the green list. Because the top two ranks fall into different parities, this choice embeds a detectable signal while guaranteeing that at least one high-probability token remains available for sampling. In the multi-bit regime (k>2), the current payload digit d selects the color class whose ranks satisfy rank mod k = d. Biasing the logits of that class embeds exactly one base-k digit per decoding step, thereby enabling fine-grained provenance tracing. The same modular arithmetic therefore supports both binary attribution and rich payloads. Experimental results demonstrate that WaterMod consistently attains strong watermark detection performance while maintaining generation quality in both zero-bit and multi-bit settings. This robustness holds across a range of tasks, including natural language generation, mathematical reasoning, and code synthesis. Our code and data are available at https://github.com/Shinwoo-Park/WaterMod.
<div id='section'>Paperid: <span id='pid'>8, <a href='https://arxiv.org/pdf/2510.22366.pdf' target='_blank'>https://arxiv.org/pdf/2510.22366.pdf</a></span>   <span><a href='https://github.com/0xD009/T2SMark' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/0xD009/T2SMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jindong Yang, Han Fang, Weiming Zhang, Nenghai Yu, Kejiang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22366">T2SMark: Balancing Robustness and Diversity in Noise-as-Watermark for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have advanced rapidly in recent years, producing high-fidelity images while raising concerns about intellectual property protection and the misuse of generative AI. Image watermarking for diffusion models, particularly Noise-as-Watermark (NaW) methods, encode watermark as specific standard Gaussian noise vector for image generation, embedding the infomation seamlessly while maintaining image quality. For detection, the generation process is inverted to recover the initial noise vector containing the watermark before extraction. However, existing NaW methods struggle to balance watermark robustness with generation diversity. Some methods achieve strong robustness by heavily constraining initial noise sampling, which degrades user experience, while others preserve diversity but prove too fragile for real-world deployment. To address this issue, we propose T2SMark, a two-stage watermarking scheme based on Tail-Truncated Sampling (TTS). Unlike prior methods that simply map bits to positive or negative values, TTS enhances robustness by embedding bits exclusively in the reliable tail regions while randomly sampling the central zone to preserve the latent distribution. Our two-stage framework then ensures sampling diversity by integrating a randomly generated session key into both encryption pipelines. We evaluate T2SMark on diffusion models with both U-Net and DiT backbones. Extensive experiments show that it achieves an optimal balance between robustness and diversity. Our code is available at \href{https://github.com/0xD009/T2SMark}{https://github.com/0xD009/T2SMark}.
<div id='section'>Paperid: <span id='pid'>9, <a href='https://arxiv.org/pdf/2510.21946.pdf' target='_blank'>https://arxiv.org/pdf/2510.21946.pdf</a></span>   <span><a href='https://github.com/kirudang/LDP_Stealing_Attack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kieu Dang, Phung Lai, NhatHai Phan, Yelong Shen, Ruoming Jin, Abdallah Khreishah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21946">$δ$-STEAL: LLM Stealing Attack with Local Differential Privacy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) demonstrate remarkable capabilities across various tasks. However, their deployment introduces significant risks related to intellectual property. In this context, we focus on model stealing attacks, where adversaries replicate the behaviors of these models to steal services. These attacks are highly relevant to proprietary LLMs and pose serious threats to revenue and financial stability. To mitigate these risks, the watermarking solution embeds imperceptible patterns in LLM outputs, enabling model traceability and intellectual property verification. In this paper, we study the vulnerability of LLM service providers by introducing $δ$-STEAL, a novel model stealing attack that bypasses the service provider's watermark detectors while preserving the adversary's model utility. $δ$-STEAL injects noise into the token embeddings of the adversary's model during fine-tuning in a way that satisfies local differential privacy (LDP) guarantees. The adversary queries the service provider's model to collect outputs and form input-output training pairs. By applying LDP-preserving noise to these pairs, $δ$-STEAL obfuscates watermark signals, making it difficult for the service provider to determine whether its outputs were used, thereby preventing claims of model theft. Our experiments show that $δ$-STEAL with lightweight modifications achieves attack success rates of up to $96.95\%$ without significantly compromising the adversary's model utility. The noise scale in LDP controls the trade-off between attack effectiveness and model utility. This poses a significant risk, as even robust watermarks can be bypassed, allowing adversaries to deceive watermark detectors and undermine current intellectual property protection methods.
<div id='section'>Paperid: <span id='pid'>10, <a href='https://arxiv.org/pdf/2510.21053.pdf' target='_blank'>https://arxiv.org/pdf/2510.21053.pdf</a></span>   <span><a href='https://github.com/UCSB-NLP-Chang/RL-watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Li An, Yujian Liu, Yepeng Liu, Yuheng Bu, Yang Zhang, Shiyu Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21053">A Reinforcement Learning Framework for Robust and Secure LLM Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has emerged as a promising solution for tracing and authenticating text generated by large language models (LLMs). A common approach to LLM watermarking is to construct a green/red token list and assign higher or lower generation probabilities to the corresponding tokens, respectively. However, most existing watermarking algorithms rely on heuristic green/red token list designs, as directly optimizing the list design with techniques such as reinforcement learning (RL) comes with several challenges. First, desirable watermarking involves multiple criteria, i.e., detectability, text quality, robustness against removal attacks, and security against spoofing attacks. Directly optimizing for these criteria introduces many partially conflicting reward terms, leading to an unstable convergence process. Second, the vast action space of green/red token list choices is susceptible to reward hacking. In this paper, we propose an end-to-end RL framework for robust and secure LLM watermarking. Our approach adopts an anchoring mechanism for reward terms to ensure stable training and introduces additional regularization terms to prevent reward hacking. Experiments on standard benchmarks with two backbone LLMs show that our method achieves a state-of-the-art trade-off across all criteria, with notable improvements in resistance to spoofing attacks without degrading other criteria. Our code is available at https://github.com/UCSB-NLP-Chang/RL-watermark.
<div id='section'>Paperid: <span id='pid'>11, <a href='https://arxiv.org/pdf/2510.15976.pdf' target='_blank'>https://arxiv.org/pdf/2510.15976.pdf</a></span>   <span><a href='https://github.com/fattyray/learning-to-watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenrui Wang, Junyi Shu, Billy Chiu, Yu Li, Saleh Alharbi, Min Zhang, Jing Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15976">Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of LLMs has raised concerns about their potential misuse, leading to various watermarking schemes that typically offer high detectability. However, existing watermarking techniques often face trade-off between watermark detectability and generated text quality. In this paper, we introduce Learning to Watermark (LTW), a novel selective watermarking framework that leverages multi-objective optimization to effectively balance these competing goals. LTW features a lightweight network that adaptively decides when to apply the watermark by analyzing sentence embeddings, token entropy, and current watermarking ratio. Training of the network involves two specifically constructed loss functions that guide the model toward Pareto-optimal solutions, thereby harmonizing watermark detectability and text quality. By integrating LTW with two baseline watermarking methods, our experimental evaluations demonstrate that LTW significantly enhances text quality without compromising detectability. Our selective watermarking approach offers a new perspective for designing watermarks for LLMs and a way to preserve high text quality for watermarks. The code is publicly available at: https://github.com/fattyray/learning-to-watermark
<div id='section'>Paperid: <span id='pid'>12, <a href='https://arxiv.org/pdf/2510.14304.pdf' target='_blank'>https://arxiv.org/pdf/2510.14304.pdf</a></span>   <span><a href='https://github.com/KR-0822/TCD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kyungryul Back, Seongbeom Park, Milim Kim, Mincheol Kwon, SangHyeok Lee, Hyunyoung Lee, Junhee Cho, Seunghyun Park, Jinkyu Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14304">Watermarking for Factuality: Guiding Vision-Language Models Toward Truth via Tri-layer Contrastive Decoding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Vision-Language Models (LVLMs) have recently shown promising results on various multimodal tasks, even achieving human-comparable performance in certain cases. Nevertheless, LVLMs remain prone to hallucinations -- they often rely heavily on a single modality or memorize training data without properly grounding their outputs. To address this, we propose a training-free, tri-layer contrastive decoding with watermarking, which proceeds in three steps: (1) select a mature layer and an amateur layer among the decoding layers, (2) identify a pivot layer using a watermark-related question to assess whether the layer is visually well-grounded, and (3) apply tri-layer contrastive decoding to generate the final output. Experiments on public benchmarks such as POPE, MME and AMBER demonstrate that our method achieves state-of-the-art performance in reducing hallucinations in LVLMs and generates more visually grounded responses.
<div id='section'>Paperid: <span id='pid'>13, <a href='https://arxiv.org/pdf/2510.13829.pdf' target='_blank'>https://arxiv.org/pdf/2510.13829.pdf</a></span>   <span><a href='https://github.com/Shinwoo-Park/stela_watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shinwoo Park, Hyejin Park, Hyeseon Ahn, Yo-Sub Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13829">A Linguistics-Aware LLM Watermarking via Syntactic Predictability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) continue to advance rapidly, reliable governance tools have become critical. Publicly verifiable watermarking is particularly essential for fostering a trustworthy AI ecosystem. A central challenge persists: balancing text quality against detection robustness. Recent studies have sought to navigate this trade-off by leveraging signals from model output distributions (e.g., token-level entropy); however, their reliance on these model-specific signals presents a significant barrier to public verification, as the detection process requires access to the logits of the underlying model. We introduce STELA, a novel framework that aligns watermark strength with the linguistic degrees of freedom inherent in language. STELA dynamically modulates the signal using part-of-speech (POS) n-gram-modeled linguistic indeterminacy, weakening it in grammatically constrained contexts to preserve quality and strengthen it in contexts with greater linguistic flexibility to enhance detectability. Our detector operates without access to any model logits, thus facilitating publicly verifiable detection. Through extensive experiments on typologically diverse languages-analytic English, isolating Chinese, and agglutinative Korean-we show that STELA surpasses prior methods in detection robustness. Our code is available at https://github.com/Shinwoo-Park/stela_watermark.
<div id='section'>Paperid: <span id='pid'>14, <a href='https://arxiv.org/pdf/2510.13793.pdf' target='_blank'>https://arxiv.org/pdf/2510.13793.pdf</a></span>   <span><a href='https://github.com/nirgoren/NoisePrints' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nir Goren, Oren Katzir, Abhinav Nakarmi, Eyal Ronen, Mahmood Sharif, Or Patashnik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13793">NoisePrints: Distortion-Free Watermarks for Authorship in Private Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid adoption of diffusion models for visual content generation, proving authorship and protecting copyright have become critical. This challenge is particularly important when model owners keep their models private and may be unwilling or unable to handle authorship issues, making third-party verification essential. A natural solution is to embed watermarks for later verification. However, existing methods require access to model weights and rely on computationally heavy procedures, rendering them impractical and non-scalable. To address these challenges, we propose , a lightweight watermarking scheme that utilizes the random seed used to initialize the diffusion process as a proof of authorship without modifying the generation process. Our key observation is that the initial noise derived from a seed is highly correlated with the generated visual content. By incorporating a hash function into the noise sampling process, we further ensure that recovering a valid seed from the content is infeasible. We also show that sampling an alternative seed that passes verification is infeasible, and demonstrate the robustness of our method under various manipulations. Finally, we show how to use cryptographic zero-knowledge proofs to prove ownership without revealing the seed. By keeping the seed secret, we increase the difficulty of watermark removal. In our experiments, we validate NoisePrints on multiple state-of-the-art diffusion models for images and videos, demonstrating efficient verification using only the seed and output, without requiring access to model weights.
<div id='section'>Paperid: <span id='pid'>15, <a href='https://arxiv.org/pdf/2510.12119.pdf' target='_blank'>https://arxiv.org/pdf/2510.12119.pdf</a></span>   <span><a href='https://github.com/luo-ziyuan/ImageSentinel' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyuan Luo, Yangyi Zhao, Ka Chun Cheung, Simon See, Renjie Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12119">ImageSentinel: Protecting Visual Datasets from Unauthorized Retrieval-Augmented Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread adoption of Retrieval-Augmented Image Generation (RAIG) has raised significant concerns about the unauthorized use of private image datasets. While these systems have shown remarkable capabilities in enhancing generation quality through reference images, protecting visual datasets from unauthorized use in such systems remains a challenging problem. Traditional digital watermarking approaches face limitations in RAIG systems, as the complex feature extraction and recombination processes fail to preserve watermark signals during generation. To address these challenges, we propose ImageSentinel, a novel framework for protecting visual datasets in RAIG. Our framework synthesizes sentinel images that maintain visual consistency with the original dataset. These sentinels enable protection verification through randomly generated character sequences that serve as retrieval keys. To ensure seamless integration, we leverage vision-language models to generate the sentinel images. Experimental results demonstrate that ImageSentinel effectively detects unauthorized dataset usage while preserving generation quality for authorized applications. Code is available at https://github.com/luo-ziyuan/ImageSentinel.
<div id='section'>Paperid: <span id='pid'>16, <a href='https://arxiv.org/pdf/2510.10987.pdf' target='_blank'>https://arxiv.org/pdf/2510.10987.pdf</a></span>   <span><a href='https://github.com/hsannn/ditto.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyeseon Ahn, Shinwoo Park, Yo-Sub Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.10987">DITTO: A Spoofing Attack Framework on Watermarked LLMs via Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The promise of LLM watermarking rests on a core assumption that a specific watermark proves authorship by a specific model. We demonstrate that this assumption is dangerously flawed. We introduce the threat of watermark spoofing, a sophisticated attack that allows a malicious model to generate text containing the authentic-looking watermark of a trusted, victim model. This enables the seamless misattribution of harmful content, such as disinformation, to reputable sources. The key to our attack is repurposing watermark radioactivity, the unintended inheritance of data patterns during fine-tuning, from a discoverable trait into an attack vector. By distilling knowledge from a watermarked teacher model, our framework allows an attacker to steal and replicate the watermarking signal of the victim model. This work reveals a critical security gap in text authorship verification and calls for a paradigm shift towards technologies capable of distinguishing authentic watermarks from expertly imitated ones. Our code is available at https://github.com/hsannn/ditto.git.
<div id='section'>Paperid: <span id='pid'>17, <a href='https://arxiv.org/pdf/2510.07538.pdf' target='_blank'>https://arxiv.org/pdf/2510.07538.pdf</a></span>   <span><a href='https://github.com/Pragati-Meshram/DAWN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pragati Shuddhodhan Meshram, Varun Chandrasekaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07538">D2RA: Dual Domain Regeneration Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing use of generative models has intensified the need for watermarking methods that ensure content attribution and provenance. While recent semantic watermarking schemes improve robustness by embedding signals in latent or frequency representations, we show they remain vulnerable even under resource-constrained adversarial settings. We present D2RA, a training-free, single-image attack that removes or weakens watermarks without access to the underlying model. By projecting watermarked images onto natural priors across complementary representations, D2RA suppresses watermark signals while preserving visual fidelity. Experiments across diverse watermarking schemes demonstrate that our approach consistently reduces watermark detectability, revealing fundamental weaknesses in current designs. Our code is available at https://github.com/Pragati-Meshram/DAWN.
<div id='section'>Paperid: <span id='pid'>18, <a href='https://arxiv.org/pdf/2510.07302.pdf' target='_blank'>https://arxiv.org/pdf/2510.07302.pdf</a></span>   <span><a href='https://github.com/inzamamulDU/SpecGuard_ICCV_2025' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/inzamamulDU/SpecGuard_ICCV_2025' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Inzamamul Alam, Md Tanvir Islam, Khan Muhammad, Simon S. Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07302">SpecGuard: Spectral Projection-based Advanced Invisible Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking embeds imperceptible patterns into images for authenticity verification. However, existing methods often lack robustness against various transformations primarily including distortions, image regeneration, and adversarial perturbation, creating real-world challenges. In this work, we introduce SpecGuard, a novel watermarking approach for robust and invisible image watermarking. Unlike prior approaches, we embed the message inside hidden convolution layers by converting from the spatial domain to the frequency domain using spectral projection of a higher frequency band that is decomposed by wavelet projection. Spectral projection employs Fast Fourier Transform approximation to transform spatial data into the frequency domain efficiently. In the encoding phase, a strength factor enhances resilience against diverse attacks, including adversarial, geometric, and regeneration-based distortions, ensuring the preservation of copyrighted information. Meanwhile, the decoder leverages Parseval's theorem to effectively learn and extract the watermark pattern, enabling accurate retrieval under challenging transformations. We evaluate the proposed SpecGuard based on the embedded watermark's invisibility, capacity, and robustness. Comprehensive experiments demonstrate the proposed SpecGuard outperforms the state-of-the-art models. To ensure reproducibility, the full code is released on \href{https://github.com/inzamamulDU/SpecGuard_ICCV_2025}{\textcolor{blue}{\textbf{GitHub}}}.
<div id='section'>Paperid: <span id='pid'>19, <a href='https://arxiv.org/pdf/2510.02962.pdf' target='_blank'>https://arxiv.org/pdf/2510.02962.pdf</a></span>   <span><a href='https://github.com/NusIoraPrivacy/TRACE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingqi Zhang, Ruibo Chen, Yingqing Yang, Peihua Mai, Heng Huang, Yan Pang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02962">Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are increasingly fine-tuned on smaller, domain-specific datasets to improve downstream performance. These datasets often contain proprietary or copyrighted material, raising the need for reliable safeguards against unauthorized use. Existing membership inference attacks (MIAs) and dataset-inference methods typically require access to internal signals such as logits, while current black-box approaches often rely on handcrafted prompts or a clean reference dataset for calibration, both of which limit practical applicability. Watermarking is a promising alternative, but prior techniques can degrade text quality or reduce task performance. We propose TRACE, a practical framework for fully black-box detection of copyrighted dataset usage in LLM fine-tuning. \texttt{TRACE} rewrites datasets with distortion-free watermarks guided by a private key, ensuring both text quality and downstream utility. At detection time, we exploit the radioactivity effect of fine-tuning on watermarked data and introduce an entropy-gated procedure that selectively scores high-uncertainty tokens, substantially amplifying detection power. Across diverse datasets and model families, TRACE consistently achieves significant detections (p<0.05), often with extremely strong statistical evidence. Furthermore, it supports multi-dataset attribution and remains robust even after continued pretraining on large non-watermarked corpora. These results establish TRACE as a practical route to reliable black-box verification of copyrighted dataset usage. We will make our code available at: https://github.com/NusIoraPrivacy/TRACE.
<div id='section'>Paperid: <span id='pid'>20, <a href='https://arxiv.org/pdf/2509.21057.pdf' target='_blank'>https://arxiv.org/pdf/2509.21057.pdf</a></span>   <span><a href='https://github.com/PMark-repo/PMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Huo, Shuliang Liu, Bin Wang, Junyan Zhang, Yibo Yan, Aiwei Liu, Xuming Hu, Mingxun Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21057">PMark: Towards Robust and Distortion-free Semantic-level Watermarking with Channel Constraints</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic-level watermarking (SWM) for large language models (LLMs) enhances watermarking robustness against text modifications and paraphrasing attacks by treating the sentence as the fundamental unit. However, existing methods still lack strong theoretical guarantees of robustness, and reject-sampling-based generation often introduces significant distribution distortions compared with unwatermarked outputs. In this work, we introduce a new theoretical framework on SWM through the concept of proxy functions (PFs) $\unicode{x2013}$ functions that map sentences to scalar values. Building on this framework, we propose PMark, a simple yet powerful SWM method that estimates the PF median for the next sentence dynamically through sampling while enforcing multiple PF constraints (which we call channels) to strengthen watermark evidence. Equipped with solid theoretical guarantees, PMark achieves the desired distortion-free property and improves the robustness against paraphrasing-style attacks. We also provide an empirically optimized version that further removes the requirement for dynamical median estimation for better sampling efficiency. Experimental results show that PMark consistently outperforms existing SWM baselines in both text quality and robustness, offering a more effective paradigm for detecting machine-generated text. Our code will be released at [this URL](https://github.com/PMark-repo/PMark).
<div id='section'>Paperid: <span id='pid'>21, <a href='https://arxiv.org/pdf/2509.20736.pdf' target='_blank'>https://arxiv.org/pdf/2509.20736.pdf</a></span>   <span><a href='https://github.com/Alphawarheads/Watermark_Spoofing.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenshan Zhang, Xueping Zhang, Yechen Wang, Liwei Jin, Ming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20736">The Impact of Audio Watermarking on Audio Anti-Spoofing Countermeasures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the first study on the impact of audio watermarking on spoofing countermeasures. While anti-spoofing systems are essential for securing speech-based applications, the influence of widely used audio watermarking, originally designed for copyright protection, remains largely unexplored. We construct watermark-augmented training and evaluation datasets, named the Watermark-Spoofing dataset, by applying diverse handcrafted and neural watermarking methods to existing anti-spoofing datasets. Experiments show that watermarking consistently degrades anti-spoofing performance, with higher watermark density correlating with higher Equal Error Rates (EERs). To mitigate this, we propose the Knowledge-Preserving Watermark Learning (KPWL) framework, enabling models to adapt to watermark-induced shifts while preserving their original-domain spoofing detection capability. These findings reveal audio watermarking as a previously overlooked domain shift and establish the first benchmark for developing watermark-resilient anti-spoofing systems. All related protocols are publicly available at https://github.com/Alphawarheads/Watermark_Spoofing.git
<div id='section'>Paperid: <span id='pid'>22, <a href='https://arxiv.org/pdf/2509.17773.pdf' target='_blank'>https://arxiv.org/pdf/2509.17773.pdf</a></span>   <span><a href='https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanjie Wang, Zehua Ma, Han Fang, Weiming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17773">I2VWM: Robust Watermarking for Image to Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of image-guided video generation (I2V) has raised concerns about its potential misuse in misinformation and fraud, underscoring the urgent need for effective digital watermarking. While existing watermarking methods demonstrate robustness within a single modality, they fail to trace source images in I2V settings. To address this gap, we introduce the concept of Robust Diffusion Distance, which measures the temporal persistence of watermark signals in generated videos. Building on this, we propose I2VWM, a cross-modal watermarking framework designed to enhance watermark robustness across time. I2VWM leverages a video-simulation noise layer during training and employs an optical-flow-based alignment module during inference. Experiments on both open-source and commercial I2V models demonstrate that I2VWM significantly improves robustness while maintaining imperceptibility, establishing a new paradigm for cross-modal watermarking in the era of generative video. \href{https://github.com/MrCrims/I2VWM-Robust-Watermarking-for-Image-to-Video-Generation}{Code Released.}
<div id='section'>Paperid: <span id='pid'>23, <a href='https://arxiv.org/pdf/2509.15208.pdf' target='_blank'>https://arxiv.org/pdf/2509.15208.pdf</a></span>   <span><a href='https://github.com/facebookresearch/wmar/tree/main/syncseal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Fernandez, TomÃ¡Å¡ SouÄek, Nikola JovanoviÄ, Hady Elsahar, Sylvestre-Alvise Rebuffi, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15208">Geometric Image Synchronization with Deep Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synchronization is the task of estimating and inverting geometric transformations (e.g., crop, rotation) applied to an image. This work introduces SyncSeal, a bespoke watermarking method for robust image synchronization, which can be applied on top of existing watermarking methods to enhance their robustness against geometric transformations. It relies on an embedder network that imperceptibly alters images and an extractor network that predicts the geometric transformation to which the image was subjected. Both networks are end-to-end trained to minimize the error between the predicted and ground-truth parameters of the transformation, combined with a discriminator to maintain high perceptual quality. We experimentally validate our method on a wide variety of geometric and valuemetric transformations, demonstrating its effectiveness in accurately synchronizing images. We further show that our synchronization can effectively upgrade existing watermarking methods to withstand geometric transformations to which they were previously vulnerable.
<div id='section'>Paperid: <span id='pid'>24, <a href='https://arxiv.org/pdf/2509.07647.pdf' target='_blank'>https://arxiv.org/pdf/2509.07647.pdf</a></span>   <span><a href='https://github.com/thomas11809/SFWMark' target='_blank'>  GitHub</a></span> <span><a href='https://thomas11809.github.io/SFWMark/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sung Ju Lee, Nam Ik Cho
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07647">Semantic Watermarking Reinvented: Enhancing Robustness and Generation Quality with Fourier Integrity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic watermarking techniques for latent diffusion models (LDMs) are robust against regeneration attacks, but often suffer from detection performance degradation due to the loss of frequency integrity. To tackle this problem, we propose a novel embedding method called Hermitian Symmetric Fourier Watermarking (SFW), which maintains frequency integrity by enforcing Hermitian symmetry. Additionally, we introduce a center-aware embedding strategy that reduces the vulnerability of semantic watermarking due to cropping attacks by ensuring robust information retention. To validate our approach, we apply these techniques to existing semantic watermarking schemes, enhancing their frequency-domain structures for better robustness and retrieval accuracy. Extensive experiments demonstrate that our methods achieve state-of-the-art verification and identification performance, surpassing previous approaches across various attack scenarios. Ablation studies confirm the impact of SFW on detection capabilities, the effectiveness of the center-aware embedding against cropping, and how message capacity influences identification accuracy. Notably, our method achieves the highest detection accuracy while maintaining superior image fidelity, as evidenced by FID and CLIP scores. Conclusively, our proposed SFW is shown to be an effective framework for balancing robustness and image fidelity, addressing the inherent trade-offs in semantic watermarking. Code available at https://github.com/thomas11809/SFWMark
<div id='section'>Paperid: <span id='pid'>25, <a href='https://arxiv.org/pdf/2509.03117.pdf' target='_blank'>https://arxiv.org/pdf/2509.03117.pdf</a></span>   <span><a href='https://github.com/LianPing-cyber/PromptCOS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Yang, Yiming Li, Hongwei Yao, Enhao Huang, Shuo Shao, Bingrun Yang, Zhibo Wang, Dacheng Tao, Zhan Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03117">PromptCOS: Towards System Prompt Copyright Auditing for LLMs via Content-level Output Similarity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid progress of large language models (LLMs) has greatly enhanced reasoning tasks and facilitated the development of LLM-based applications. A critical factor in improving LLM-based applications is the design of effective system prompts, which significantly impact the behavior and output quality of LLMs. However, system prompts are susceptible to theft and misuse, which could undermine the interests of prompt owners. Existing methods protect prompt copyrights through watermark injection and verification but face challenges due to their reliance on intermediate LLM outputs (e.g., logits), which limits their practical feasibility. In this paper, we propose PromptCOS, a method for auditing prompt copyright based on content-level output similarity. It embeds watermarks by optimizing the prompt while simultaneously co-optimizing a special verification query and content-level signal marks. This is achieved by leveraging cyclic output signals and injecting auxiliary tokens to ensure reliable auditing in content-only scenarios. Additionally, it incorporates cover tokens to protect the watermark from malicious deletion. For copyright verification, PromptCOS identifies unauthorized usage by comparing the similarity between the suspicious output and the signal mark. Experimental results demonstrate that our method achieves high effectiveness (99.3% average watermark similarity), strong distinctiveness (60.8% greater than the best baseline), high fidelity (accuracy degradation of no more than 0.58%), robustness (resilience against three types of potential attacks), and computational efficiency (up to 98.1% reduction in computational cost). Our code is available at GitHub https://github.com/LianPing-cyber/PromptCOS.
<div id='section'>Paperid: <span id='pid'>26, <a href='https://arxiv.org/pdf/2509.03006.pdf' target='_blank'>https://arxiv.org/pdf/2509.03006.pdf</a></span>   <span><a href='https://github.com/aiiu-lab/DeepRobustWatermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tzuhsuan Huang, Cheng Yu Yeo, Tsai-Ling Huang, Hong-Han Shuai, Wen-Huang Cheng, Jun-Cheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.03006">Enhancing Robustness in Post-Processing Watermarking: An Ensemble Attack Network Using CNNs and Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent studies on deep watermarking have predominantly focused on in-processing watermarking, which integrates the watermarking process into image generation. However, post-processing watermarking, which embeds watermarks after image generation, offers more flexibility. It can be applied to outputs from any generative model (e.g. GANs, diffusion models) without needing access to the model's internal structure. It also allows users to embed unique watermarks into individual images. Therefore, this study focuses on post-processing watermarking and enhances its robustness by incorporating an ensemble attack network during training. We construct various versions of attack networks using CNN and Transformer in both spatial and frequency domains to investigate how each combination influences the robustness of the watermarking model. Our results demonstrate that combining a CNN-based attack network in the spatial domain with a Transformer-based attack network in the frequency domain yields the highest robustness in watermarking models. Extensive evaluation on the WAVES benchmark, using average bit accuracy as the metric, demonstrates that our ensemble attack network significantly enhances the robustness of baseline watermarking methods under various stress tests. In particular, for the Regeneration Attack defined in WAVES, our method improves StegaStamp by 18.743%. The code is released at:https://github.com/aiiu-lab/DeepRobustWatermark.
<div id='section'>Paperid: <span id='pid'>27, <a href='https://arxiv.org/pdf/2509.00757.pdf' target='_blank'>https://arxiv.org/pdf/2509.00757.pdf</a></span>   <span><a href='https://kevinhuangxf.github.io/marksplatter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiufeng Huang, Ziyuan Luo, Qi Song, Ruofei Wang, Renjie Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.00757">MarkSplatter: Generalizable Watermarking for 3D Gaussian Splatting Model via Splatter Image Structure</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing popularity of 3D Gaussian Splatting (3DGS) has intensified the need for effective copyright protection. Current 3DGS watermarking methods rely on computationally expensive fine-tuning procedures for each predefined message. We propose the first generalizable watermarking framework that enables efficient protection of Splatter Image-based 3DGS models through a single forward pass. We introduce GaussianBridge that transforms unstructured 3D Gaussians into Splatter Image format, enabling direct neural processing for arbitrary message embedding. To ensure imperceptibility, we design a Gaussian-Uncertainty-Perceptual heatmap prediction strategy for preserving visual quality. For robust message recovery, we develop a dense segmentation-based extraction mechanism that maintains reliable extraction even when watermarked objects occupy minimal regions in rendered views. Project page: https://kevinhuangxf.github.io/marksplatter.
<div id='section'>Paperid: <span id='pid'>28, <a href='https://arxiv.org/pdf/2508.20228.pdf' target='_blank'>https://arxiv.org/pdf/2508.20228.pdf</a></span>   <span><a href='https://github.com/githshine/SynGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xia Han, Qi Li, Jianbing Ni, Mohammad Zulkernine
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20228">Robustness Assessment and Enhancement of Text Watermarking for Google's SynthID</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in LLM watermarking methods such as SynthID-Text by Google DeepMind offer promising solutions for tracing the provenance of AI-generated text. However, our robustness assessment reveals that SynthID-Text is vulnerable to meaning-preserving attacks, such as paraphrasing, copy-paste modifications, and back-translation, which can significantly degrade watermark detectability. To address these limitations, we propose SynGuard, a hybrid framework that combines the semantic alignment strength of Semantic Information Retrieval (SIR) with the probabilistic watermarking mechanism of SynthID-Text. Our approach jointly embeds watermarks at both lexical and semantic levels, enabling robust provenance tracking while preserving the original meaning. Experimental results across multiple attack scenarios show that SynGuard improves watermark recovery by an average of 11.1\% in F1 score compared to SynthID-Text. These findings demonstrate the effectiveness of semantic-aware watermarking in resisting real-world tampering. All code, datasets, and evaluation scripts are publicly available at: https://github.com/githshine/SynGuard.
<div id='section'>Paperid: <span id='pid'>29, <a href='https://arxiv.org/pdf/2508.08211.pdf' target='_blank'>https://arxiv.org/pdf/2508.08211.pdf</a></span>   <span><a href='https://zhuohaoyu.github.io/SAEMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuohao Yu, Xingru Jiang, Weizheng Gu, Yidong Wang, Shikun Zhang, Wei Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08211">SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking LLM-generated text is critical for content attribution and misinformation prevention. However, existing methods compromise text quality, require white-box model access and logit manipulation. These limitations exclude API-based models and multilingual scenarios. We propose SAEMark, a general framework for post-hoc multi-bit watermarking that embeds personalized messages solely via inference-time, feature-based rejection sampling without altering model logits or requiring training. Our approach operates on deterministic features extracted from generated text, selecting outputs whose feature statistics align with key-derived targets. This framework naturally generalizes across languages and domains while preserving text quality through sampling LLM outputs instead of modifying. We provide theoretical guarantees relating watermark success probability and compute budget that hold for any suitable feature extractor. Empirically, we demonstrate the framework's effectiveness using Sparse Autoencoders (SAEs), achieving superior detection accuracy and text quality. Experiments across 4 datasets show SAEMark's consistent performance, with 99.7% F1 on English and strong multi-bit detection accuracy. SAEMark establishes a new paradigm for scalable watermarking that works out-of-the-box with closed-source LLMs while enabling content attribution.
<div id='section'>Paperid: <span id='pid'>30, <a href='https://arxiv.org/pdf/2508.02115.pdf' target='_blank'>https://arxiv.org/pdf/2508.02115.pdf</a></span>   <span><a href='https://github.com/still2009/cowardFL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjie Li, Siying Gu, Yiming Li, Kangjie Chen, Zhili Chen, Tianwei Zhang, Shu-Tao Xia, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.02115">Coward: Toward Practical Proactive Federated Backdoor Defense via Collision-based Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Backdoor detection is currently the mainstream defense against backdoor attacks in federated learning (FL), where malicious clients upload poisoned updates that compromise the global model and undermine the reliability of FL deployments. Existing backdoor detection techniques fall into two categories, including passive and proactive ones, depending on whether the server proactively modifies the global model. However, both have inherent limitations in practice: passive defenses are vulnerable to common non-i.i.d. data distributions and random participation of FL clients, whereas current proactive defenses suffer inevitable out-of-distribution (OOD) bias because they rely on backdoor co-existence effects. To address these issues, we introduce a new proactive defense, dubbed Coward, inspired by our discovery of multi-backdoor collision effects, in which consecutively planted, distinct backdoors significantly suppress earlier ones. In general, we detect attackers by evaluating whether the server-injected, conflicting global watermark is erased during local training rather than retained. Our method preserves the advantages of proactive defenses in handling data heterogeneity (\ie, non-i.i.d. data) while mitigating the adverse impact of OOD bias through a revised detection mechanism. Extensive experiments on benchmark datasets confirm the effectiveness of Coward and its resilience to potential adaptive attacks. The code for our method would be available at https://github.com/still2009/cowardFL.
<div id='section'>Paperid: <span id='pid'>31, <a href='https://arxiv.org/pdf/2507.21150.pdf' target='_blank'>https://arxiv.org/pdf/2507.21150.pdf</a></span>   <span><a href='https://github.com/pujariaditya/WaveVerify' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/vcbsl/WaveVerify' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Pujari, Ajita Rattani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21150">WaveVerify: A Novel Audio Watermarking Framework for Media Authentication and Combatting Deepfakes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of voice generation technologies has enabled the synthesis of speech that is perceptually indistinguishable from genuine human voices. While these innovations facilitate beneficial applications such as personalized text-to-speech systems and voice preservation, they have also introduced significant risks, including deepfake impersonation scams and synthetic media-driven disinformation campaigns. Recent reports indicate that in 2024, deepfake fraud attempts surged by over 1,300% compared to 2023, underscoring the urgent need for robust audio content authentication. The financial sector has been particularly impacted, with a loss of over 10 million USD to voice scams and individual victims reporting losses exceeding $6,000 from AI-generated deepfake calls. In response, regulators and governments worldwide are enacting measures to improve AI content transparency and traceability, emphasizing the development of forensic tools and watermarking techniques as essential strategies to uphold media integrity.
<div id='section'>Paperid: <span id='pid'>32, <a href='https://arxiv.org/pdf/2507.11137.pdf' target='_blank'>https://arxiv.org/pdf/2507.11137.pdf</a></span>   <span><a href='https://github.com/AIResearch-Group/NeuralMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Yao, Jin Song, Jian Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.11137">Hashed Watermark as a Filter: Defeating Forging and Overwriting Attacks in Weight-based Neural Network Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As valuable digital assets, deep neural networks necessitate robust ownership protection, positioning neural network watermarking (NNW) as a promising solution. Among various NNW approaches, weight-based methods are favored for their simplicity and practicality; however, they remain vulnerable to forging and overwriting attacks. To address those challenges, we propose NeuralMark, a robust method built around a hashed watermark filter. Specifically, we utilize a hash function to generate an irreversible binary watermark from a secret key, which is then used as a filter to select the model parameters for embedding. This design cleverly intertwines the embedding parameters with the hashed watermark, providing a robust defense against both forging and overwriting attacks. An average pooling is also incorporated to resist fine-tuning and pruning attacks. Furthermore, it can be seamlessly integrated into various neural network architectures, ensuring broad applicability. Theoretically, we analyze its security boundary. Empirically, we verify its effectiveness and robustness across 13 distinct Convolutional and Transformer architectures, covering five image classification tasks and one text generation task. The source codes are available at https://github.com/AIResearch-Group/NeuralMark.
<div id='section'>Paperid: <span id='pid'>33, <a href='https://arxiv.org/pdf/2507.10475.pdf' target='_blank'>https://arxiv.org/pdf/2507.10475.pdf</a></span>   <span><a href='https://github.com/ismailtrm/ceng_404' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ä°smail TarÄ±m, AytuÄ Onan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.10475">Can You Detect the Difference?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of large language models (LLMs) has raised concerns about reliably detecting AI-generated text. Stylometric metrics work well on autoregressive (AR) outputs, but their effectiveness on diffusion-based models is unknown. We present the first systematic comparison of diffusion-generated text (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity, burstiness, lexical diversity, readability, and BLEU/ROUGE scores show that LLaDA closely mimics human text in perplexity and burstiness, yielding high false-negative rates for AR-oriented detectors. LLaMA shows much lower perplexity but reduced lexical fidelity. Relying on any single metric fails to separate diffusion outputs from human writing. We highlight the need for diffusion-aware detectors and outline directions such as hybrid models, diffusion-specific stylometric signatures, and robust watermarking.
<div id='section'>Paperid: <span id='pid'>34, <a href='https://arxiv.org/pdf/2507.01428.pdf' target='_blank'>https://arxiv.org/pdf/2507.01428.pdf</a></span>   <span><a href='https://github.com/vpsg-research/DiffMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Sun, Haiyang Sun, Zhiqing Guo, Yunfeng Diao, Liejun Wang, Dan Ma, Gaobo Yang, Keqin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.01428">DiffMark: Diffusion-based Robust Watermark Against Deepfakes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfakes pose significant security and privacy threats through malicious facial manipulations. While robust watermarking can aid in authenticity verification and source tracking, existing methods often lack the sufficient robustness against Deepfake manipulations. Diffusion models have demonstrated remarkable performance in image generation, enabling the seamless fusion of watermark with image during generation. In this study, we propose a novel robust watermarking framework based on diffusion model, called DiffMark. By modifying the training and sampling scheme, we take the facial image and watermark as conditions to guide the diffusion model to progressively denoise and generate corresponding watermarked image. In the construction of facial condition, we weight the facial image by a timestep-dependent factor that gradually reduces the guidance intensity with the decrease of noise, thus better adapting to the sampling process of diffusion model. To achieve the fusion of watermark condition, we introduce a cross information fusion (CIF) module that leverages a learnable embedding table to adaptively extract watermark features and integrates them with image features via cross-attention. To enhance the robustness of the watermark against Deepfake manipulations, we integrate a frozen autoencoder during training phase to simulate Deepfake manipulations. Additionally, we introduce Deepfake-resistant guidance that employs specific Deepfake model to adversarially guide the diffusion sampling process to generate more robust watermarked images. Experimental results demonstrate the effectiveness of the proposed DiffMark on typical Deepfakes. Our code will be available at https://github.com/vpsg-research/DiffMark.
<div id='section'>Paperid: <span id='pid'>35, <a href='https://arxiv.org/pdf/2506.23484.pdf' target='_blank'>https://arxiv.org/pdf/2506.23484.pdf</a></span>   <span><a href='https://github.com/Suchenl/TAG-WM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhuo Chen, Zehua Ma, Han Fang, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23484">TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated content (AIGC) enables efficient visual creation but raises copyright and authenticity risks. As a common technique for integrity verification and source tracing, digital image watermarking is regarded as a potential solution to above issues. However, the widespread adoption and advancing capabilities of generative image editing tools have amplified malicious tampering risks, while simultaneously posing new challenges to passive tampering detection and watermark robustness. To address these challenges, this paper proposes a Tamper-Aware Generative image WaterMarking method named TAG-WM. The proposed method comprises four key modules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright and localization watermarks into the latent space while preserving generative quality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a dense variation region detector (DVRD) leveraging diffusion inversion sensitivity to identify tampered areas via statistical deviation analysis, and the tamper-aware decoding (TAD) guided by localization results. The experimental results demonstrate that TAG-WM achieves state-of-the-art performance in both tampering robustness and localization capability even under distortion, while preserving lossless generation quality and maintaining a watermark capacity of 256 bits. The code is available at: https://github.com/Suchenl/TAG-WM.
<div id='section'>Paperid: <span id='pid'>36, <a href='https://arxiv.org/pdf/2506.16349.pdf' target='_blank'>https://arxiv.org/pdf/2506.16349.pdf</a></span>   <span><a href='https://github.com/facebookresearch/wmar' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikola JovanoviÄ, Ismail Labiad, TomÃ¡Å¡ SouÄek, Martin Vechev, Pierre Fernandez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.16349">Watermarking Autoregressive Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking the outputs of generative models has emerged as a promising approach for tracking their provenance. Despite significant interest in autoregressive image generation models and their potential for misuse, no prior work has attempted to watermark their outputs at the token level. In this work, we present the first such approach by adapting language model watermarking techniques to this setting. We identify a key challenge: the lack of reverse cycle-consistency (RCC), wherein re-tokenizing generated image tokens significantly alters the token sequence, effectively erasing the watermark. To address this and to make our method robust to common image transformations, neural compression, and removal attacks, we introduce (i) a custom tokenizer-detokenizer finetuning procedure that improves RCC, and (ii) a complementary watermark synchronization layer. As our experiments demonstrate, our approach enables reliable and robust watermark detection with theoretically grounded p-values.
<div id='section'>Paperid: <span id='pid'>37, <a href='https://arxiv.org/pdf/2506.13160.pdf' target='_blank'>https://arxiv.org/pdf/2506.13160.pdf</a></span>   <span><a href='https://github.com/NcepuQiaoTing/CertDW' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/NcepuQiaoTing/CertDW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Qiao, Yiming Li, Jianbin Li, Yingjia Wang, Leyi Qi, Junfeng Guo, Ruili Feng, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13160">CertDW: Towards Certified Dataset Ownership Verification via Conformal Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) rely heavily on high-quality open-source datasets (e.g., ImageNet) for their success, making dataset ownership verification (DOV) crucial for protecting public dataset copyrights. In this paper, we find existing DOV methods (implicitly) assume that the verification process is faithful, where the suspicious model will directly verify ownership by using the verification samples as input and returning their results. However, this assumption may not necessarily hold in practice and their performance may degrade sharply when subjected to intentional or unintentional perturbations. To address this limitation, we propose the first certified dataset watermark (i.e., CertDW) and CertDW-based certified dataset ownership verification method that ensures reliable verification even under malicious attacks, under certain conditions (e.g., constrained pixel-level perturbation). Specifically, inspired by conformal prediction, we introduce two statistical measures, including principal probability (PP) and watermark robustness (WR), to assess model prediction stability on benign and watermarked samples under noise perturbations. We prove there exists a provable lower bound between PP and WR, enabling ownership verification when a suspicious model's WR value significantly exceeds the PP values of multiple benign models trained on watermark-free datasets. If the number of PP values smaller than WR exceeds a threshold, the suspicious model is regarded as having been trained on the protected dataset. Extensive experiments on benchmark datasets verify the effectiveness of our CertDW method and its resistance to potential adaptive attacks. Our codes are at \href{https://github.com/NcepuQiaoTing/CertDW}{GitHub}.
<div id='section'>Paperid: <span id='pid'>38, <a href='https://arxiv.org/pdf/2506.06409.pdf' target='_blank'>https://arxiv.org/pdf/2506.06409.pdf</a></span>   <span><a href='https://github.com/DorTsur/HeavyWater_SimplexWater' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dor Tsur, Carol Xuan Long, Claudio Mayrink Verdun, Hsiang Hsu, Chen-Fu Chen, Haim Permuter, Sajani Vithana, Flavio P. Calmon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06409">HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks - such as coding - where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in https://github.com/DorTsur/HeavyWater_SimplexWater
<div id='section'>Paperid: <span id='pid'>39, <a href='https://arxiv.org/pdf/2506.05891.pdf' target='_blank'>https://arxiv.org/pdf/2506.05891.pdf</a></span>   <span><a href='https://thuhcsi.github.io/WAKE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaoxun Xu, Jianwei Yu, Hangting Chen, Zhiyong Wu, Xixin Wu, Dong Yu, Rongzhi Gu, Yi Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05891">WAKE: Watermarking Audio with Key Enrichment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As deep learning advances in audio generation, challenges in audio security and copyright protection highlight the need for robust audio watermarking. Recent neural network-based methods have made progress but still face three main issues: preventing unauthorized access, decoding initial watermarks after multiple embeddings, and embedding varying lengths of watermarks. To address these issues, we propose WAKE, the first key-controllable audio watermark framework. WAKE embeds watermarks using specific keys and recovers them with corresponding keys, enhancing security by making incorrect key decoding impossible. It also resolves the overwriting issue by allowing watermark decoding after multiple embeddings and supports variable-length watermark insertion. WAKE outperforms existing models in both watermarked audio quality and watermark detection accuracy. Code, more results, and demo page: https://thuhcsi.github.io/WAKE.
<div id='section'>Paperid: <span id='pid'>40, <a href='https://arxiv.org/pdf/2506.04879.pdf' target='_blank'>https://arxiv.org/pdf/2506.04879.pdf</a></span>   <span><a href='https://github.com/aiiu-lab/BackdoorImageEditing' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu-Feng Chen, Tzuhsuan Huang, Pin-Yen Chiu, Jun-Cheng Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04879">Invisible Backdoor Triggers in Image Editing Model via Deep Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have achieved remarkable progress in both image generation and editing. However, recent studies have revealed their vulnerability to backdoor attacks, in which specific patterns embedded in the input can manipulate the model's behavior. Most existing research in this area has proposed attack frameworks focused on the image generation pipeline, leaving backdoor attacks in image editing relatively unexplored. Among the few studies targeting image editing, most utilize visible triggers, which are impractical because they introduce noticeable alterations to the input image before editing. In this paper, we propose a novel attack framework that embeds invisible triggers into the image editing process via poisoned training data. We leverage off-the-shelf deep watermarking models to encode imperceptible watermarks as backdoor triggers. Our goal is to make the model produce the predefined backdoor target when it receives watermarked inputs, while editing clean images normally according to the given prompt. With extensive experiments across different watermarking models, the proposed method achieves promising attack success rates. In addition, the analysis results of the watermark characteristics in term of backdoor attack further support the effectiveness of our approach. The code is available at:https://github.com/aiiu-lab/BackdoorImageEditing
<div id='section'>Paperid: <span id='pid'>41, <a href='https://arxiv.org/pdf/2506.04462.pdf' target='_blank'>https://arxiv.org/pdf/2506.04462.pdf</a></span>   <span><a href='https://github.com/dapurv5/alignmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Apurv Verma, NhatHai Phan, Shubhendu Trivedi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.04462">Watermarking Degrades Alignment in Language Models: Analysis and Mitigation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking techniques for large language models (LLMs) can significantly impact output quality, yet their effects on truthfulness, safety, and helpfulness remain critically underexamined. This paper presents a systematic analysis of how two popular watermarking approaches-Gumbel and KGW-affect these core alignment properties across four aligned LLMs. Our experiments reveal two distinct degradation patterns: guard attenuation, where enhanced helpfulness undermines model safety, and guard amplification, where excessive caution reduces model helpfulness. These patterns emerge from watermark-induced shifts in token distribution, surfacing the fundamental tension that exists between alignment objectives.
  To mitigate these degradations, we propose Alignment Resampling (AR), an inference-time sampling method that uses an external reward model to restore alignment. We establish a theoretical lower bound on the improvement in expected reward score as the sample size is increased and empirically demonstrate that sampling just 2-4 watermarked generations effectively recovers or surpasses baseline (unwatermarked) alignment scores. To overcome the limited response diversity of standard Gumbel watermarking, our modified implementation sacrifices strict distortion-freeness while maintaining robust detectability, ensuring compatibility with AR. Experimental results confirm that AR successfully recovers baseline alignment in both watermarking approaches, while maintaining strong watermark detectability. This work reveals the critical balance between watermark strength and model alignment, providing a simple inference-time solution to responsibly deploy watermarked LLMs in practice.
<div id='section'>Paperid: <span id='pid'>42, <a href='https://arxiv.org/pdf/2506.00652.pdf' target='_blank'>https://arxiv.org/pdf/2506.00652.pdf</a></span>   <span><a href='https://github.com/hardenyu21/Video-Signature' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/hardenyu21/Video-Signature' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Huang, Junhao Chen, Shuliang Liu, Hanqian Li, Qi Zheng, Yi R. Fung, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.00652">Video Signature: In-generation Watermarking for Latent Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of Artificial Intelligence Generated Content (AIGC) has led to significant progress in video generation but also raises serious concerns about intellectual property protection and reliable content tracing. Watermarking is a widely adopted solution to this issue, but existing methods for video generation mainly follow a post-generation paradigm, which introduces additional computational overhead and often fails to effectively balance the trade-off between video quality and watermark extraction. To address these issues, we propose Video Signature (VIDSIG), an in-generation watermarking method for latent video diffusion models, which enables implicit and adaptive watermark integration during generation. Specifically, we achieve this by partially fine-tuning the latent decoder, where Perturbation-Aware Suppression (PAS) pre-identifies and freezes perceptually sensitive layers to preserve visual quality. Beyond spatial fidelity, we further enhance temporal consistency by introducing a lightweight Temporal Alignment module that guides the decoder to generate coherent frame sequences during fine-tuning. Experimental results show that VIDSIG achieves the best overall performance in watermark extraction, visual quality, and generation efficiency. It also demonstrates strong robustness against both spatial and temporal tampering, highlighting its practicality in real-world scenarios. Our code is available at \href{https://github.com/hardenyu21/Video-Signature}{here}
<div id='section'>Paperid: <span id='pid'>43, <a href='https://arxiv.org/pdf/2505.24536.pdf' target='_blank'>https://arxiv.org/pdf/2505.24536.pdf</a></span>   <span><a href='https://github.com/Dshm212/CHIP' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaohui Xu, Qi Cui, Chip-Hong Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24536">CHIP: Chameleon Hash-based Irreversible Passport for Robust Deep Model Ownership Verification and Active Usage Control</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The pervasion of large-scale Deep Neural Networks (DNNs) and their enormous training costs make their intellectual property (IP) protection of paramount importance. Recently introduced passport-based methods attempt to steer DNN watermarking towards strengthening ownership verification against ambiguity attacks by modulating the affine parameters of normalization layers. Unfortunately, neither watermarking nor passport-based methods provide a holistic protection with robust ownership proof, high fidelity, active usage authorization and user traceability for offline access distributed models and multi-user Machine-Learning as a Service (MLaaS) cloud model. In this paper, we propose a Chameleon Hash-based Irreversible Passport (CHIP) protection framework that utilizes the cryptographic chameleon hash function to achieve all these goals. The collision-resistant property of chameleon hash allows for strong model ownership claim upon IP infringement and liable user traceability, while the trapdoor-collision property enables hashing of multiple user passports and licensee certificates to the same immutable signature to realize active usage control. Using the owner passport as an oracle, multiple user-specific triplets, each contains a passport-aware user model, a user passport, and a licensee certificate can be created for secure offline distribution. The watermarked master model can also be deployed for MLaaS with usage permission verifiable by the provision of any trapdoor-colliding user passports. CHIP is extensively evaluated on four datasets and two architectures to demonstrate its protection versatility and robustness. Our code is released at https://github.com/Dshm212/CHIP.
<div id='section'>Paperid: <span id='pid'>44, <a href='https://arxiv.org/pdf/2505.24267.pdf' target='_blank'>https://arxiv.org/pdf/2505.24267.pdf</a></span>   <span><a href='https://github.com/fangliancheng/MUSE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Liancheng Fang, Aiwei Liu, Henry Peng Zou, Yankai Chen, Hengrui Zhang, Zhongfen Deng, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.24267">MUSE: Model-Agnostic Tabular Watermarking via Multi-Sample Selection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MUSE, a watermarking algorithm for tabular generative models. Previous approaches typically leverage DDIM invertibility to watermark tabular diffusion models, but tabular diffusion models exhibit significantly poorer invertibility compared to other modalities, compromising performance. Simultaneously, tabular diffusion models require substantially less computation than other modalities, enabling a multi-sample selection approach to tabular generative model watermarking. MUSE embeds watermarks by generating multiple candidate samples and selecting one based on a specialized scoring function, without relying on model invertibility. Our theoretical analysis establishes the relationship between watermark detectability, candidate count, and dataset size, allowing precise calibration of watermarking strength. Extensive experiments demonstrate that MUSE achieves state-of-the-art watermark detectability and robustness against various attacks while maintaining data quality, and remains compatible with any tabular generative model supporting repeated sampling, effectively addressing key challenges in tabular data watermarking. Specifically, it reduces the distortion rates on fidelity metrics by 81-89%, while achieving a 1.0 TPR@0.1%FPR detection rate. Implementation of MUSE can be found at https://github.com/fangliancheng/MUSE.
<div id='section'>Paperid: <span id='pid'>45, <a href='https://arxiv.org/pdf/2505.23873.pdf' target='_blank'>https://arxiv.org/pdf/2505.23873.pdf</a></span>   <span><a href='https://github.com/phrara/kgmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongrui Peng, Haolang Lu, Yuanlong Yu, Weiye Fu, Kun Wang, Guoshun Nan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23873">KGMark: A Diffusion Watermark for Knowledge Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Knowledge graphs (KGs) are ubiquitous in numerous real-world applications, and watermarking facilitates protecting intellectual property and preventing potential harm from AI-generated content. Existing watermarking methods mainly focus on static plain text or image data, while they can hardly be applied to dynamic graphs due to spatial and temporal variations of structured data. This motivates us to propose KGMARK, the first graph watermarking framework that aims to generate robust, detectable, and transparent diffusion fingerprints for dynamic KG data. Specifically, we propose a novel clustering-based alignment method to adapt the watermark to spatial variations. Meanwhile, we present a redundant embedding strategy to harden the diffusion watermark against various attacks, facilitating the robustness of the watermark to the temporal variations. Additionally, we introduce a novel learnable mask matrix to improve the transparency of diffusion fingerprints. By doing so, our KGMARK properly tackles the variation challenges of structured data. Experiments on various public benchmarks show the effectiveness of our proposed KGMARK. Our code is available at https://github.com/phrara/kgmark.
<div id='section'>Paperid: <span id='pid'>46, <a href='https://arxiv.org/pdf/2505.21620.pdf' target='_blank'>https://arxiv.org/pdf/2505.21620.pdf</a></span>   <span><a href='https://github.com/zhengyuan-jiang/VideoMarkBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyuan Jiang, Moyang Guo, Kecen Li, Yuepeng Hu, Yupu Wang, Zhicong Huang, Cheng Hong, Neil Zhenqiang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21620">VideoMarkBench: Benchmarking Robustness of Video Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of video generative models has led to a surge in highly realistic synthetic videos, raising ethical concerns related to disinformation and copyright infringement. Recently, video watermarking has been proposed as a mitigation strategy by embedding invisible marks into AI-generated videos to enable subsequent detection. However, the robustness of existing video watermarking methods against both common and adversarial perturbations remains underexplored. In this work, we introduce VideoMarkBench, the first systematic benchmark designed to evaluate the robustness of video watermarks under watermark removal and watermark forgery attacks. Our study encompasses a unified dataset generated by three state-of-the-art video generative models, across three video styles, incorporating four watermarking methods and seven aggregation strategies used during detection. We comprehensively evaluate 12 types of perturbations under white-box, black-box, and no-box threat models. Our findings reveal significant vulnerabilities in current watermarking approaches and highlight the urgent need for more robust solutions. Our code is available at https://github.com/zhengyuan-jiang/VideoMarkBench.
<div id='section'>Paperid: <span id='pid'>47, <a href='https://arxiv.org/pdf/2505.19504.pdf' target='_blank'>https://arxiv.org/pdf/2505.19504.pdf</a></span>   <span><a href='https://github.com/UNITES-Lab/DOGe' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pingzhi Li, Zhen Tan, Huaizhi Qu, Huan Liu, Tianlong Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19504">DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) represent substantial intellectual and economic investments, yet their effectiveness can inadvertently facilitate model imitation via knowledge distillation (KD).In practical scenarios, competitors can distill proprietary LLM capabilities by simply observing publicly accessible outputs, akin to reverse-engineering a complex performance by observation alone. Existing protective methods like watermarking only identify imitation post-hoc, while other defenses assume the student model mimics the teacher's internal logits, rendering them ineffective against distillation purely from observed output text. This paper confronts the challenge of actively protecting LLMs within the realistic constraints of API-based access. We introduce an effective and efficient Defensive Output Generation (DOGe) strategy that subtly modifies the output behavior of an LLM. Its outputs remain accurate and useful for legitimate users, yet are designed to be misleading for distillation, significantly undermining imitation attempts. We achieve this by fine-tuning only the final linear layer of the teacher LLM with an adversarial loss. This targeted training approach anticipates and disrupts distillation attempts during inference time. Our experiments show that, while preserving or even improving the original performance of the teacher model, student models distilled from the defensively generated teacher outputs demonstrate catastrophically reduced performance, demonstrating our method's effectiveness as a practical safeguard against KD-based model imitation.
<div id='section'>Paperid: <span id='pid'>48, <a href='https://arxiv.org/pdf/2505.16530.pdf' target='_blank'>https://arxiv.org/pdf/2505.16530.pdf</a></span>   <span><a href='https://github.com/yuliangyan0807/llm-fingerprint' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuliang Yan, Haochun Tang, Shuo Yan, Enyan Dai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16530">DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are considered valuable Intellectual Properties (IP) for legitimate owners due to the enormous computational cost of training. It is crucial to protect the IP of LLMs from malicious stealing or unauthorized deployment. Despite existing efforts in watermarking and fingerprinting LLMs, these methods either impact the text generation process or are limited in white-box access to the suspect model, making them impractical. Hence, we propose DuFFin, a novel $\textbf{Du}$al-Level $\textbf{Fin}$gerprinting $\textbf{F}$ramework for black-box setting ownership verification. DuFFin extracts the trigger pattern and the knowledge-level fingerprints to identify the source of a suspect model. We conduct experiments on a variety of models collected from the open-source website, including four popular base models as protected LLMs and their fine-tuning, quantization, and safety alignment versions, which are released by large companies, start-ups, and individual users. Results show that our method can accurately verify the copyright of the base protected LLM on their model variants, achieving the IP-ROC metric greater than 0.95. Our code is available at https://github.com/yuliangyan0807/llm-fingerprint.
<div id='section'>Paperid: <span id='pid'>49, <a href='https://arxiv.org/pdf/2505.14112.pdf' target='_blank'>https://arxiv.org/pdf/2505.14112.pdf</a></span>   <span><a href='https://github.com/Carol-gutianle/IE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianle Gu, Zongqi Wang, Kexin Huang, Yuanqi Yao, Xiangliang Zhang, Yujiu Yang, Xiuying Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14112">Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Logit-based LLM watermarking traces and verifies AI-generated content by maintaining green and red token lists and increasing the likelihood of green tokens during generation. However, it fails in low-entropy scenarios, where predictable outputs make green token selection difficult without disrupting natural text flow. Existing approaches address this by assuming access to the original LLM to calculate entropy and selectively watermark high-entropy tokens. However, these methods face two major challenges: (1) high computational costs and detection delays due to reliance on the original LLM, and (2) potential risks of model leakage. To address these limitations, we propose Invisible Entropy (IE), a watermarking paradigm designed to enhance both safety and efficiency. Instead of relying on the original LLM, IE introduces a lightweight feature extractor and an entropy tagger to predict whether the entropy of the next token is high or low. Furthermore, based on theoretical analysis, we develop a threshold navigator that adaptively sets entropy thresholds. It identifies a threshold where the watermark ratio decreases as the green token count increases, enhancing the naturalness of the watermarked text and improving detection robustness. Experiments on HumanEval and MBPP datasets demonstrate that IE reduces parameter size by 99\% while achieving performance on par with state-of-the-art methods. Our work introduces a safe and efficient paradigm for low-entropy watermarking. https://github.com/Carol-gutianle/IE https://huggingface.co/datasets/Carol0110/IE-Tagger
<div id='section'>Paperid: <span id='pid'>50, <a href='https://arxiv.org/pdf/2505.12667.pdf' target='_blank'>https://arxiv.org/pdf/2505.12667.pdf</a></span>   <span><a href='https://github.com/Sugewud/Safe-Sora' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Su, Xuerui Qiu, Hongbin Xu, Tangyu Jiang, Junhao Zhuang, Chun Yuan, Ming Li, Shengfeng He, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12667">Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The explosive growth of generative video models has amplified the demand for reliable copyright preservation of AI-generated content. Despite its popularity in image synthesis, invisible generative watermarking remains largely underexplored in video generation. To address this gap, we propose Safe-Sora, the first framework to embed graphical watermarks directly into the video generation process. Motivated by the observation that watermarking performance is closely tied to the visual similarity between the watermark and cover content, we introduce a hierarchical coarse-to-fine adaptive matching mechanism. Specifically, the watermark image is divided into patches, each assigned to the most visually similar video frame, and further localized to the optimal spatial region for seamless embedding. To enable spatiotemporal fusion of watermark patches across video frames, we develop a 3D wavelet transform-enhanced Mamba architecture with a novel spatiotemporal local scanning strategy, effectively modeling long-range dependencies during watermark embedding and retrieval. To the best of our knowledge, this is the first attempt to apply state space models to watermarking, opening new avenues for efficient and robust watermark protection. Extensive experiments demonstrate that Safe-Sora achieves state-of-the-art performance in terms of video quality, watermark fidelity, and robustness, which is largely attributed to our proposals. Code is publicly available at https://github.com/Sugewud/Safe-Sora
<div id='section'>Paperid: <span id='pid'>51, <a href='https://arxiv.org/pdf/2505.09924.pdf' target='_blank'>https://arxiv.org/pdf/2505.09924.pdf</a></span>   <span><a href='https://github.com/redwyd/SymMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yidan Wang, Yubing Ren, Yanan Cao, Binxing Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.09924">From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of Large Language Models (LLMs) has heightened concerns about the misuse of AI-generated text, making watermarking a promising solution. Mainstream watermarking schemes for LLMs fall into two categories: logits-based and sampling-based. However, current schemes entail trade-offs among robustness, text quality, and security. To mitigate this, we integrate logits-based and sampling-based schemes, harnessing their respective strengths to achieve synergy. In this paper, we propose a versatile symbiotic watermarking framework with three strategies: serial, parallel, and hybrid. The hybrid framework adaptively embeds watermarks using token entropy and semantic entropy, optimizing the balance between detectability, robustness, text quality, and security. Furthermore, we validate our approach through comprehensive experiments on various datasets and models. Experimental results indicate that our method outperforms existing baselines and achieves state-of-the-art (SOTA) performance. We believe this framework provides novel insights into diverse watermarking paradigms. Our code is available at https://github.com/redwyd/SymMark.
<div id='section'>Paperid: <span id='pid'>52, <a href='https://arxiv.org/pdf/2505.08878.pdf' target='_blank'>https://arxiv.org/pdf/2505.08878.pdf</a></span>   <span><a href='https://github.com/Carol-Long/CC_Watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dor Tsur, Carol Xuan Long, Claudio Mayrink Verdun, Hsiang Hsu, Haim Permuter, Flavio P. Calmon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08878">Optimized Couplings for Watermarking Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-language models (LLMs) are now able to produce text that is, in many cases, seemingly indistinguishable from human-generated content. This has fueled the development of watermarks that imprint a ``signal'' in LLM-generated text with minimal perturbation of an LLM's output. This paper provides an analysis of text watermarking in a one-shot setting. Through the lens of hypothesis testing with side information, we formulate and analyze the fundamental trade-off between watermark detection power and distortion in generated textual quality. We argue that a key component in watermark design is generating a coupling between the side information shared with the watermark detector and a random partition of the LLM vocabulary. Our analysis identifies the optimal coupling and randomization strategy under the worst-case LLM next-token distribution that satisfies a min-entropy constraint. We provide a closed-form expression of the resulting detection rate under the proposed scheme and quantify the cost in a max-min sense. Finally, we provide an array of numerical results, comparing the proposed scheme with the theoretical optimum and existing schemes, in both synthetic data and LLM watermarking. Our code is available at https://github.com/Carol-Long/CC_Watermark
<div id='section'>Paperid: <span id='pid'>53, <a href='https://arxiv.org/pdf/2505.08614.pdf' target='_blank'>https://arxiv.org/pdf/2505.08614.pdf</a></span>   <span><a href='https://github.com/vpsg-research/WaveGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyuan He, Zhiqing Guo, Liejun Wang, Gaobo Yang, Yunfeng Diao, Dan Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08614">WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake technology poses increasing risks such as privacy invasion and identity theft. To address these threats, we propose WaveGuard, a proactive watermarking framework that enhances robustness and imperceptibility via frequency-domain embedding and graph-based structural consistency. Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph Neural Network (SC-GNN) to preserve visual quality. We also design an attention module to refine embedding precision. Experimental results on face swap and reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art methods in both robustness and visual quality. Code is available at https://github.com/vpsg-research/WaveGuard.
<div id='section'>Paperid: <span id='pid'>54, <a href='https://arxiv.org/pdf/2505.05088.pdf' target='_blank'>https://arxiv.org/pdf/2505.05088.pdf</a></span>   <span><a href='https://github.com/wenyang001/SSH-Net' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyang Liu, Jianjun Gao, Kim-Hui Yap
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05088">SSH-Net: A Self-Supervised and Hybrid Network for Noisy Image Watermark Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visible watermark removal is challenging due to its inherent complexities and the noise carried within images. Existing methods primarily rely on supervised learning approaches that require paired datasets of watermarked and watermark-free images, which are often impractical to obtain in real-world scenarios. To address this challenge, we propose SSH-Net, a Self-Supervised and Hybrid Network specifically designed for noisy image watermark removal. SSH-Net synthesizes reference watermark-free images using the watermark distribution in a self-supervised manner and adopts a dual-network design to address the task. The upper network, focused on the simpler task of noise removal, employs a lightweight CNN-based architecture, while the lower network, designed to handle the more complex task of simultaneously removing watermarks and noise, incorporates Transformer blocks to model long-range dependencies and capture intricate image features. To enhance the model's effectiveness, a shared CNN-based feature encoder is introduced before dual networks to extract common features that both networks can leverage. Our code will be available at https://github.com/wenyang001/SSH-Net.
<div id='section'>Paperid: <span id='pid'>55, <a href='https://arxiv.org/pdf/2505.05064.pdf' target='_blank'>https://arxiv.org/pdf/2505.05064.pdf</a></span>   <span><a href='https://github.com/lululu008/WaterDrum' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyang Lu, Xinyuan Niu, Gregory Kang Ruey Lau, Bui Thi Cam Nhung, Rachael Hwee Ling Sim, Fanyu Wen, Chuan-Sheng Foo, See-Kiong Ng, Bryan Kian Hsiang Low
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05064">WaterDrum: Watermarking for Data-centric Unlearning Metric</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language model (LLM) unlearning is critical in real-world applications where it is necessary to efficiently remove the influence of private, copyrighted, or harmful data from some users. However, existing utility-centric unlearning metrics (based on model utility) may fail to accurately evaluate the extent of unlearning in realistic settings such as when (a) the forget and retain set have semantically similar content, (b) retraining the model from scratch on the retain set is impractical, and/or (c) the model owner can improve the unlearning metric without directly performing unlearning on the LLM. This paper presents the first data-centric unlearning metric for LLMs called WaterDrum that exploits robust text watermarking for overcoming these limitations. We also introduce new benchmark datasets for LLM unlearning that contain varying levels of similar data points and can be used to rigorously evaluate unlearning algorithms using WaterDrum. Our code is available at https://github.com/lululu008/WaterDrum and our new benchmark datasets are released at https://huggingface.co/datasets/Glow-AI/WaterDrum-Ax.
<div id='section'>Paperid: <span id='pid'>56, <a href='https://arxiv.org/pdf/2505.02344.pdf' target='_blank'>https://arxiv.org/pdf/2505.02344.pdf</a></span>   <span><a href='https://github.com/KAHIMWONG/E2E_LLM_WM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kahim Wong, Jicheng Zhou, Jiantao Zhou, Yain-Whar Si
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02344">An End-to-End Model For Logits Based Large Language Models Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of LLMs has increased concerns over source tracing and copyright protection for AIGC, highlighting the need for advanced detection technologies. Passive detection methods usually face high false positives, while active watermarking techniques using logits or sampling manipulation offer more effective protection. Existing LLM watermarking methods, though effective on unaltered content, suffer significant performance drops when the text is modified and could introduce biases that degrade LLM performance in downstream tasks. These methods fail to achieve an optimal tradeoff between text quality and robustness, particularly due to the lack of end-to-end optimization of the encoder and decoder. In this paper, we introduce a novel end-to-end logits perturbation method for watermarking LLM-generated text. By jointly optimization, our approach achieves a better balance between quality and robustness. To address non-differentiable operations in the end-to-end training pipeline, we introduce an online prompting technique that leverages the on-the-fly LLM as a differentiable surrogate. Our method achieves superior robustness, outperforming distortion-free methods by 37-39% under paraphrasing and 17.2% on average, while maintaining text quality on par with these distortion-free methods in terms of text perplexity and downstream tasks. Our method can be easily generalized to different LLMs. Code is available at https://github.com/KAHIMWONG/E2E_LLM_WM.
<div id='section'>Paperid: <span id='pid'>57, <a href='https://arxiv.org/pdf/2505.01406.pdf' target='_blank'>https://arxiv.org/pdf/2505.01406.pdf</a></span>   <span><a href='https://github.com/SPIN-UMass/VidStamp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammadreza Teymoorianfard, Shiqing Ma, Amir Houmansadr
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01406">VIDSTAMP: A Temporally-Aware Watermark for Ownership and Integrity in Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid rise of video diffusion models has enabled the generation of highly realistic and temporally coherent videos, raising critical concerns about content authenticity, provenance, and misuse. Existing watermarking approaches, whether passive, post-hoc, or adapted from image-based techniques, often struggle to withstand video-specific manipulations such as frame insertion, dropping, or reordering, and typically degrade visual quality. In this work, we introduce VIDSTAMP, a watermarking framework that embeds per-frame or per-segment messages directly into the latent space of temporally-aware video diffusion models. By fine-tuning the model's decoder through a two-stage pipeline, first on static image datasets to promote spatial message separation, and then on synthesized video sequences to restore temporal consistency, VIDSTAMP learns to embed high-capacity, flexible watermarks with minimal perceptual impact. Leveraging architectural components such as 3D convolutions and temporal attention, our method imposes no additional inference cost and offers better perceptual quality than prior methods, while maintaining comparable robustness against common distortions and tampering. VIDSTAMP embeds 768 bits per video (48 bits per frame) with a bit accuracy of 95.0%, achieves a log P-value of -166.65 (lower is better), and maintains a video quality score of 0.836, comparable to unwatermarked outputs (0.838) and surpassing prior methods in capacity-quality tradeoffs. Code: Code: \url{https://github.com/SPIN-UMass/VidStamp}
<div id='section'>Paperid: <span id='pid'>58, <a href='https://arxiv.org/pdf/2504.16359.pdf' target='_blank'>https://arxiv.org/pdf/2504.16359.pdf</a></span>   <span><a href='https://github.com/KYRIE-LI11/VideoMark' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/KYRIE-LI11/VideoMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuming Hu, Hanqian Li, Jungang Li, Yu Huang, Aiwei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.16359">VideoMark: A Distortion-Free Robust Watermarking Framework for Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work introduces \textbf{VideoMark}, a distortion-free robust watermarking framework for video diffusion models. As diffusion models excel in generating realistic videos, reliable content attribution is increasingly critical. However, existing video watermarking methods often introduce distortion by altering the initial distribution of diffusion variables and are vulnerable to temporal attacks, such as frame deletion, due to variable video lengths. VideoMark addresses these challenges by employing a \textbf{pure pseudorandom initialization} to embed watermarks, avoiding distortion while ensuring uniform noise distribution in the latent space to preserve generation quality. To enhance robustness, we adopt a frame-wise watermarking strategy with pseudorandom error correction (PRC) codes, using a fixed watermark sequence with randomly selected starting indices for each video. For watermark extraction, we propose a Temporal Matching Module (TMM) that leverages edit distance to align decoded messages with the original watermark sequence, ensuring resilience against temporal attacks. Experimental results show that VideoMark achieves higher decoding accuracy than existing methods while maintaining video quality comparable to watermark-free generation. The watermark remains imperceptible to attackers without the secret key, offering superior invisibility compared to other frameworks. VideoMark provides a practical, training-free solution for content attribution in diffusion-based video generation. Code and data are available at \href{https://github.com/KYRIE-LI11/VideoMark}{https://github.com/KYRIE-LI11/VideoMark}{Project Page}.
<div id='section'>Paperid: <span id='pid'>59, <a href='https://arxiv.org/pdf/2504.13416.pdf' target='_blank'>https://arxiv.org/pdf/2504.13416.pdf</a></span>   <span><a href='https://github.com/codeboy5/stamp' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Saksham Rastogi, Pratyush Maini, Danish Pruthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13416">STAMP Your Content: Proving Dataset Membership via Watermarked Rephrasings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given how large parts of publicly available text are crawled to pretrain large language models (LLMs), data creators increasingly worry about the inclusion of their proprietary data for model training without attribution or licensing. Their concerns are also shared by benchmark curators whose test-sets might be compromised. In this paper, we present STAMP, a framework for detecting dataset membership-i.e., determining the inclusion of a dataset in the pretraining corpora of LLMs. Given an original piece of content, our proposal involves first generating multiple rephrases, each embedding a watermark with a unique secret key. One version is to be released publicly, while others are to be kept private. Subsequently, creators can compare model likelihoods between public and private versions using paired statistical tests to prove membership. We show that our framework can successfully detect contamination across four benchmarks which appear only once in the training data and constitute less than 0.001% of the total tokens, outperforming several contamination detection and dataset inference baselines. We verify that STAMP preserves both the semantic meaning and utility of the original data. We apply STAMP to two real-world scenarios to confirm the inclusion of paper abstracts and blog articles in the pretraining corpora.
<div id='section'>Paperid: <span id='pid'>60, <a href='https://arxiv.org/pdf/2504.13061.pdf' target='_blank'>https://arxiv.org/pdf/2504.13061.pdf</a></span>   <span><a href='https://github.com/Jozenn/ArtistAuditor' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Linkang Du, Zheng Zhu, Min Chen, Zhou Su, Shouling Ji, Peng Cheng, Jiming Chen, Zhikun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13061">ArtistAuditor: Auditing Artist Style Pirate in Text-to-Image Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image models based on diffusion processes, such as DALL-E, Stable Diffusion, and Midjourney, are capable of transforming texts into detailed images and have widespread applications in art and design. As such, amateur users can easily imitate professional-level paintings by collecting an artist's work and fine-tuning the model, leading to concerns about artworks' copyright infringement. To tackle these issues, previous studies either add visually imperceptible perturbation to the artwork to change its underlying styles (perturbation-based methods) or embed post-training detectable watermarks in the artwork (watermark-based methods). However, when the artwork or the model has been published online, i.e., modification to the original artwork or model retraining is not feasible, these strategies might not be viable.
  To this end, we propose a novel method for data-use auditing in the text-to-image generation model. The general idea of ArtistAuditor is to identify if a suspicious model has been finetuned using the artworks of specific artists by analyzing the features related to the style. Concretely, ArtistAuditor employs a style extractor to obtain the multi-granularity style representations and treats artworks as samplings of an artist's style. Then, ArtistAuditor queries a trained discriminator to gain the auditing decisions. The experimental results on six combinations of models and datasets show that ArtistAuditor can achieve high AUC values (> 0.937). By studying ArtistAuditor's transferability and core modules, we provide valuable insights into the practical implementation. Finally, we demonstrate the effectiveness of ArtistAuditor in real-world cases by an online platform Scenario. ArtistAuditor is open-sourced at https://github.com/Jozenn/ArtistAuditor.
<div id='section'>Paperid: <span id='pid'>61, <a href='https://arxiv.org/pdf/2504.12809.pdf' target='_blank'>https://arxiv.org/pdf/2504.12809.pdf</a></span>   <span><a href='https://github.com/inzamamulDU/SADRE' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/inzamamulDU/SADRE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Inzamamul Alam, Md Tanvir Islam, Simon S. Woo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12809">Saliency-Aware Diffusion Reconstruction for Effective Invisible Watermark Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As digital content becomes increasingly ubiquitous, the need for robust watermark removal techniques has grown due to the inadequacy of existing embedding techniques, which lack robustness. This paper introduces a novel Saliency-Aware Diffusion Reconstruction (SADRE) framework for watermark elimination on the web, combining adaptive noise injection, region-specific perturbations, and advanced diffusion-based reconstruction. SADRE disrupts embedded watermarks by injecting targeted noise into latent representations guided by saliency masks although preserving essential image features. A reverse diffusion process ensures high-fidelity image restoration, leveraging adaptive noise levels determined by watermark strength. Our framework is theoretically grounded with stability guarantees and achieves robust watermark removal across diverse scenarios. Empirical evaluations on state-of-the-art (SOTA) watermarking techniques demonstrate SADRE's superiority in balancing watermark disruption and image quality. SADRE sets a new benchmark for watermark elimination, offering a flexible and reliable solution for real-world web content. Code is available on~\href{https://github.com/inzamamulDU/SADRE}{\textbf{https://github.com/inzamamulDU/SADRE}}.
<div id='section'>Paperid: <span id='pid'>62, <a href='https://arxiv.org/pdf/2504.07002.pdf' target='_blank'>https://arxiv.org/pdf/2504.07002.pdf</a></span>   <span><a href='https://github.com/xiaoyuanpigo/DeCoMa' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuan Xiao, Yuchen Chen, Shiqing Ma, Haocheng Huang, Chunrong Fang, Yanwei Chen, Weisong Sun, Yunfeng Zhu, Xiaofang Zhang, Zhenyu Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.07002">DeCoMa: Detecting and Purifying Code Dataset Watermarks through Dual Channel Code Abstraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is a technique to help identify the source of data points, which can be used to help prevent the misuse of protected datasets. Existing methods on code watermarking, leveraging the idea from the backdoor research, embed stealthy triggers as watermarks. Despite their high resilience against dilution attacks and backdoor detections, the robustness has not been fully evaluated. To fill this gap, we propose DeCoMa, a dual-channel approach to Detect and purify Code dataset waterMarks. To overcome the high barrier created by the stealthy and hidden nature of code watermarks, DeCoMa leverages dual-channel constraints on code to generalize and map code samples into standardized templates. Subsequently, DeCoMa extracts hidden watermarks by identifying outlier associations between paired elements within the standardized templates. Finally, DeCoMa purifies the watermarked dataset by removing all samples containing the detected watermark, enabling the silent appropriation of protected code. We conduct extensive experiments to evaluate the effectiveness and efficiency of DeCoMa, covering 14 types of code watermarks and 3 representative intelligent code tasks (a total of 14 scenarios). Experimental results demonstrate that DeCoMa achieves a stable recall of 100% in 14 code watermark detection scenarios, significantly outperforming the baselines. Additionally, DeCoMa effectively attacks code watermarks with embedding rates as low as 0.1%, while maintaining comparable model performance after training on the purified dataset. Furthermore, as DeCoMa requires no model training for detection, it achieves substantially higher efficiency than all baselines, with a speedup ranging from 31.5 to 130.9X. The results call for more advanced watermarking techniques for code models, while DeCoMa can serve as a baseline for future evaluation. Code is available at https://github.com/xiaoyuanpigo/DeCoMa
<div id='section'>Paperid: <span id='pid'>63, <a href='https://arxiv.org/pdf/2504.06575.pdf' target='_blank'>https://arxiv.org/pdf/2504.06575.pdf</a></span>   <span><a href='https://github.com/UCSB-NLP-Chang/contrastive-watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Li An, Yujian Liu, Yepeng Liu, Yang Zhang, Yuheng Bu, Shiyu Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06575">Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has emerged as a promising technique for detecting texts generated by LLMs. Current research has primarily focused on three design criteria: high quality of the watermarked text, high detectability, and robustness against removal attack. However, the security against spoofing attacks remains relatively understudied. For example, a piggyback attack can maliciously alter the meaning of watermarked text-transforming it into hate speech-while preserving the original watermark, thereby damaging the reputation of the LLM provider. We identify two core challenges that make defending against spoofing difficult: (1) the need for watermarks to be both sensitive to semantic-distorting changes and insensitive to semantic-preserving edits, and (2) the contradiction between the need to detect global semantic shifts and the local, auto-regressive nature of most watermarking schemes. To address these challenges, we propose a semantic-aware watermarking algorithm that post-hoc embeds watermarks into a given target text while preserving its original meaning. Our method introduces a semantic mapping model, which guides the generation of a green-red token list, contrastively trained to be sensitive to semantic-distorting changes and insensitive to semantic-preserving changes. Experiments on two standard benchmarks demonstrate strong robustness against removal attacks and security against spoofing attacks, including sentiment reversal and toxic content insertion, while maintaining high watermark detectability. Our approach offers a significant step toward more secure and semantically aware watermarking for LLMs. Our code is available at https://github.com/UCSB-NLP-Chang/contrastive-watermark.
<div id='section'>Paperid: <span id='pid'>64, <a href='https://arxiv.org/pdf/2504.03850.pdf' target='_blank'>https://arxiv.org/pdf/2504.03850.pdf</a></span>   <span><a href='https://github.com/dsgiitr/flux-watermarking' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/dsgiitr/flux-watermarking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ved Umrajkar, Aakash Kumar Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03850">Detection Limits and Statistical Separability of Tree Ring Watermarks in Rectified Flow-based Text-to-Image Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tree-Ring Watermarking is a significant technique for authenticating AI-generated images. However, its effectiveness in rectified flow-based models remains unexplored, particularly given the inherent challenges of these models with noise latent inversion. Through extensive experimentation, we evaluated and compared the detection and separability of watermarks between SD 2.1 and FLUX.1-dev models. By analyzing various text guidance configurations and augmentation attacks, we demonstrate how inversion limitations affect both watermark recovery and the statistical separation between watermarked and unwatermarked images. Our findings provide valuable insights into the current limitations of Tree-Ring Watermarking in the current SOTA models and highlight the critical need for improved inversion methods to achieve reliable watermark detection and separability. The official implementation, dataset release and all experimental results are available at this \href{https://github.com/dsgiitr/flux-watermarking}{\textbf{link}}.
<div id='section'>Paperid: <span id='pid'>65, <a href='https://arxiv.org/pdf/2504.03128.pdf' target='_blank'>https://arxiv.org/pdf/2504.03128.pdf</a></span>   <span><a href='https://github.com/KAHIMWONG/FontGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kahim Wong, Jicheng Zhou, Kemou Li, Yain-Whar Si, Xiaowei Wu, Jiantao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03128">FontGuard: A Robust Font Watermarking Approach Leveraging Deep Font Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of AI-generated content brings significant concerns on the forensic and security issues such as source tracing, copyright protection, etc, highlighting the need for effective watermarking technologies. Font-based text watermarking has emerged as an effective solution to embed information, which could ensure copyright, traceability, and compliance of the generated text content. Existing font watermarking methods usually neglect essential font knowledge, which leads to watermarked fonts of low quality and limited embedding capacity. These methods are also vulnerable to real-world distortions, low-resolution fonts, and inaccurate character segmentation. In this paper, we introduce FontGuard, a novel font watermarking model that harnesses the capabilities of font models and language-guided contrastive learning. Unlike previous methods that focus solely on the pixel-level alteration, FontGuard modifies fonts by altering hidden style features, resulting in better font quality upon watermark embedding. We also leverage the font manifold to increase the embedding capacity of our proposed method by generating substantial font variants closely resembling the original font. Furthermore, in the decoder, we employ an image-text contrastive learning to reconstruct the embedded bits, which can achieve desirable robustness against various real-world transmission distortions. FontGuard outperforms state-of-the-art methods by +5.4%, +7.4%, and +5.8% in decoding accuracy under synthetic, cross-media, and online social network distortions, respectively, while improving the visual quality by 52.7% in terms of LPIPS. Moreover, FontGuard uniquely allows the generation of watermarked fonts for unseen fonts without re-training the network. The code and dataset are available at https://github.com/KAHIMWONG/FontGuard.
<div id='section'>Paperid: <span id='pid'>66, <a href='https://arxiv.org/pdf/2503.21824.pdf' target='_blank'>https://arxiv.org/pdf/2503.21824.pdf</a></span>   <span><a href='https://github.com/ttthhl/Protecting_Your_Video_Content' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haitong Liu, Kuofeng Gao, Yang Bai, Jinmin Li, Jinxiao Shan, Tao Dai, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.21824">Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, video-based large language models (video-based LLMs) have achieved impressive performance across various video comprehension tasks. However, this rapid advancement raises significant privacy and security concerns, particularly regarding the unauthorized use of personal video data in automated annotation by video-based LLMs. These unauthorized annotated video-text pairs can then be used to improve the performance of downstream tasks, such as text-to-video generation. To safeguard personal videos from unauthorized use, we propose two series of protective video watermarks with imperceptible adversarial perturbations, named Ramblings and Mutes. Concretely, Ramblings aim to mislead video-based LLMs into generating inaccurate captions for the videos, thereby degrading the quality of video annotations through inconsistencies between video content and captions. Mutes, on the other hand, are designed to prompt video-based LLMs to produce exceptionally brief captions, lacking descriptive detail. Extensive experiments demonstrate that our video watermarking methods effectively protect video data by significantly reducing video annotation performance across various video-based LLMs, showcasing both stealthiness and robustness in protecting personal video content. Our code is available at https://github.com/ttthhl/Protecting_Your_Video_Content.
<div id='section'>Paperid: <span id='pid'>67, <a href='https://arxiv.org/pdf/2503.20802.pdf' target='_blank'>https://arxiv.org/pdf/2503.20802.pdf</a></span>   <span><a href='https://github.com/DrankXs/BalancedWatermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuhao Zhang, Bo Cheng, Jiale Han, Yuli Chen, Zhixuan Wu, Changbao Li, Pingli Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.20802">CEFW: A Comprehensive Evaluation Framework for Watermark in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking provides an effective solution for identifying synthetic text generated by large language models. However, existing techniques often focus on satisfying specific criteria while ignoring other key aspects, lacking a unified evaluation. To fill this gap, we propose the Comprehensive Evaluation Framework for Watermark (CEFW), a unified framework that comprehensively evaluates watermarking methods across five key dimensions: ease of detection, fidelity of text quality, minimal embedding cost, robustness to adversarial attacks, and imperceptibility to prevent imitation or forgery. By assessing watermarks according to all these key criteria, CEFW offers a thorough evaluation of their practicality and effectiveness. Moreover, we introduce a simple and effective watermarking method called Balanced Watermark (BW), which guarantees robustness and imperceptibility through balancing the way watermark information is added. Extensive experiments show that BW outperforms existing methods in overall performance across all evaluation dimensions. We release our code to the community for future research. https://github.com/DrankXs/BalancedWatermark.
<div id='section'>Paperid: <span id='pid'>68, <a href='https://arxiv.org/pdf/2503.19176.pdf' target='_blank'>https://arxiv.org/pdf/2503.19176.pdf</a></span>   <span><a href='https://sokaudiowm.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhu Wen, Ashwin Innuganti, Aaron Bien Ramos, Hanqing Guo, Qiben Yan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.19176">SoK: How Robust is Audio Watermarking in Generative AI models?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio watermarking is increasingly used to verify the provenance of AI-generated content, enabling applications such as detecting AI-generated speech, protecting music IP, and defending against voice cloning. To be effective, audio watermarks must resist removal attacks that distort signals to evade detection. While many schemes claim robustness, these claims are typically tested in isolation and against a limited set of attacks. A systematic evaluation against diverse removal attacks is lacking, hindering practical deployment. In this paper, we investigate whether recent watermarking schemes that claim robustness can withstand a broad range of removal attacks. First, we introduce a taxonomy covering 22 audio watermarking schemes. Next, we summarize their underlying technologies and potential vulnerabilities. We then present a large-scale empirical study to assess their robustness. To support this, we build an evaluation framework encompassing 22 types of removal attacks (109 configurations) including signal-level, physical-level, and AI-induced distortions. We reproduce 9 watermarking schemes using open-source code, identify 8 new highly effective attacks, and highlight 11 key findings that expose the fundamental limitations of these methods across 3 public datasets. Our results reveal that none of the surveyed schemes can withstand all tested distortions. This evaluation offers a comprehensive view of how current watermarking methods perform under real-world threats. Our demo and code are available at https://sokaudiowm.github.io/.
<div id='section'>Paperid: <span id='pid'>69, <a href='https://arxiv.org/pdf/2503.05794.pdf' target='_blank'>https://arxiv.org/pdf/2503.05794.pdf</a></span>   <span><a href='https://github.com/Radiant0726/CBW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Li, Kaiying Yan, Shuo Shao, Tongqing Zhai, Shu-Tao Xia, Zhan Qin, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.05794">CBW: Towards Dataset Ownership Verification for Speaker Verification via Clustering-based Backdoor Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing adoption of deep learning in speaker verification, large-scale speech datasets have become valuable intellectual property. To audit and prevent the unauthorized usage of these valuable released datasets, especially in commercial or open-source scenarios, we propose a novel dataset ownership verification method. Our approach introduces a clustering-based backdoor watermark (CBW), enabling dataset owners to determine whether a suspicious third-party model has been trained on a protected dataset under a black-box setting. The CBW method consists of two key stages: dataset watermarking and ownership verification. During watermarking, we implant multiple trigger patterns in the dataset to make similar samples (measured by their feature similarities) close to the same trigger while dissimilar samples are near different triggers. This ensures that any model trained on the watermarked dataset exhibits specific misclassification behaviors when exposed to trigger-embedded inputs. To verify dataset ownership, we design a hypothesis-test-based framework that statistically evaluates whether a suspicious model exhibits the expected backdoor behavior. We conduct extensive experiments on benchmark datasets, verifying the effectiveness and robustness of our method against potential adaptive attacks. The code for reproducing main experiments is available at https://github.com/Radiant0726/CBW
<div id='section'>Paperid: <span id='pid'>70, <a href='https://arxiv.org/pdf/2502.19125.pdf' target='_blank'>https://arxiv.org/pdf/2502.19125.pdf</a></span>   <span><a href='https://github.com/luo-ziyuan/NeRF_Signature' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyuan Luo, Anderson Rocha, Boxin Shi, Qing Guo, Haoliang Li, Renjie Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.19125">The NeRF Signature: Codebook-Aided Watermarking for Neural Radiance Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Radiance Fields (NeRF) have been gaining attention as a significant form of 3D content representation. With the proliferation of NeRF-based creations, the need for copyright protection has emerged as a critical issue. Although some approaches have been proposed to embed digital watermarks into NeRF, they often neglect essential model-level considerations and incur substantial time overheads, resulting in reduced imperceptibility and robustness, along with user inconvenience. In this paper, we extend the previous criteria for image watermarking to the model level and propose NeRF Signature, a novel watermarking method for NeRF. We employ a Codebook-aided Signature Embedding (CSE) that does not alter the model structure, thereby maintaining imperceptibility and enhancing robustness at the model level. Furthermore, after optimization, any desired signatures can be embedded through the CSE, and no fine-tuning is required when NeRF owners want to use new binary signatures. Then, we introduce a joint pose-patch encryption watermarking strategy to hide signatures into patches rendered from a specific viewpoint for higher robustness. In addition, we explore a Complexity-Aware Key Selection (CAKS) scheme to embed signatures in high visual complexity patches to enhance imperceptibility. The experimental results demonstrate that our method outperforms other baseline methods in terms of imperceptibility and robustness. The source code is available at: https://github.com/luo-ziyuan/NeRF_Signature.
<div id='section'>Paperid: <span id='pid'>71, <a href='https://arxiv.org/pdf/2502.18851.pdf' target='_blank'>https://arxiv.org/pdf/2502.18851.pdf</a></span>   <span><a href='https://github.com/inistory/STONE-watermarking/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jungin Kim, Shinwoo Park, Yo-Sub Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18851">Marking Code Without Breaking It: Code Watermarking for Detecting LLM-Generated Code</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code watermarking identifies AI-generated code by embedding patterns into the code during generation. Effective watermarking requires meeting two key conditions: the watermark should be reliably detectable, and the code should retain its original functionality. However, existing methods often modify tokens that are critical for program logic, such as keywords in conditional expressions or operators in arithmetic computations. These modifications can cause syntax errors or functional failures, limiting the practical use of watermarking. We present STONE, a method that preserves functional integrity by selectively inserting watermarks only into non-syntax tokens. By excluding tokens essential for code execution, STONE minimizes the risk of functional degradation.
  In addition, we introduce CWEM, a comprehensive evaluation metric that evaluates watermarking techniques based on correctness, detectability, and naturalness. While correctness and detectability have been widely used, naturalness remains underexplored despite its importance. Unnatural patterns can reveal the presence of a watermark, making it easier for adversaries to remove. We evaluate STONE using CWEM and compare its performance with the state-of-the-art approach. The results show that STONE achieves an average improvement of 7.69% in CWEM across Python, C++, and Java. Our code is available in https://github.com/inistory/STONE-watermarking/.
<div id='section'>Paperid: <span id='pid'>72, <a href='https://arxiv.org/pdf/2502.11598.pdf' target='_blank'>https://arxiv.org/pdf/2502.11598.pdf</a></span>   <span><a href='https://github.com/THU-BPM/Watermark-Radioactivity-Attack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leyi Pan, Aiwei Liu, Shiyu Huang, Yijian Lu, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11598">Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The radioactive nature of Large Language Model (LLM) watermarking enables the detection of watermarks inherited by student models when trained on the outputs of watermarked teacher models, making it a promising tool for preventing unauthorized knowledge distillation. However, the robustness of watermark radioactivity against adversarial actors remains largely unexplored. In this paper, we investigate whether student models can acquire the capabilities of teacher models through knowledge distillation while avoiding watermark inheritance. We propose two categories of watermark removal approaches: pre-distillation removal through untargeted and targeted training data paraphrasing (UP and TP), and post-distillation removal through inference-time watermark neutralization (WN). Extensive experiments across multiple model pairs, watermarking schemes and hyper-parameter settings demonstrate that both TP and WN thoroughly eliminate inherited watermarks, with WN achieving this while maintaining knowledge transfer efficiency and low computational overhead. Given the ongoing deployment of watermarking techniques in production LLMs, these findings emphasize the urgent need for more robust defense strategies. Our code is available at https://github.com/THU-BPM/Watermark-Radioactivity-Attack.
<div id='section'>Paperid: <span id='pid'>73, <a href='https://arxiv.org/pdf/2502.05215.pdf' target='_blank'>https://arxiv.org/pdf/2502.05215.pdf</a></span>   <span><a href='https://pierrefdz.github.io/publications/phd-thesis' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Fernandez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05215">Watermarking across Modalities for Content Tracing and Generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking embeds information into digital content like images, audio, or text, imperceptible to humans but robustly detectable by specific algorithms. This technology has important applications in many challenges of the industry such as content moderation, tracing AI-generated content, and monitoring the usage of AI models. The contributions of this thesis include the development of new watermarking techniques for images, audio, and text. We first introduce methods for active moderation of images on social platforms. We then develop specific techniques for AI-generated content. We specifically demonstrate methods to adapt latent generative models to embed watermarks in all generated content, identify watermarked sections in speech, and improve watermarking in large language models with tests that ensure low false positive rates. Furthermore, we explore the use of digital watermarking to detect model misuse, including the detection of watermarks in language models fine-tuned on watermarked text, and introduce training-free watermarks for the weights of large transformers. Through these contributions, the thesis provides effective solutions for the challenges posed by the increasing use of generative AI models and the need for model monitoring and content moderation. It finally examines the challenges and limitations of watermarking techniques and discuss potential future directions for research in this area.
<div id='section'>Paperid: <span id='pid'>74, <a href='https://arxiv.org/pdf/2502.04230.pdf' target='_blank'>https://arxiv.org/pdf/2502.04230.pdf</a></span>   <span><a href='https://liuyixin-louis.github.io/xattnmark/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixin Liu, Lie Lu, Jihui Jin, Lichao Sun, Andrea Fanelli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04230">XAttnMark: Learning Robust Audio Watermarking with Cross-Attention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid proliferation of generative audio synthesis and editing technologies has raised significant concerns about copyright infringement, data provenance, and the spread of misinformation through deepfake audio. Watermarking offers a proactive solution by embedding imperceptible, identifiable, and traceable marks into audio content. While recent neural network-based watermarking methods like WavMark and AudioSeal have improved robustness and quality, they struggle to achieve both robust detection and accurate attribution simultaneously. This paper introduces Cross-Attention Robust Audio Watermark (XAttnMark), which bridges this gap by leveraging partial parameter sharing between the generator and the detector, a cross-attention mechanism for efficient message retrieval, and a temporal conditioning module for improved message distribution. Additionally, we propose a psychoacoustic-aligned temporal-frequency masking loss that captures fine-grained auditory masking effects, enhancing watermark imperceptibility. Our approach achieves state-of-the-art performance in both detection and attribution, demonstrating superior robustness against a wide range of audio transformations, including challenging generative editing with strong editing strength. The project webpage is available at https://liuyixin-louis.github.io/xattnmark/.
<div id='section'>Paperid: <span id='pid'>75, <a href='https://arxiv.org/pdf/2501.17858.pdf' target='_blank'>https://arxiv.org/pdf/2501.17858.pdf</a></span>   <span><a href='https://github.com/sail-sg/Rigging-ChatbotArena' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Min, Tianyu Pang, Chao Du, Qian Liu, Minhao Cheng, Min Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17858">Improving Your Model Ranking on Chatbot Arena by Vote Rigging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model $m_{t}$. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving $m_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about $1\%$ of new battles will involve $m_{t}$. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle. We conduct experiments on around $1.7$ million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. Our code is available at https://github.com/sail-sg/Rigging-ChatbotArena.
<div id='section'>Paperid: <span id='pid'>76, <a href='https://arxiv.org/pdf/2501.14195.pdf' target='_blank'>https://arxiv.org/pdf/2501.14195.pdf</a></span>   <span><a href='https://github.com/hurunyi/VideoShield' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyi Hu, Jie Zhang, Yiming Li, Jiwei Li, Qing Guo, Han Qiu, Tianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.14195">VideoShield: Regulating Diffusion-based Video Generation Models via Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence Generated Content (AIGC) has advanced significantly, particularly with the development of video generation models such as text-to-video (T2V) models and image-to-video (I2V) models. However, like other AIGC types, video generation requires robust content control. A common approach is to embed watermarks, but most research has focused on images, with limited attention given to videos. Traditional methods, which embed watermarks frame-by-frame in a post-processing manner, often degrade video quality. In this paper, we propose VideoShield, a novel watermarking framework specifically designed for popular diffusion-based video generation models. Unlike post-processing methods, VideoShield embeds watermarks directly during video generation, eliminating the need for additional training. To ensure video integrity, we introduce a tamper localization feature that can detect changes both temporally (across frames) and spatially (within individual frames). Our method maps watermark bits to template bits, which are then used to generate watermarked noise during the denoising process. Using DDIM Inversion, we can reverse the video to its original watermarked noise, enabling straightforward watermark extraction. Additionally, template bits allow precise detection for potential temporal and spatial modification. Extensive experiments across various video models (both T2V and I2V models) demonstrate that our method effectively extracts watermarks and detects tamper without compromising video quality. Furthermore, we show that this approach is applicable to image generation models, enabling tamper detection in generated images as well. Codes and models are available at https://github.com/hurunyi/VideoShield.
<div id='section'>Paperid: <span id='pid'>77, <a href='https://arxiv.org/pdf/2501.09328.pdf' target='_blank'>https://arxiv.org/pdf/2501.09328.pdf</a></span>   <span><a href='https://github.com/NeurHT/NeurHT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixiao Xu, Binxing Fang, Rui Wang, Yinghai Zhou, Yuan Liu, Mohan Li, Zhihong Tian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09328">Neural Honeytrace: A Robust Plug-and-Play Watermarking Framework against Model Extraction Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Developing high-performance deep learning models is resource-intensive, leading model owners to utilize Machine Learning as a Service (MLaaS) platforms instead of publicly releasing their models. However, malicious users may exploit query interfaces to execute model extraction attacks, reconstructing the target model's functionality locally. While prior research has investigated triggerable watermarking techniques for asserting ownership, existing methods face significant challenges: (1) most approaches require additional training, resulting in high overhead and limited flexibility, and (2) they often fail to account for advanced attackers, leaving them vulnerable to adaptive attacks.
  In this paper, we propose Neural Honeytrace, a robust plug-and-play watermarking framework against model extraction attacks. We first formulate a watermark transmission model from an information-theoretic perspective, providing an interpretable account of the principles and limitations of existing triggerable watermarking. Guided by the model, we further introduce: (1) a similarity-based training-free watermarking method for plug-and-play and flexible watermarking, and (2) a distribution-based multi-step watermark information transmission strategy for robust watermarking. Comprehensive experiments on four datasets demonstrate that Neural Honeytrace outperforms previous methods in efficiency and resisting adaptive attacks. Neural Honeytrace reduces the average number of samples required for a worst-case t-Test-based copyright claim from 193,252 to 1,857 with zero training cost. The code is available at https://github.com/NeurHT/NeurHT.
<div id='section'>Paperid: <span id='pid'>78, <a href='https://arxiv.org/pdf/2412.13917.pdf' target='_blank'>https://arxiv.org/pdf/2412.13917.pdf</a></span>   <span><a href='https://DiscreteWM.github.io/discrete_wm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shengpeng Ji, Ziyue Jiang, Jialong Zuo, Minghui Fang, Yifu Chen, Tao Jin, Zhou Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.13917">Speech Watermarking with Discrete Intermediate Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Speech watermarking techniques can proactively mitigate the potential harmful consequences of instant voice cloning techniques. These techniques involve the insertion of signals into speech that are imperceptible to humans but can be detected by algorithms. Previous approaches typically embed watermark messages into continuous space. However, intuitively, embedding watermark information into robust discrete latent space can significantly improve the robustness of watermarking systems. In this paper, we propose DiscreteWM, a novel speech watermarking framework that injects watermarks into the discrete intermediate representations of speech. Specifically, we map speech into discrete latent space with a vector-quantized autoencoder and inject watermarks by changing the modular arithmetic relation of discrete IDs. To ensure the imperceptibility of watermarks, we also propose a manipulator model to select the candidate tokens for watermark embedding. Experimental results demonstrate that our framework achieves state-of-the-art performance in robustness and imperceptibility, simultaneously. Moreover, our flexible frame-wise approach can serve as an efficient solution for both voice cloning detection and information hiding. Additionally, DiscreteWM can encode 1 to 150 bits of watermark information within a 1-second speech clip, indicating its encoding capacity. Audio samples are available at https://DiscreteWM.github.io/discrete_wm.
<div id='section'>Paperid: <span id='pid'>79, <a href='https://arxiv.org/pdf/2412.12511.pdf' target='_blank'>https://arxiv.org/pdf/2412.12511.pdf</a></span>   <span><a href='https://github.com/tomputer-g/IDL_WAR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjun Hwang, Sungwon Woo, Tom Gao, Raymond Luo, Sunghwan Baek
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12511">Invisible Watermarks: Attacks and Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Generative AI continues to become more accessible, the case for robust detection of generated images in order to combat misinformation is stronger than ever. Invisible watermarking methods act as identifiers of generated content, embedding image- and latent-space messages that are robust to many forms of perturbations. The majority of current research investigates full-image attacks against images with a single watermarking method applied. We introduce novel improvements to watermarking robustness as well as minimizing degradation on image quality during attack. Firstly, we examine the application of both image-space and latent-space watermarking methods on a single image, where we propose a custom watermark remover network which preserves one of the watermarking modalities while completely removing the other during decoding. Then, we investigate localized blurring attacks (LBA) on watermarked images based on the GradCAM heatmap acquired from the watermark decoder in order to reduce the amount of degradation to the target image. Our evaluation suggests that 1) implementing the watermark remover model to preserve one of the watermark modalities when decoding the other modality slightly improves on the baseline performance, and that 2) LBA degrades the image significantly less compared to uniform blurring of the entire image. Code is available at: https://github.com/tomputer-g/IDL_WAR
<div id='section'>Paperid: <span id='pid'>80, <a href='https://arxiv.org/pdf/2412.09492.pdf' target='_blank'>https://arxiv.org/pdf/2412.09492.pdf</a></span>   <span><a href='https://github.com/facebookresearch/videoseal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Fernandez, Hady Elsahar, I. Zeki Yalniz, Alexandre Mourachko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09492">Video Seal: Open and Efficient Video Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of AI-generated content and sophisticated video editing tools has made it both important and challenging to moderate digital platforms. Video watermarking addresses these challenges by embedding imperceptible signals into videos, allowing for identification. However, the rare open tools and methods often fall short on efficiency, robustness, and flexibility. To reduce these gaps, this paper introduces Video Seal, a comprehensive framework for neural video watermarking and a competitive open-sourced model. Our approach jointly trains an embedder and an extractor, while ensuring the watermark robustness by applying transformations in-between, e.g., video codecs. This training is multistage and includes image pre-training, hybrid post-training and extractor fine-tuning. We also introduce temporal watermark propagation, a technique to convert any image watermarking model to an efficient video watermarking model without the need to watermark every high-resolution frame. We present experimental results demonstrating the effectiveness of the approach in terms of speed, imperceptibility, and robustness. Video Seal achieves higher robustness compared to strong baselines especially under challenging distortions combining geometric transformations and video compression. Additionally, we provide new insights such as the impact of video compression during training, and how to compare methods operating on different payloads. Contributions in this work - including the codebase, models, and a public demo - are open-sourced under permissive licenses to foster further research and development in the field.
<div id='section'>Paperid: <span id='pid'>81, <a href='https://arxiv.org/pdf/2412.04852.pdf' target='_blank'>https://arxiv.org/pdf/2412.04852.pdf</a></span>   <span><a href='https://github.com/taco-group/SleeperMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zilan Wang, Junfeng Guo, Jiacheng Zhu, Yiming Li, Heng Huang, Muhao Chen, Zhengzhong Tu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04852">SleeperMark: Towards Robust Watermark against Fine-Tuning Text-to-image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large-scale text-to-image (T2I) diffusion models have enabled a variety of downstream applications, including style customization, subject-driven personalization, and conditional generation. As T2I models require extensive data and computational resources for training, they constitute highly valued intellectual property (IP) for their legitimate owners, yet making them incentive targets for unauthorized fine-tuning by adversaries seeking to leverage these models for customized, usually profitable applications. Existing IP protection methods for diffusion models generally involve embedding watermark patterns and then verifying ownership through generated outputs examination, or inspecting the model's feature space. However, these techniques are inherently ineffective in practical scenarios when the watermarked model undergoes fine-tuning, and the feature space is inaccessible during verification ((i.e., black-box setting). The model is prone to forgetting the previously learned watermark knowledge when it adapts to a new task. To address this challenge, we propose SleeperMark, a novel framework designed to embed resilient watermarks into T2I diffusion models. SleeperMark explicitly guides the model to disentangle the watermark information from the semantic concepts it learns, allowing the model to retain the embedded watermark while continuing to be adapted to new downstream tasks. Our extensive experiments demonstrate the effectiveness of SleeperMark across various types of diffusion models, including latent diffusion models (e.g., Stable Diffusion) and pixel diffusion models (e.g., DeepFloyd-IF), showing robustness against downstream fine-tuning and various attacks at both the image and model levels, with minimal impact on the model's generative capability. The code is available at https://github.com/taco-group/SleeperMark.
<div id='section'>Paperid: <span id='pid'>82, <a href='https://arxiv.org/pdf/2412.03123.pdf' target='_blank'>https://arxiv.org/pdf/2412.03123.pdf</a></span>   <span><a href='https://github.com/xiaojunxu/multi-bit-text-watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojun Xu, Jinghan Jia, Yuanshun Yao, Yang Liu, Hang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03123">Robust Multi-bit Text Watermark with LLM-based Paraphrasers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an imperceptible multi-bit text watermark embedded by paraphrasing with LLMs. We fine-tune a pair of LLM paraphrasers that are designed to behave differently so that their paraphrasing difference reflected in the text semantics can be identified by a trained decoder. To embed our multi-bit watermark, we use two paraphrasers alternatively to encode the pre-defined binary code at the sentence level. Then we use a text classifier as the decoder to decode each bit of the watermark. Through extensive experiments, we show that our watermarks can achieve over 99.99\% detection AUC with small (1.1B) text paraphrasers while keeping the semantic information of the original sentence. More importantly, our pipeline is robust under word substitution and sentence paraphrasing perturbations and generalizes well to out-of-distributional data. We also show the stealthiness of our watermark with LLM-based evaluation. We open-source the code: https://github.com/xiaojunxu/multi-bit-text-watermark.
<div id='section'>Paperid: <span id='pid'>83, <a href='https://arxiv.org/pdf/2412.02576.pdf' target='_blank'>https://arxiv.org/pdf/2412.02576.pdf</a></span>   <span><a href='https://github.com/Ardor-Wu/transfer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qilong Wu, Varun Chandrasekaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.02576">The Efficacy of Transfer-based No-box Attacks on Image Watermarking: A Pragmatic Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking approaches are widely used to identify if images being circulated are authentic or AI-generated. Determining the robustness of image watermarking methods in the ``no-box'' setting, where the attacker is assumed to have no knowledge about the watermarking model, is an interesting problem. Our main finding is that evading the no-box setting is challenging: the success of optimization-based transfer attacks (involving training surrogate models) proposed in prior work~\cite{hu2024transfer} depends on impractical assumptions, including (i) aligning the architecture and training configurations of both the victim and attacker's surrogate watermarking models, as well as (ii) a large number of surrogate models with potentially large computational requirements. Relaxing these assumptions i.e., moving to a more pragmatic threat model results in a failed attack, with an evasion rate at most $21.1\%$. We show that when the configuration is mostly aligned, a simple non-optimization attack we propose, OFT, with one single surrogate model can already exceed the success of optimization-based efforts. Under the same $\ell_\infty$ norm perturbation budget of $0.25$, prior work~\citet{hu2024transfer} is comparable to or worse than OFT in $11$ out of $12$ configurations and has a limited advantage on the remaining one. The code used for all our experiments is available at \url{https://github.com/Ardor-Wu/transfer}.
<div id='section'>Paperid: <span id='pid'>84, <a href='https://arxiv.org/pdf/2411.19895.pdf' target='_blank'>https://arxiv.org/pdf/2411.19895.pdf</a></span>   <span><a href='https://github.com/NarcissusEx/GuardSplat' target='_blank'>  GitHub</a></span> <span><a href='https://narcissusex.github.io/GuardSplat,' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zixuan Chen, Guangcong Wang, Jiahao Zhu, Jianhuang Lai, Xiaohua Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19895">GuardSplat: Efficient and Robust Watermarking for 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has recently created impressive 3D assets for various applications. However, considering security, capacity, invisibility, and training efficiency, the copyright of 3DGS assets is not well protected as existing watermarking methods are unsuited for its rendering pipeline. In this paper, we propose GuardSplat, an innovative and efficient framework for watermarking 3DGS assets. Specifically, 1) We propose a CLIP-guided pipeline for optimizing the message decoder with minimal costs. The key objective is to achieve high-accuracy extraction by leveraging CLIP's aligning capability and rich representations, demonstrating exceptional capacity and efficiency. 2) We tailor a Spherical-Harmonic-aware (SH-aware) Message Embedding module for 3DGS, seamlessly embedding messages into the SH features of each 3D Gaussian while preserving the original 3D structure. This enables watermarking 3DGS assets with minimal fidelity trade-offs and prevents malicious users from removing the watermarks from the model files, meeting the demands for invisibility and security. 3) We present an Anti-distortion Message Extraction module to improve robustness against various distortions. Experiments demonstrate that GuardSplat outperforms state-of-the-art and achieves fast optimization speed. Project page is at https://narcissusex.github.io/GuardSplat, and Code is at https://github.com/NarcissusEx/GuardSplat.
<div id='section'>Paperid: <span id='pid'>85, <a href='https://arxiv.org/pdf/2411.19563.pdf' target='_blank'>https://arxiv.org/pdf/2411.19563.pdf</a></span>   <span><a href='http://github.com/CommodoreEU/ensemble-watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Georg Niess, Roman Kern
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.19563">Ensemble Watermarks for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) reach human-like fluency, reliably distinguishing AI-generated text from human authorship becomes increasingly difficult. While watermarks already exist for LLMs, they often lack flexibility and struggle with attacks such as paraphrasing. To address these issues, we propose a multi-feature method for generating watermarks that combines multiple distinct watermark features into an ensemble watermark. Concretely, we combine acrostica and sensorimotor norms with the established red-green watermark to achieve a 98% detection rate. After a paraphrasing attack, the performance remains high with 95% detection rate. In comparison, the red-green feature alone as a baseline achieves a detection rate of 49% after paraphrasing. The evaluation of all feature combinations reveals that the ensemble of all three consistently has the highest detection rate across several LLMs and watermark strength settings. Due to the flexibility of combining features in the ensemble, various requirements and trade-offs can be addressed. Additionally, the same detection function can be used without adaptations for all ensemble configurations. This method is particularly of interest to facilitate accountability and prevent societal harm.
<div id='section'>Paperid: <span id='pid'>86, <a href='https://arxiv.org/pdf/2411.13553.pdf' target='_blank'>https://arxiv.org/pdf/2411.13553.pdf</a></span>   <span><a href='https://github.com/moyangkuo/ImageDetectBench.git' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Moyang Guo, Yuepeng Hu, Zhengyuan Jiang, Zeyu Li, Amir Sadovnik, Arka Daw, Neil Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13553">AI-generated Image Detection: Passive or Watermark?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While text-to-image models offer numerous benefits, they also pose significant societal risks. Detecting AI-generated images is crucial for mitigating these risks. Detection methods can be broadly categorized into passive and watermark-based approaches: passive detectors rely on artifacts present in AI-generated images, whereas watermark-based detectors proactively embed watermarks into such images. A key question is which type of detector performs better in terms of effectiveness, robustness, and efficiency. However, the current literature lacks a comprehensive understanding of this issue. In this work, we aim to bridge that gap by developing ImageDetectBench, the first comprehensive benchmark to compare the effectiveness, robustness, and efficiency of passive and watermark-based detectors. Our benchmark includes four datasets, each containing a mix of AI-generated and non-AI-generated images. We evaluate five passive detectors and four watermark-based detectors against eight types of common perturbations and three types of adversarial perturbations. Our benchmark results reveal several interesting findings. For instance, watermark-based detectors consistently outperform passive detectors, both in the presence and absence of perturbations. Based on these insights, we provide recommendations for detecting AI-generated images, e.g., when both types of detectors are applicable, watermark-based detectors should be the preferred choice. Our code and data are publicly available at https://github.com/moyangkuo/ImageDetectBench.git.
<div id='section'>Paperid: <span id='pid'>87, <a href='https://arxiv.org/pdf/2411.09359.pdf' target='_blank'>https://arxiv.org/pdf/2411.09359.pdf</a></span>   <span><a href='https://github.com/Zk4-ps/EaaS-Embedding-Watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zekun Fei, Biao Yi, Jianing Geng, Ruiqi He, Lihai Nie, Zheli Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.09359">Your Semantic-Independent Watermark is Fragile: A Semantic Perturbation Attack against EaaS Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embedding-as-a-Service (EaaS) has emerged as a successful business pattern but faces significant challenges related to various forms of copyright infringement, particularly, the API misuse and model extraction attacks. Various studies have proposed backdoor-based watermarking schemes to protect the copyright of EaaS services. In this paper, we reveal that previous watermarking schemes possess semantic-independent characteristics and propose the Semantic Perturbation Attack (SPA). Our theoretical and experimental analysis demonstrate that this semantic-independent nature makes current watermarking schemes vulnerable to adaptive attacks that exploit semantic perturbations tests to bypass watermark verification. Extensive experimental results across multiple datasets demonstrate that the True Positive Rate (TPR) for identifying watermarked samples under SPA can reach up to more than 95\%, rendering watermarks ineffective while maintaining the high utility of embeddings. Furthermore, we discuss potential defense strategies to mitigate SPA. Our code is available at https://github.com/Zk4-ps/EaaS-Embedding-Watermark.
<div id='section'>Paperid: <span id='pid'>88, <a href='https://arxiv.org/pdf/2411.07795.pdf' target='_blank'>https://arxiv.org/pdf/2411.07795.pdf</a></span>   <span><a href='https://github.com/microsoft/InvisMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Xu, Mengya Hu, Deren Lei, Yaxi Li, David Lowe, Alex Gorevski, Mingyu Wang, Emily Ching, Alex Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07795">InvisMark: Invisible and Robust Watermarking for AI-generated Image Provenance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of AI-generated images has intensified the need for robust content authentication methods. We present InvisMark, a novel watermarking technique designed for high-resolution AI-generated images. Our approach leverages advanced neural network architectures and training strategies to embed imperceptible yet highly robust watermarks. InvisMark achieves state-of-the-art performance in imperceptibility (PSNR$\sim$51, SSIM $\sim$ 0.998) while maintaining over 97\% bit accuracy across various image manipulations. Notably, we demonstrate the successful encoding of 256-bit watermarks, significantly expanding payload capacity while preserving image quality. This enables the embedding of UUIDs with error correction codes, achieving near-perfect decoding success rates even under challenging image distortions. We also address potential vulnerabilities against advanced attacks and propose mitigation strategies. By combining high imperceptibility, extended payload capacity, and resilience to manipulations, InvisMark provides a robust foundation for ensuring media provenance in an era of increasingly sophisticated AI-generated content. Source code of this paper is available at: https://github.com/microsoft/InvisMark.
<div id='section'>Paperid: <span id='pid'>89, <a href='https://arxiv.org/pdf/2411.07231.pdf' target='_blank'>https://arxiv.org/pdf/2411.07231.pdf</a></span>   <span><a href='https://github.com/facebookresearch/watermark-anything' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Sander, Pierre Fernandez, Alain Durmus, Teddy Furon, Matthijs Douze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.07231">Watermark Anything with Localized Messages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image watermarking methods are not tailored to handle small watermarked areas. This restricts applications in real-world scenarios where parts of the image may come from different sources or have been edited. We introduce a deep-learning model for localized image watermarking, dubbed the Watermark Anything Model (WAM). The WAM embedder imperceptibly modifies the input image, while the extractor segments the received image into watermarked and non-watermarked areas and recovers one or several hidden messages from the areas found to be watermarked. The models are jointly trained at low resolution and without perceptual constraints, then post-trained for imperceptibility and multiple watermarks. Experiments show that WAM is competitive with state-of-the art methods in terms of imperceptibility and robustness, especially against inpainting and splicing, even on high-resolution images. Moreover, it offers new capabilities: WAM can locate watermarked areas in spliced images and extract distinct 32-bit messages with less than 1 bit error from multiple small regions -- no larger than 10% of the image surface -- even for small 256x256 images. Training and inference code and model weights are available at https://github.com/facebookresearch/watermark-anything.
<div id='section'>Paperid: <span id='pid'>90, <a href='https://arxiv.org/pdf/2411.03862.pdf' target='_blank'>https://arxiv.org/pdf/2411.03862.pdf</a></span>   <span><a href='https://github.com/Hannah1102/ROBIN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Huayang Huang, Yu Wu, Qian Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03862">ROBIN: Robust and Invisible Watermarks for Diffusion Models with Adversarial Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking generative content serves as a vital tool for authentication, ownership protection, and mitigation of potential misuse. Existing watermarking methods face the challenge of balancing robustness and concealment. They empirically inject a watermark that is both invisible and robust and passively achieve concealment by limiting the strength of the watermark, thus reducing the robustness. In this paper, we propose to explicitly introduce a watermark hiding process to actively achieve concealment, thus allowing the embedding of stronger watermarks. To be specific, we implant a robust watermark in an intermediate diffusion state and then guide the model to hide the watermark in the final generated image. We employ an adversarial optimization algorithm to produce the optimal hiding prompt guiding signal for each watermark. The prompt embedding is optimized to minimize artifacts in the generated image, while the watermark is optimized to achieve maximum strength. The watermark can be verified by reversing the generation process. Experiments on various diffusion models demonstrate the watermark remains verifiable even under significant image tampering and shows superior invisibility compared to other state-of-the-art robust watermarking methods. Code is available at https://github.com/Hannah1102/ROBIN.
<div id='section'>Paperid: <span id='pid'>91, <a href='https://arxiv.org/pdf/2410.22705.pdf' target='_blank'>https://arxiv.org/pdf/2410.22705.pdf</a></span>   <span><a href='https://qsong2001.github.io/geometry_cloak' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Song, Ziyuan Luo, Ka Chun Cheung, Simon See, Renjie Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22705">Geometry Cloak: Preventing TGS-based 3D Reconstruction from Copyrighted Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Single-view 3D reconstruction methods like Triplane Gaussian Splatting (TGS) have enabled high-quality 3D model generation from just a single image input within seconds. However, this capability raises concerns about potential misuse, where malicious users could exploit TGS to create unauthorized 3D models from copyrighted images. To prevent such infringement, we propose a novel image protection approach that embeds invisible geometry perturbations, termed "geometry cloaks", into images before supplying them to TGS. These carefully crafted perturbations encode a customized message that is revealed when TGS attempts 3D reconstructions of the cloaked image. Unlike conventional adversarial attacks that simply degrade output quality, our method forces TGS to fail the 3D reconstruction in a specific way - by generating an identifiable customized pattern that acts as a watermark. This watermark allows copyright holders to assert ownership over any attempted 3D reconstructions made from their protected images. Extensive experiments have verified the effectiveness of our geometry cloak. Our project is available at https://qsong2001.github.io/geometry_cloak.
<div id='section'>Paperid: <span id='pid'>92, <a href='https://arxiv.org/pdf/2410.21179.pdf' target='_blank'>https://arxiv.org/pdf/2410.21179.pdf</a></span>   <span><a href='https://hku-tasr.github.io/Sanitizer/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaijing Luo, Ka-Ho Chow
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21179">Harmless Backdoor-based Client-side Watermarking in Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protecting intellectual property (IP) in federated learning (FL) is increasingly important as clients contribute proprietary data to collaboratively train models. Model watermarking, particularly through backdoor-based methods, has emerged as a popular approach for verifying ownership and contributions in deep neural networks trained via FL. By manipulating their datasets, clients can embed a secret pattern, resulting in non-intuitive predictions that serve as proof of participation, useful for claiming incentives or IP co-ownership. However, this technique faces practical challenges: (i) client watermarks can collide, leading to ambiguous ownership claims, and (ii) malicious clients may exploit watermarks to manipulate model predictions for harmful purposes. To address these issues, we propose Sanitizer, a server-side method that ensures client-embedded backdoors can only be activated in harmless environments but not natural queries. It identifies subnets within client-submitted models, extracts backdoors throughout the FL process, and confines them to harmless, client-specific input subspaces. This approach not only enhances Sanitizer's efficiency but also resolves conflicts when clients use similar triggers with different target labels. Our empirical results demonstrate that Sanitizer achieves near-perfect success verifying client contributions while mitigating the risks of malicious watermark use. Additionally, it reduces GPU memory consumption by 85% and cuts processing time by at least 5x compared to the baseline. Our code is open-sourced at https://hku-tasr.github.io/Sanitizer/.
<div id='section'>Paperid: <span id='pid'>93, <a href='https://arxiv.org/pdf/2410.21088.pdf' target='_blank'>https://arxiv.org/pdf/2410.21088.pdf</a></span>   <span><a href='https://github.com/liwd190019/Shallow-Diffuse' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenda Li, Huijie Zhang, Qing Qu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.21088">Shallow Diffuse: Robust and Invisible Watermarking through Low-Dimensional Subspaces in Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread use of AI-generated content from diffusion models has raised significant concerns regarding misinformation and copyright infringement. Watermarking is a crucial technique for identifying these AI-generated images and preventing their misuse. In this paper, we introduce Shallow Diffuse, a new watermarking technique that embeds robust and invisible watermarks into diffusion model outputs. Unlike existing approaches that integrate watermarking throughout the entire diffusion sampling process, Shallow Diffuse decouples these steps by leveraging the presence of a low-dimensional subspace in the image generation process. This method ensures that a substantial portion of the watermark lies in the null space of this subspace, effectively separating it from the image generation process. Our theoretical and empirical analyses show that this decoupling strategy greatly enhances the consistency of data generation and the detectability of the watermark. Extensive experiments further validate that our Shallow Diffuse outperforms existing watermarking methods in terms of robustness and consistency. The codes will be released at https://github.com/liwd190019/Shallow-Diffuse.
<div id='section'>Paperid: <span id='pid'>94, <a href='https://arxiv.org/pdf/2410.20670.pdf' target='_blank'>https://arxiv.org/pdf/2410.20670.pdf</a></span>   <span><a href='https://github.com/doccstat/llm-watermark-cpd' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingchi Li, Guanxun Li, Xianyang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20670">Segmenting Watermarked Texts From Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is a technique that involves embedding nearly unnoticeable statistical signals within generated content to help trace its source. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted language model (LLM) provider, who then generates a text from their LLM with a watermark. This setup makes it possible for a detector to later identify the source of the text if the user publishes it. The user can modify the generated text by substitutions, insertions, or deletions. Our objective is to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. We further propose a methodology to segment the published text into watermarked and non-watermarked sub-strings. The proposed approach is built upon randomization tests and change point detection techniques. We demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. To validate our technique, we apply it to texts generated by several language models with prompts extracted from Google's C4 dataset and obtain encouraging numerical results. We release all code publicly at https://github.com/doccstat/llm-watermark-cpd.
<div id='section'>Paperid: <span id='pid'>95, <a href='https://arxiv.org/pdf/2410.18775.pdf' target='_blank'>https://arxiv.org/pdf/2410.18775.pdf</a></span>   <span><a href='https://github.com/Shilin-LU/VINE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shilin Lu, Zihan Zhou, Jiayou Lu, Yuanzhi Zhu, Adams Wai-Kin Kong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18775">Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, we demonstrate that most methods fail to detect watermarks after such edits. To address this limitation, we propose VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. Our approach involves two key innovations: (1) we analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows us to use them as surrogate attacks during training to bolster watermark robustness; (2) we leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that our method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness. Code is available at https://github.com/Shilin-LU/VINE.
<div id='section'>Paperid: <span id='pid'>96, <a href='https://arxiv.org/pdf/2410.17533.pdf' target='_blank'>https://arxiv.org/pdf/2410.17533.pdf</a></span>   <span><a href='https://github.com/Yuxin104/FedGMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Yang, Qiang Li, Yuan Hong, Binghui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17533">FedGMark: Certifiably Robust Watermarking for Federated Graph Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated graph learning (FedGL) is an emerging learning paradigm to collaboratively train graph data from various clients. However, during the development and deployment of FedGL models, they are susceptible to illegal copying and model theft. Backdoor-based watermarking is a well-known method for mitigating these attacks, as it offers ownership verification to the model owner. We take the first step to protect the ownership of FedGL models via backdoor-based watermarking. Existing techniques have challenges in achieving the goal: 1) they either cannot be directly applied or yield unsatisfactory performance; 2) they are vulnerable to watermark removal attacks; and 3) they lack of formal guarantees. To address all the challenges, we propose FedGMark, the first certified robust backdoor-based watermarking for FedGL. FedGMark leverages the unique graph structure and client information in FedGL to learn customized and diverse watermarks. It also designs a novel GL architecture that facilitates defending against both the empirical and theoretically worst-case watermark removal attacks. Extensive experiments validate the promising empirical and provable watermarking performance of FedGMark. Source code is available at: https://github.com/Yuxin104/FedGMark.
<div id='section'>Paperid: <span id='pid'>97, <a href='https://arxiv.org/pdf/2410.13907.pdf' target='_blank'>https://arxiv.org/pdf/2410.13907.pdf</a></span>   <span><a href='https://github.com/dongdongzhaoUP/NSmark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Haodong Zhao, Jinming Hu, Peixuan Li, Fangqi Li, Jinrui Sha, Tianjie Ju, Peixuan Chen, Zhuosheng Zhang, Gongshen Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13907">NSmark: Null Space Based Black-box Watermarking Defense Framework for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language models (LMs) have emerged as critical intellectual property (IP) assets that necessitate protection. Although various watermarking strategies have been proposed, they remain vulnerable to Linear Functionality Equivalence Attack (LFEA), which can invalidate most existing white-box watermarks without prior knowledge of the watermarking scheme or training data. This paper analyzes and extends the attack scenarios of LFEA to the commonly employed black-box settings for LMs by considering Last-Layer outputs (dubbed LL-LFEA). We discover that the null space of the output matrix remains invariant against LL-LFEA attacks. Based on this finding, we propose NSmark, a black-box watermarking scheme that is task-agnostic and capable of resisting LL-LFEA attacks. NSmark consists of three phases: (i) watermark generation using the digital signature of the owner, enhanced by spread spectrum modulation for increased robustness; (ii) watermark embedding through an output mapping extractor that preserves the LM performance while maximizing watermark capacity; (iii) watermark verification, assessed by extraction rate and null space conformity. Extensive experiments on both pre-training and downstream tasks confirm the effectiveness, scalability, reliability, fidelity, and robustness of our approach. Code is available at https://github.com/dongdongzhaoUP/NSmark.
<div id='section'>Paperid: <span id='pid'>98, <a href='https://arxiv.org/pdf/2410.07369.pdf' target='_blank'>https://arxiv.org/pdf/2410.07369.pdf</a></span>   <span><a href='https://github.com/XuandongZhao/PRC-Watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Gunn, Xuandong Zhao, Dawn Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.07369">An Undetectable Watermark for Generative Image Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the first undetectable watermarking scheme for generative image models. Undetectability ensures that no efficient adversary can distinguish between watermarked and un-watermarked images, even after making many adaptive queries. In particular, an undetectable watermark does not degrade image quality under any efficiently computable metric. Our scheme works by selecting the initial latents of a diffusion model using a pseudorandom error-correcting code (Christ and Gunn, 2024), a strategy which guarantees undetectability and robustness. We experimentally demonstrate that our watermarks are quality-preserving and robust using Stable Diffusion 2.1. Our experiments verify that, in contrast to every prior scheme we tested, our watermark does not degrade image quality. Our experiments also demonstrate robustness: existing watermark removal attacks fail to remove our watermark from images without significantly degrading the quality of the images. Finally, we find that we can robustly encode 512 bits in our watermark, and up to 2500 bits when the images are not subjected to watermark removal attacks. Our code is available at https://github.com/XuandongZhao/PRC-Watermark.
<div id='section'>Paperid: <span id='pid'>99, <a href='https://arxiv.org/pdf/2410.05470.pdf' target='_blank'>https://arxiv.org/pdf/2410.05470.pdf</a></span>   <span><a href='https://github.com/yepengliu/CtrlRegen' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, Yuheng Bu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.05470">Image Watermarks are Removable Using Controllable Regeneration from Clean Noise</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image watermark techniques provide an effective way to assert ownership, deter misuse, and trace content sources, which has become increasingly essential in the era of large generative models. A critical attribute of watermark techniques is their robustness against various manipulations. In this paper, we introduce a watermark removal approach capable of effectively nullifying state-of-the-art watermarking techniques. Our primary insight involves regenerating the watermarked image starting from a clean Gaussian noise via a controllable diffusion model, utilizing the extracted semantic and spatial features from the watermarked image. The semantic control adapter and the spatial control network are specifically trained to control the denoising process towards ensuring image quality and enhancing consistency between the cleaned image and the original watermarked image. To achieve a smooth trade-off between watermark removal performance and image consistency, we further propose an adjustable and controllable regeneration scheme. This scheme adds varying numbers of noise steps to the latent representation of the watermarked image, followed by a controlled denoising process starting from this noisy latent representation. As the number of noise steps increases, the latent representation progressively approaches clean Gaussian noise, facilitating the desired trade-off. We apply our watermark removal methods across various watermarking techniques, and the results demonstrate that our methods offer superior visual consistency/quality and enhanced watermark removal performance compared to existing regeneration approaches. Our code is available at https://github.com/yepengliu/CtrlRegen.
<div id='section'>Paperid: <span id='pid'>100, <a href='https://arxiv.org/pdf/2410.03600.pdf' target='_blank'>https://arxiv.org/pdf/2410.03600.pdf</a></span>   <span><a href='https://github.com/XuandongZhao/llm-watermark-location' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuandong Zhao, Chenwen Liao, Yu-Xiang Wang, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03600">Efficiently Identifying Watermarked Segments in Mixed-Source Texts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarks in large language models (LLMs) are increasingly used to detect synthetic text, mitigating misuse cases like fake news and academic dishonesty. While existing watermarking detection techniques primarily focus on classifying entire documents as watermarked or not, they often neglect the common scenario of identifying individual watermark segments within longer, mixed-source documents. Drawing inspiration from plagiarism detection systems, we propose two novel methods for partial watermark detection. First, we develop a geometry cover detection framework aimed at determining whether there is a watermark segment in long text. Second, we introduce an adaptive online learning algorithm to pinpoint the precise location of watermark segments within the text. Evaluated on three popular watermarking techniques (KGW-Watermark, Unigram-Watermark, and Gumbel-Watermark), our approach achieves high accuracy, significantly outperforming baseline methods. Moreover, our framework is adaptable to other watermarking techniques, offering new insights for precise watermark detection. Our code is publicly available at https://github.com/XuandongZhao/llm-watermark-location
<div id='section'>Paperid: <span id='pid'>101, <a href='https://arxiv.org/pdf/2410.03168.pdf' target='_blank'>https://arxiv.org/pdf/2410.03168.pdf</a></span>   <span><a href='https://github.com/THU-BPM/Watermarked_LLM_Identification' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aiwei Liu, Sheng Guan, Yiming Liu, Leyi Pan, Yifei Zhang, Liancheng Fang, Lijie Wen, Philip S. Yu, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.03168">Can Watermarked LLMs be Identified by Users via Crafted Prompts?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking for Large Language Models (LLMs) has made significant progress in detecting LLM outputs and preventing misuse. Current watermarking techniques offer high detectability, minimal impact on text quality, and robustness to text editing. However, current researches lack investigation into the imperceptibility of watermarking techniques in LLM services. This is crucial as LLM providers may not want to disclose the presence of watermarks in real-world scenarios, as it could reduce user willingness to use the service and make watermarks more vulnerable to attacks. This work is the first to investigate the imperceptibility of watermarked LLMs. We design an identification algorithm called Water-Probe that detects watermarks through well-designed prompts to the LLM. Our key motivation is that current watermarked LLMs expose consistent biases under the same watermark key, resulting in similar differences across prompts under different watermark keys. Experiments show that almost all mainstream watermarking algorithms are easily identified with our well-designed prompts, while Water-Probe demonstrates a minimal false positive rate for non-watermarked LLMs. Finally, we propose that the key to enhancing the imperceptibility of watermarked LLMs is to increase the randomness of watermark key selection. Based on this, we introduce the Water-Bag strategy, which significantly improves watermark imperceptibility by merging multiple watermark keys.
<div id='section'>Paperid: <span id='pid'>102, <a href='https://arxiv.org/pdf/2410.02693.pdf' target='_blank'>https://arxiv.org/pdf/2410.02693.pdf</a></span>   <span><a href='https://github.com/eth-sri/watermark-spoofing-detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Thibaud Gloaguen, Nikola JovanoviÄ, Robin Staab, Martin Vechev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02693">Discovering Spoofing Attempts on Language Model Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM watermarks stand out as a promising way to attribute ownership of LLM-generated text. One threat to watermark credibility comes from spoofing attacks, where an unauthorized third party forges the watermark, enabling it to falsely attribute arbitrary texts to a particular LLM. Despite recent work demonstrating that state-of-the-art schemes are, in fact, vulnerable to spoofing, no prior work has focused on post-hoc methods to discover spoofing attempts. In this work, we for the first time propose a reliable statistical method to distinguish spoofed from genuinely watermarked text, suggesting that current spoofing attacks are less effective than previously thought. In particular, we show that regardless of their underlying approach, all current learning-based spoofing methods consistently leave observable artifacts in spoofed texts, indicative of watermark forgery. We build upon these findings to propose rigorous statistical tests that reliably reveal the presence of such artifacts and thus demonstrate that a watermark has been spoofed. Our experimental evaluation shows high test power across all learning-based spoofing methods, providing insights into their fundamental limitations and suggesting a way to mitigate this threat. We make all our code available at https://github.com/eth-sri/watermark-spoofing-detection .
<div id='section'>Paperid: <span id='pid'>103, <a href='https://arxiv.org/pdf/2410.02440.pdf' target='_blank'>https://arxiv.org/pdf/2410.02440.pdf</a></span>   <span><a href='https://github.com/nilslukas/ada-wm-evasion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdulrahman Diaa, Toluwani Aremu, Nils Lukas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02440">Optimizing Adaptive Attacks against Watermarks for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) can be misused to spread unwanted content at scale. Content watermarking deters misuse by hiding messages in content, enabling its detection using a secret watermarking key. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content's quality. Many LLM watermarking methods have been proposed, but robustness is tested only against non-adaptive attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate watermark robustness as an objective function and use preference-based optimization to tune adaptive attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarks, (ii) training against any watermark succeeds in evading unseen watermarks, and (iii) optimization-based attacks are cost-effective. Our findings underscore the need to test robustness against adaptively tuned attacks. We release our adaptively optimized paraphrasers at https://github.com/nilslukas/ada-wm-evasion.
<div id='section'>Paperid: <span id='pid'>104, <a href='https://arxiv.org/pdf/2409.18211.pdf' target='_blank'>https://arxiv.org/pdf/2409.18211.pdf</a></span>   <span><a href='https://github.com/vkinakh/ssl-watermarking-attacks' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vitaliy Kinakh, Brian Pulfer, Yury Belousov, Pierre Fernandez, Teddy Furon, Slava Voloshynovskiy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18211">Evaluation of Security of ML-based Watermarking: Copy and Removal Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The vast amounts of digital content captured from the real world or AI-generated media necessitate methods for copyright protection, traceability, or data provenance verification. Digital watermarking serves as a crucial approach to address these challenges. Its evolution spans three generations: handcrafted, autoencoder-based, and foundation model based methods. While the robustness of these systems is well-documented, the security against adversarial attacks remains underexplored. This paper evaluates the security of foundation models' latent space digital watermarking systems that utilize adversarial embedding techniques. A series of experiments investigate the security dimensions under copy and removal attacks, providing empirical insights into these systems' vulnerabilities. All experimental codes and results are available at https://github.com/vkinakh/ssl-watermarking-attacks .
<div id='section'>Paperid: <span id='pid'>105, <a href='https://arxiv.org/pdf/2409.13222.pdf' target='_blank'>https://arxiv.org/pdf/2409.13222.pdf</a></span>   <span><a href='https://kuai-lab.github.io/cvpr20253dgsw/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngdong Jang, Hyunje Park, Feng Yang, Heeju Ko, Euijin Choo, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13222">3D-GSW: 3D Gaussian Splatting for Robust Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As 3D Gaussian Splatting (3D-GS) gains significant attention and its commercial usage increases, the need for watermarking technologies to prevent unauthorized use of the 3D-GS models and rendered images has become increasingly important. In this paper, we introduce a robust watermarking method for 3D-GS that secures copyright of both the model and its rendered images. Our proposed method remains robust against distortions in rendered images and model attacks while maintaining high rendering quality. To achieve these objectives, we present Frequency-Guided Densification (FGD), which removes 3D Gaussians based on their contribution to rendering quality, enhancing real-time rendering and the robustness of the message. FGD utilizes Discrete Fourier Transform to split 3D Gaussians in high-frequency areas, improving rendering quality. Furthermore, we employ a gradient mask for 3D Gaussians and design a wavelet-subband loss to enhance rendering quality. Our experiments show that our method embeds the message in the rendered images invisibly and robustly against various attacks, including model distortion. Our method achieves superior performance in both rendering quality and watermark robustness while improving real-time rendering efficiency. Project page: https://kuai-lab.github.io/cvpr20253dgsw/
<div id='section'>Paperid: <span id='pid'>106, <a href='https://arxiv.org/pdf/2409.05112.pdf' target='_blank'>https://arxiv.org/pdf/2409.05112.pdf</a></span>   <span><a href='https://github.com/THU-BPM/WaterSeeker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leyi Pan, Aiwei Liu, Yijian Lu, Zitian Gao, Yichen Di, Shiyu Huang, Lijie Wen, Irwin King, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.05112">WaterSeeker: Pioneering Efficient Detection of Watermarked Segments in Large Documents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking algorithms for large language models (LLMs) have attained high accuracy in detecting LLM-generated text. However, existing methods primarily focus on distinguishing fully watermarked text from non-watermarked text, overlooking real-world scenarios where LLMs generate only small sections within large documents. In this scenario, balancing time complexity and detection performance poses significant challenges. This paper presents WaterSeeker, a novel approach to efficiently detect and locate watermarked segments amid extensive natural text. It first applies an efficient anomaly extraction method to preliminarily locate suspicious watermarked regions. Following this, it conducts a local traversal and performs full-text detection for more precise verification. Theoretical analysis and experimental results demonstrate that WaterSeeker achieves a superior balance between detection accuracy and computational efficiency. Moreover, its localization capability lays the foundation for building interpretable AI detection systems. Our code is available at https://github.com/THU-BPM/WaterSeeker.
<div id='section'>Paperid: <span id='pid'>107, <a href='https://arxiv.org/pdf/2409.02718.pdf' target='_blank'>https://arxiv.org/pdf/2409.02718.pdf</a></span>   <span><a href='https://github.com/liangzid/LoRD-MEA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi Liang, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02718">"Yes, My LoRD." Guiding Language Model Extraction with Locality Reinforced Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model extraction attacks (MEAs) on large language models (LLMs) have received increasing attention in recent research. However, existing attack methods typically adapt the extraction strategies originally developed for deep neural networks (DNNs). They neglect the underlying inconsistency between the training tasks of MEA and LLM alignment, leading to suboptimal attack performance. To tackle this issue, we propose Locality Reinforced Distillation (LoRD), a novel model extraction algorithm specifically designed for LLMs. In particular, LoRD employs a newly defined policy-gradient-style training task that utilizes the responses of victim model as the signal to guide the crafting of preference for the local model. Theoretical analyses demonstrate that I) The convergence procedure of LoRD in model extraction is consistent with the alignment procedure of LLMs, and II) LoRD can reduce query complexity while mitigating watermark protection through our exploration-based stealing. Extensive experiments validate the superiority of our method in extracting various state-of-the-art commercial LLMs. Our code is available at: https://github.com/liangzid/LoRD-MEA .
<div id='section'>Paperid: <span id='pid'>108, <a href='https://arxiv.org/pdf/2408.08340.pdf' target='_blank'>https://arxiv.org/pdf/2408.08340.pdf</a></span>   <span><a href='https://github.com/deepvk/metr' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Varlamov, Daria Diatlova, Egor Spirin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.08340">METR: Image Watermarking with Large Number of Unique Messages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Improvements in diffusion models have boosted the quality of image generation, which has led researchers, companies, and creators to focus on improving watermarking algorithms. This provision would make it possible to clearly identify the creators of generative art. The main challenges that modern watermarking algorithms face have to do with their ability to withstand attacks and encrypt many unique messages, such as user IDs. In this paper, we present METR: Message Enhanced Tree-Ring, which is an approach that aims to address these challenges. METR is built on the Tree-Ring watermarking algorithm, a technique that makes it possible to encode multiple distinct messages without compromising attack resilience or image quality. This ensures the suitability of this watermarking algorithm for any Diffusion Model. In order to surpass the limitations on the quantity of encoded messages, we propose METR++, an enhanced version of METR. This approach, while limited to the Latent Diffusion Model architecture, is designed to inject a virtually unlimited number of unique messages. We demonstrate its robustness to attacks and ability to encrypt many unique messages while preserving image quality, which makes METR and METR++ hold great potential for practical applications in real-world settings. Our code is available at https://github.com/deepvk/metr
<div id='section'>Paperid: <span id='pid'>109, <a href='https://arxiv.org/pdf/2407.20544.pdf' target='_blank'>https://arxiv.org/pdf/2407.20544.pdf</a></span>   <span><a href='https://github.com/ruisizhang123/PD_WM_GNN' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruisi Zhang, Rachel Selina Rajarathnam, David Z. Pan, Farinaz Koushanfar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.20544">Automated Physical Design Watermarking Leveraging Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents AutoMarks, an automated and transferable watermarking framework that leverages graph neural networks to reduce the watermark search overheads during the placement stage. AutoMarks's novel automated watermark search is accomplished by (i) constructing novel graph and node features with physical, semantic, and design constraint-aware representation; (ii) designing a data-efficient sampling strategy for watermarking fidelity label collection; and (iii) leveraging a graph neural network to learn the connectivity between cells and predict the watermarking fidelity on unseen layouts. Extensive evaluations on ISPD'15 and ISPD'19 benchmarks demonstrate that our proposed automated methodology: (i) is capable of finding quality-preserving watermarks in a short time; and (ii) is transferable across various designs, i.e., AutoMarks trained on one layout is generalizable to other benchmark circuits. AutoMarks is also resilient against potential watermark removal and forging attacks
<div id='section'>Paperid: <span id='pid'>110, <a href='https://arxiv.org/pdf/2407.17417.pdf' target='_blank'>https://arxiv.org/pdf/2407.17417.pdf</a></span>   <span><a href='https://github.com/michael-panaitescu/watermark_copyright_aaai25' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael-Andrei Panaitescu-Liess, Zora Che, Bang An, Yuancheng Xu, Pankayaraj Pathmanathan, Souradip Chakraborty, Sicheng Zhu, Tom Goldstein, Furong Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.17417">Can Watermarking Large Language Models Prevent Copyrighted Text Generation and Hide Training Data?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated impressive capabilities in generating diverse and contextually rich text. However, concerns regarding copyright infringement arise as LLMs may inadvertently produce copyrighted material. In this paper, we first investigate the effectiveness of watermarking LLMs as a deterrent against the generation of copyrighted texts. Through theoretical analysis and empirical evaluation, we demonstrate that incorporating watermarks into LLMs significantly reduces the likelihood of generating copyrighted content, thereby addressing a critical concern in the deployment of LLMs. However, we also find that watermarking can have unintended consequences on Membership Inference Attacks (MIAs), which aim to discern whether a sample was part of the pretraining dataset and may be used to detect copyright violations. Surprisingly, we find that watermarking adversely affects the success rate of MIAs, complicating the task of detecting copyrighted text in the pretraining dataset. These results reveal the complex interplay between different regulatory measures, which may impact each other in unforeseen ways. Finally, we propose an adaptive technique to improve the success rate of a recent MIA under watermarking. Our findings underscore the importance of developing adaptive methods to study critical problems in LLMs with potential legal implications.
<div id='section'>Paperid: <span id='pid'>111, <a href='https://arxiv.org/pdf/2407.07735.pdf' target='_blank'>https://arxiv.org/pdf/2407.07735.pdf</a></span>   <span><a href='https://qsong2001.github.io/NeRFProtector' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Song, Ziyuan Luo, Ka Chun Cheung, Simon See, Renjie Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07735">Protecting NeRFs' Copyright via Plug-And-Play Watermarking Base Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Radiance Fields (NeRFs) have become a key method for 3D scene representation. With the rising prominence and influence of NeRF, safeguarding its intellectual property has become increasingly important. In this paper, we propose \textbf{NeRFProtector}, which adopts a plug-and-play strategy to protect NeRF's copyright during its creation. NeRFProtector utilizes a pre-trained watermarking base model, enabling NeRF creators to embed binary messages directly while creating their NeRF. Our plug-and-play property ensures NeRF creators can flexibly choose NeRF variants without excessive modifications. Leveraging our newly designed progressive distillation, we demonstrate performance on par with several leading-edge neural rendering methods. Our project is available at: \url{https://qsong2001.github.io/NeRFProtector}.
<div id='section'>Paperid: <span id='pid'>112, <a href='https://arxiv.org/pdf/2407.04411.pdf' target='_blank'>https://arxiv.org/pdf/2407.04411.pdf</a></span>   <span><a href='https://github.com/aoi3142/Waterfall' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Gregory Kang Ruey Lau, Xinyuan Niu, Hieu Dao, Jiangwei Chen, Chuan-Sheng Foo, Bryan Kian Hsiang Low
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04411">Waterfall: Framework for Robust and Scalable Text Watermarking and Provenance for LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protecting intellectual property (IP) of text such as articles and code is increasingly important, especially as sophisticated attacks become possible, such as paraphrasing by large language models (LLMs) or even unauthorized training of LLMs on copyrighted text to infringe such IP. However, existing text watermarking methods are not robust enough against such attacks nor scalable to millions of users for practical implementation. In this paper, we propose Waterfall, the first training-free framework for robust and scalable text watermarking applicable across multiple text types (e.g., articles, code) and languages supportable by LLMs, for general text and LLM data provenance. Waterfall comprises several key innovations, such as being the first to use LLM as paraphrasers for watermarking along with a novel combination of techniques that are surprisingly effective in achieving robust verifiability and scalability. We empirically demonstrate that Waterfall achieves significantly better scalability, robust verifiability, and computational efficiency compared to SOTA article-text watermarking methods, and showed how it could be directly applied to the watermarking of code. We also demonstrated that Waterfall can be used for LLM data provenance, where the watermarks of LLM training data can be detected in LLM output, allowing for detection of unauthorized use of data for LLM training and potentially enabling model-centric watermarking of open-sourced LLMs which has been a limitation of existing LLM watermarking works. Our code is available at https://github.com/aoi3142/Waterfall.
<div id='section'>Paperid: <span id='pid'>113, <a href='https://arxiv.org/pdf/2407.04086.pdf' target='_blank'>https://arxiv.org/pdf/2407.04086.pdf</a></span>   <span><a href='https://github.com/zhengyuan-jiang/Watermark-Library' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Jinyuan Jia, Neil Zhenqiang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04086">Certifiably Robust Image Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI raises many societal concerns such as boosting disinformation and propaganda campaigns. Watermarking AI-generated content is a key technology to address these concerns and has been widely deployed in industry. However, watermarking is vulnerable to removal attacks and forgery attacks. In this work, we propose the first image watermarks with certified robustness guarantees against removal and forgery attacks. Our method leverages randomized smoothing, a popular technique to build certifiably robust classifiers and regression models. Our major technical contributions include extending randomized smoothing to watermarking by considering its unique characteristics, deriving the certified robustness guarantees, and designing algorithms to estimate them. Moreover, we extensively evaluate our image watermarks in terms of both certified and empirical robustness. Our code is available at \url{https://github.com/zhengyuan-jiang/Watermark-Library}.
<div id='section'>Paperid: <span id='pid'>114, <a href='https://arxiv.org/pdf/2406.14517.pdf' target='_blank'>https://arxiv.org/pdf/2406.14517.pdf</a></span>   <span><a href='https://github.com/lilakk/PostMark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yapei Chang, Kalpesh Krishna, Amir Houmansadr, John Wieting, Mohit Iyyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14517">PostMark: A Robust Blackbox Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The most effective techniques to detect LLM-generated text rely on inserting a detectable signature -- or watermark -- during the model's decoding process. Most existing watermarking methods require access to the underlying LLM's logits, which LLM API providers are loath to share due to fears of model distillation. As such, these watermarks must be implemented independently by each LLM provider. In this paper, we develop PostMark, a modular post-hoc watermarking procedure in which an input-dependent set of words (determined via a semantic embedding) is inserted into the text after the decoding process has completed. Critically, PostMark does not require logit access, which means it can be implemented by a third party. We also show that PostMark is more robust to paraphrasing attacks than existing watermarking methods: our experiments cover eight baseline algorithms, five base LLMs, and three datasets. Finally, we evaluate the impact of PostMark on text quality using both automated and human assessments, highlighting the trade-off between quality and robustness to paraphrasing. We release our code, outputs, and annotations at https://github.com/lilakk/PostMark.
<div id='section'>Paperid: <span id='pid'>115, <a href='https://arxiv.org/pdf/2406.06979.pdf' target='_blank'>https://arxiv.org/pdf/2406.06979.pdf</a></span>   <span><a href='https://github.com/moyangkuo/AudioMarkBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbin Liu, Moyang Guo, Zhengyuan Jiang, Lun Wang, Neil Zhenqiang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.06979">AudioMarkBench: Benchmarking Robustness of Audio Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing realism of synthetic speech, driven by advancements in text-to-speech models, raises ethical concerns regarding impersonation and disinformation. Audio watermarking offers a promising solution via embedding human-imperceptible watermarks into AI-generated audios. However, the robustness of audio watermarking against common/adversarial perturbations remains understudied. We present AudioMarkBench, the first systematic benchmark for evaluating the robustness of audio watermarking against watermark removal and watermark forgery. AudioMarkBench includes a new dataset created from Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art watermarking methods, and 15 types of perturbations. We benchmark the robustness of these methods against the perturbations in no-box, black-box, and white-box settings. Our findings highlight the vulnerabilities of current watermarking techniques and emphasize the need for more robust and fair audio watermarking solutions. Our dataset and code are publicly available at https://github.com/moyangkuo/AudioMarkBench.
<div id='section'>Paperid: <span id='pid'>116, <a href='https://arxiv.org/pdf/2406.04840.pdf' target='_blank'>https://arxiv.org/pdf/2406.04840.pdf</a></span>   <span><a href='https://github.com/zjzser/TraceableSpeech' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junzuo Zhou, Jiangyan Yi, Tao Wang, Jianhua Tao, Ye Bai, Chu Yuan Zhang, Yong Ren, Zhengqi Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04840">TraceableSpeech: Towards Proactively Traceable Text-to-Speech with Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various threats posed by the progress in text-to-speech (TTS) have prompted the need to reliably trace synthesized speech. However, contemporary approaches to this task involve adding watermarks to the audio separately after generation, a process that hurts both speech quality and watermark imperceptibility. In addition, these approaches are limited in robustness and flexibility. To address these problems, we propose TraceableSpeech, a novel TTS model that directly generates watermarked speech, improving watermark imperceptibility and speech quality. Furthermore, We design the frame-wise imprinting and extraction of watermarks, achieving higher robustness against resplicing attacks and temporal flexibility in operation. Experimental results show that TraceableSpeech outperforms the strong baseline where VALL-E or HiFicodec individually uses WavMark in watermark imperceptibility, speech quality and resilience against resplicing attacks. It also can apply to speech of various durations. The code is avaliable at https://github.com/zjzser/TraceableSpeech
<div id='section'>Paperid: <span id='pid'>117, <a href='https://arxiv.org/pdf/2406.03728.pdf' target='_blank'>https://arxiv.org/pdf/2406.03728.pdf</a></span>   <span><a href='https://mmwatermark-robustness.github.io/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Jielin Qiu, William Han, Xuandong Zhao, Shangbang Long, Christos Faloutsos, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03728">Evaluating Durability: Benchmark Insights into Multimodal Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of large models, watermarks are increasingly employed to assert copyright, verify authenticity, or monitor content distribution. As applications become more multimodal, the utility of watermarking techniques becomes even more critical. The effectiveness and reliability of these watermarks largely depend on their robustness to various disturbances. However, the robustness of these watermarks in real-world scenarios, particularly under perturbations and corruption, is not well understood. To highlight the significance of robustness in watermarking techniques, our study evaluated the robustness of watermarked content generated by image and text generation models against common real-world image corruptions and text perturbations. Our results could pave the way for the development of more robust watermarking techniques in the future. Our project website can be found at \url{https://mmwatermark-robustness.github.io/}.
<div id='section'>Paperid: <span id='pid'>118, <a href='https://arxiv.org/pdf/2406.02836.pdf' target='_blank'>https://arxiv.org/pdf/2406.02836.pdf</a></span>   <span><a href='https://github.com/mehrdadsaberi/DREW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehrdad Saberi, Vinu Sankar Sadasivan, Arman Zarei, Hessam Mahdavifar, Soheil Feizi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02836">DREW : Towards Robust Data Provenance by Leveraging Error-Controlled Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Identifying the origin of data is crucial for data provenance, with applications including data ownership protection, media forensics, and detecting AI-generated content. A standard approach involves embedding-based retrieval techniques that match query data with entries in a reference dataset. However, this method is not robust against benign and malicious edits. To address this, we propose Data Retrieval with Error-corrected codes and Watermarking (DREW). DREW randomly clusters the reference dataset, injects unique error-controlled watermark keys into each cluster, and uses these keys at query time to identify the appropriate cluster for a given sample. After locating the relevant cluster, embedding vector similarity retrieval is performed within the cluster to find the most accurate matches. The integration of error control codes (ECC) ensures reliable cluster assignments, enabling the method to perform retrieval on the entire dataset in case the ECC algorithm cannot detect the correct cluster with high confidence. This makes DREW maintain baseline performance, while also providing opportunities for performance improvements due to the increased likelihood of correctly matching queries to their origin when performing retrieval on a smaller subset of the dataset. Depending on the watermark technique used, DREW can provide substantial improvements in retrieval accuracy (up to 40\% for some datasets and modification types) across multiple datasets and state-of-the-art embedding models (e.g., DinoV2, CLIP), making our method a promising solution for secure and reliable source identification. The code is available at https://github.com/mehrdadsaberi/DREW
<div id='section'>Paperid: <span id='pid'>119, <a href='https://arxiv.org/pdf/2406.01946.pdf' target='_blank'>https://arxiv.org/pdf/2406.01946.pdf</a></span>   <span><a href='https://github.com/Tongzhou0101/Bileve-official' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zhou, Xuandong Zhao, Xiaolin Xu, Shaolei Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.01946">Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability. Code is available at https://github.com/Tongzhou0101/Bileve-official.
<div id='section'>Paperid: <span id='pid'>120, <a href='https://arxiv.org/pdf/2406.00816.pdf' target='_blank'>https://arxiv.org/pdf/2406.00816.pdf</a></span>   <span><a href='https://github.com/invisibleTriggerDiffusion/invisible_triggers_for_diffusion' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/invisibleTriggerDiffusion/invisible_triggers_for_diffusion' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Li, Junchi Ma, Minhao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.00816">Invisible Backdoor Attacks on Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, diffusion models have achieved remarkable success in the realm of high-quality image generation, garnering increased attention. This surge in interest is paralleled by a growing concern over the security threats associated with diffusion models, largely attributed to their susceptibility to malicious exploitation. Notably, recent research has brought to light the vulnerability of diffusion models to backdoor attacks, enabling the generation of specific target images through corresponding triggers. However, prevailing backdoor attack methods rely on manually crafted trigger generation functions, often manifesting as discernible patterns incorporated into input noise, thus rendering them susceptible to human detection. In this paper, we present an innovative and versatile optimization framework designed to acquire invisible triggers, enhancing the stealthiness and resilience of inserted backdoors. Our proposed framework is applicable to both unconditional and conditional diffusion models, and notably, we are the pioneers in demonstrating the backdooring of diffusion models within the context of text-guided image editing and inpainting pipelines. Moreover, we also show that the backdoors in the conditional generation can be directly applied to model watermarking for model ownership verification, which further boosts the significance of the proposed framework. Extensive experiments on various commonly used samplers and datasets verify the efficacy and stealthiness of the proposed framework. Our code is publicly available at https://github.com/invisibleTriggerDiffusion/invisible_triggers_for_diffusion.
<div id='section'>Paperid: <span id='pid'>121, <a href='https://arxiv.org/pdf/2405.13360.pdf' target='_blank'>https://arxiv.org/pdf/2405.13360.pdf</a></span>   <span><a href='https://github.com/ZhentingWang/LatentTracer' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenting Wang, Vikash Sehwag, Chen Chen, Lingjuan Lyu, Dimitris N. Metaxas, Shiqing Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13360">How to Trace Latent Generative Model Generated Images without Artificial Watermark?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent generative models (e.g., Stable Diffusion) have become more and more popular, but concerns have arisen regarding potential misuse related to images generated by these models. It is, therefore, necessary to analyze the origin of images by inferring if a particular image was generated by a specific latent generative model. Most existing methods (e.g., image watermark and model fingerprinting) require extra steps during training or generation. These requirements restrict their usage on the generated images without such extra operations, and the extra required operations might compromise the quality of the generated images. In this work, we ask whether it is possible to effectively and efficiently trace the images generated by a specific latent generative model without the aforementioned requirements. To study this problem, we design a latent inversion based method called LatentTracer to trace the generated images of the inspected model by checking if the examined images can be well-reconstructed with an inverted latent input. We leverage gradient based latent inversion and identify a encoder-based initialization critical to the success of our approach. Our experiments on the state-of-the-art latent generative models, such as Stable Diffusion, show that our method can distinguish the images generated by the inspected model and other images with a high accuracy and efficiency. Our findings suggest the intriguing possibility that today's latent generative generated images are naturally watermarked by the decoder used in the source models. Code: https://github.com/ZhentingWang/LatentTracer.
<div id='section'>Paperid: <span id='pid'>122, <a href='https://arxiv.org/pdf/2405.11135.pdf' target='_blank'>https://arxiv.org/pdf/2405.11135.pdf</a></span>   <span><a href='https://github.com/Georgefwt/AquaLoRA' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitao Feng, Wenbo Zhou, Jiyan He, Jie Zhang, Tianyi Wei, Guanlin Li, Tianwei Zhang, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11135">AquaLoRA: Toward White-box Protection for Customized Stable Diffusion Models via Watermark LoRA</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have achieved remarkable success in generating high-quality images. Recently, the open-source models represented by Stable Diffusion (SD) are thriving and are accessible for customization, giving rise to a vibrant community of creators and enthusiasts. However, the widespread availability of customized SD models has led to copyright concerns, like unauthorized model distribution and unconsented commercial use. To address it, recent works aim to let SD models output watermarked content for post-hoc forensics. Unfortunately, none of them can achieve the challenging white-box protection, wherein the malicious user can easily remove or replace the watermarking module to fail the subsequent verification. For this, we propose \texttt{\method} as the first implementation under this scenario. Briefly, we merge watermark information into the U-Net of Stable Diffusion Models via a watermark Low-Rank Adaptation (LoRA) module in a two-stage manner. For watermark LoRA module, we devise a scaling matrix to achieve flexible message updates without retraining. To guarantee fidelity, we design Prior Preserving Fine-Tuning (PPFT) to ensure watermark learning with minimal impacts on model distribution, validated by proofs. Finally, we conduct extensive experiments and ablation studies to verify our design.
<div id='section'>Paperid: <span id='pid'>123, <a href='https://arxiv.org/pdf/2405.10051.pdf' target='_blank'>https://arxiv.org/pdf/2405.10051.pdf</a></span>   <span><a href='https://github.com/THU-BPM/MarkLLM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Leyi Pan, Aiwei Liu, Zhiwei He, Zitian Gao, Xuandong Zhao, Yijian Lu, Binglin Zhou, Shuliang Liu, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.10051">MarkLLM: An Open-Source Toolkit for LLM Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM watermarking, which embeds imperceptible yet algorithmically detectable signals in model outputs to identify LLM-generated text, has become crucial in mitigating the potential misuse of large language models. However, the abundance of LLM watermarking algorithms, their intricate mechanisms, and the complex evaluation procedures and perspectives pose challenges for researchers and the community to easily experiment with, understand, and assess the latest advancements. To address these issues, we introduce MarkLLM, an open-source toolkit for LLM watermarking. MarkLLM offers a unified and extensible framework for implementing LLM watermarking algorithms, while providing user-friendly interfaces to ensure ease of access. Furthermore, it enhances understanding by supporting automatic visualization of the underlying mechanisms of these algorithms. For evaluation, MarkLLM offers a comprehensive suite of 12 tools spanning three perspectives, along with two types of automated evaluation pipelines. Through MarkLLM, we aim to support researchers while improving the comprehension and involvement of the general public in LLM watermarking technology, fostering consensus and driving further advancements in research and application. Our code is available at https://github.com/THU-BPM/MarkLLM.
<div id='section'>Paperid: <span id='pid'>124, <a href='https://arxiv.org/pdf/2404.14055.pdf' target='_blank'>https://arxiv.org/pdf/2404.14055.pdf</a></span>   <span><a href='https://github.com/showlab/RingID' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Ci, Pei Yang, Yiren Song, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14055">RingID: Rethinking Tree-Ring Watermarking for Enhanced Multi-Key Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We revisit Tree-Ring Watermarking, a recent diffusion model watermarking method that demonstrates great robustness to various attacks. We conduct an in-depth study on it and reveal that the distribution shift unintentionally introduced by the watermarking process, apart from watermark pattern matching, contributes to its exceptional robustness. Our investigation further exposes inherent flaws in its original design, particularly in its ability to identify multiple distinct keys, where distribution shift offers no assistance. Based on these findings and analysis, we present RingID for enhanced multi-key identification. It consists of a novel multi-channel heterogeneous watermarking approach designed to seamlessly amalgamate distinctive advantages from diverse watermarks. Coupled with a series of suggested enhancements, RingID exhibits substantial advancements in multi-key identification. Github Page: https://github.com/showlab/RingID
<div id='section'>Paperid: <span id='pid'>125, <a href='https://arxiv.org/pdf/2404.05188.pdf' target='_blank'>https://arxiv.org/pdf/2404.05188.pdf</a></span>   <span><a href='https://github.com/ThuCCSLab/MergeGuard' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianshuo Cong, Delong Ran, Zesen Liu, Xinlei He, Jinyuan Liu, Yichen Gong, Qi Li, Anyu Wang, Xiaoyun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05188">Have You Merged My Model? On The Robustness of Large Language Model IP Protection Methods Against Model Merging</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model merging is a promising lightweight model empowerment technique that does not rely on expensive computing devices (e.g., GPUs) or require the collection of specific training data. Instead, it involves editing different upstream model parameters to absorb their downstream task capabilities. However, uncertified model merging can infringe upon the Intellectual Property (IP) rights of the original upstream models. In this paper, we conduct the first study on the robustness of IP protection methods under model merging scenarios. Specifically, we investigate two state-of-the-art IP protection techniques: Quantization Watermarking and Instructional Fingerprint, along with various advanced model merging technologies, such as Task Arithmetic, TIES-MERGING, and so on. Experimental results indicate that current Large Language Model (LLM) watermarking techniques cannot survive in the merged models, whereas model fingerprinting techniques can. Our research aims to highlight that model merging should be an indispensable consideration in the robustness assessment of model IP protection techniques, thereby promoting the healthy development of the open-source LLM community. Our code is available at https://github.com/ThuCCSLab/MergeGuard.
<div id='section'>Paperid: <span id='pid'>126, <a href='https://arxiv.org/pdf/2404.00230.pdf' target='_blank'>https://arxiv.org/pdf/2404.00230.pdf</a></span>   <span><a href='https://github.com/RichardSunnyMeng/LatentWatermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheling Meng, Bo Peng, Jing Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00230">Latent Watermark: Inject and Detect Watermarks in Latent Diffusion Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is a tool for actively identifying and attributing the images generated by latent diffusion models. Existing methods face the dilemma of image quality and watermark robustness. Watermarks with superior image quality usually have inferior robustness against attacks such as blurring and JPEG compression, while watermarks with superior robustness usually significantly damage image quality. This dilemma stems from the traditional paradigm where watermarks are injected and detected in pixel space, relying on pixel perturbation for watermark detection and resilience against attacks. In this paper, we highlight that an effective solution to the problem is to both inject and detect watermarks in the latent diffusion space, and propose Latent Watermark with a progressive training strategy. It weakens the direct connection between quality and robustness and thus alleviates their contradiction. We conduct evaluations on two datasets and against 10 watermark attacks. Six metrics measure the image quality and watermark robustness. Results show that compared to the recently proposed methods such as StableSignature, StegaStamp, RoSteALS, LaWa, TreeRing, and DiffuseTrace, LW not only surpasses them in terms of robustness but also offers superior image quality. Our code will be available at https://github.com/RichardSunnyMeng/LatentWatermark.
<div id='section'>Paperid: <span id='pid'>127, <a href='https://arxiv.org/pdf/2403.17983.pdf' target='_blank'>https://arxiv.org/pdf/2403.17983.pdf</a></span>   <span><a href='https://github.com/uiuc-arc/llm-code-watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tarun Suresh, Shubham Ugare, Gagandeep Singh, Sasa Misailovic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.17983">Is The Watermarking Of LLM-Generated Code Robust?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present the first in depth study on the robustness of existing watermarking techniques applied to code generated by large language models (LLMs). As LLMs increasingly contribute to software development, watermarking has emerged as a potential solution for detecting AI generated code and mitigating misuse, such as plagiarism or the automated generation of malicious programs. While previous research has demonstrated the resilience of watermarking in the text setting, our work reveals that watermarking techniques are significantly more fragile in code-based contexts. Specifically, we show that simple semantic-preserving transformations, such as variable renaming and dead code insertion, can effectively erase watermarks without altering the program's functionality. To systematically evaluate watermark robustness, we develop an algorithm that traverses the Abstract Syntax Tree (AST) of a watermarked program and applies a sequence of randomized, semantics-preserving transformations. Our experimental results, conducted on Python code generated by different LLMs, indicate that even minor modifications can drastically reduce watermark detectability, with true positive rates (TPR) dropping below 50% in many cases. Our code is publicly available at https://github.com/uiuc-arc/llm-code-watermark.
<div id='section'>Paperid: <span id='pid'>128, <a href='https://arxiv.org/pdf/2403.15365.pdf' target='_blank'>https://arxiv.org/pdf/2403.15365.pdf</a></span>   <span><a href='https://github.com/hifi-hyp/Watermark-Transfer-Attack' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Zhenqiang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15365">A Transfer Attack to Image Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector based on existing watermarking methods is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API. Our code is available at: https://github.com/hifi-hyp/Watermark-Transfer-Attack.
<div id='section'>Paperid: <span id='pid'>129, <a href='https://arxiv.org/pdf/2403.13485.pdf' target='_blank'>https://arxiv.org/pdf/2403.13485.pdf</a></span>   <span><a href='https://github.com/THU-BPM/MarkLLM' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/luyijian3/EWD' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, Irwin King
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13485">An Entropy-based Text Watermarking Detection Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking algorithms for large language models (LLMs) can effectively identify machine-generated texts by embedding and detecting hidden features in the text. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we opine that the influence of token entropy should be fully considered in the watermark detection process, $i.e.$, the weight of each token during watermark detection should be customized according to its entropy, rather than setting the weights of all tokens to the same value as in previous methods. Specifically, we propose \textbf{E}ntropy-based Text \textbf{W}atermarking \textbf{D}etection (\textbf{EWD}) that gives higher-entropy tokens higher influence weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free and fully automated. From the experiments, we demonstrate that our EWD can achieve better detection performance in low-entropy scenarios, and our method is also general and can be applied to texts with different entropy distributions. Our code and data is available\footnote{\url{https://github.com/luyijian3/EWD}}. Additionally, our algorithm could be accessed through MarkLLM \cite{pan2024markllm}\footnote{\url{https://github.com/THU-BPM/MarkLLM}}.
<div id='section'>Paperid: <span id='pid'>130, <a href='https://arxiv.org/pdf/2403.10663.pdf' target='_blank'>https://arxiv.org/pdf/2403.10663.pdf</a></span>   <span><a href='https://github.com/liyuxuan-github/MAT' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/liyuxuan-github/MAT' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxuan Li, Sarthak Kumar Maharana, Yunhui Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10663">Not Just Change the Labels, Learn the Features: Watermarking Deep Neural Networks with Multi-View Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing prevalence of Machine Learning as a Service (MLaaS) platforms, there is a growing focus on deep neural network (DNN) watermarking techniques. These methods are used to facilitate the verification of ownership for a target DNN model to protect intellectual property. One of the most widely employed watermarking techniques involves embedding a trigger set into the source model. Unfortunately, existing methodologies based on trigger sets are still susceptible to functionality-stealing attacks, potentially enabling adversaries to steal the functionality of the source model without a reliable means of verifying ownership. In this paper, we first introduce a novel perspective on trigger set-based watermarking methods from a feature learning perspective. Specifically, we demonstrate that by selecting data exhibiting multiple features, also referred to as \emph{multi-view data}, it becomes feasible to effectively defend functionality stealing attacks. Based on this perspective, we introduce a novel watermarking technique based on Multi-view dATa, called MAT, for efficiently embedding watermarks within DNNs. This approach involves constructing a trigger set with multi-view data and incorporating a simple feature-based regularization method for training the source model. We validate our method across various benchmarks and demonstrate its efficacy in defending against model extraction attacks, surpassing relevant baselines by a significant margin. The code is available at: \href{https://github.com/liyuxuan-github/MAT}{https://github.com/liyuxuan-github/MAT}.
<div id='section'>Paperid: <span id='pid'>131, <a href='https://arxiv.org/pdf/2403.10553.pdf' target='_blank'>https://arxiv.org/pdf/2403.10553.pdf</a></span>   <span><a href='https://github.com/xiaojunxu/learning-to-watermark-llm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaojun Xu, Yuanshun Yao, Yang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10553">Learning to Watermark LLM-generated Text via Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse. Unlike the current mainstream methods that work with a fixed LLM, we expand the watermark design space by including the LLM tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the LLM weights, and such signals can be detected by a paired detector. We propose a co-training framework based on reinforcement learning that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the LLM to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks). It also allows watermarked model open-sourcing. In addition, if used together with alignment, the extra overhead introduced is low - only training an extra reward model (i.e. our detector). We hope our work can bring more effort into studying a broader watermark design that is not limited to working with a fixed LLM. We open-source the code: https://github.com/xiaojunxu/learning-to-watermark-llm .
<div id='section'>Paperid: <span id='pid'>132, <a href='https://arxiv.org/pdf/2403.04808.pdf' target='_blank'>https://arxiv.org/pdf/2403.04808.pdf</a></span>   <span><a href='https://github.com/eva-giboulot/WaterMax' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Eva Giboulot, Teddy Furon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.04808">WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is a technical means to dissuade malfeasant usage of Large Language Models. This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM. Its new design leaves the LLM untouched (no modification of the weights, logits, temperature, or sampling technique). WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness. Its performance is both theoretically proven and experimentally validated. It outperforms all the SotA techniques under the most complete benchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.
<div id='section'>Paperid: <span id='pid'>133, <a href='https://arxiv.org/pdf/2403.02211.pdf' target='_blank'>https://arxiv.org/pdf/2403.02211.pdf</a></span>   <span><a href='https://github.com/hellloxiaotian/PSLNet' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunwei Tian, Menghua Zheng, Bo Li, Yanning Zhang, Shichao Zhang, David Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.02211">Perceptive self-supervised learning network for noisy image watermark removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Popular methods usually use a degradation model in a supervised way to learn a watermark removal model. However, it is true that reference images are difficult to obtain in the real world, as well as collected images by cameras suffer from noise. To overcome these drawbacks, we propose a perceptive self-supervised learning network for noisy image watermark removal (PSLNet) in this paper. PSLNet depends on a parallel network to remove noise and watermarks. The upper network uses task decomposition ideas to remove noise and watermarks in sequence. The lower network utilizes the degradation model idea to simultaneously remove noise and watermarks. Specifically, mentioned paired watermark images are obtained in a self supervised way, and paired noisy images (i.e., noisy and reference images) are obtained in a supervised way. To enhance the clarity of obtained images, interacting two sub-networks and fusing obtained clean images are used to improve the effects of image watermark removal in terms of structural information and pixel enhancement. Taking into texture information account, a mixed loss uses obtained images and features to achieve a robust model of noisy image watermark removal. Comprehensive experiments show that our proposed method is very effective in comparison with popular convolutional neural networks (CNNs) for noisy image watermark removal. Codes can be obtained at https://github.com/hellloxiaotian/PSLNet.
<div id='section'>Paperid: <span id='pid'>134, <a href='https://arxiv.org/pdf/2402.18059.pdf' target='_blank'>https://arxiv.org/pdf/2402.18059.pdf</a></span>   <span><a href='https://github.com/mignonjia/TS_watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, Farinaz Koushanfar, Pengtao Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.18059">Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark.
<div id='section'>Paperid: <span id='pid'>135, <a href='https://arxiv.org/pdf/2402.14904.pdf' target='_blank'>https://arxiv.org/pdf/2402.14904.pdf</a></span>   <span><a href='https://github.com/facebookresearch/radioactive-watermark' target='_blank'>  GitHub</a></span> <span><a href='https://pierrefdz.github.io/publications/radioactive/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Sander, Pierre Fernandez, Alain Durmus, Matthijs Douze, Teddy Furon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14904">Watermarking Makes Language Models Radioactive</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We investigate the radioactivity of text generated by large language models (LLM), i.e. whether it is possible to detect that such synthetic input was used to train a subsequent LLM. Current methods like membership inference or active IP protection either work only in settings where the suspected text is known or do not provide reliable statistical guarantees. We discover that, on the contrary, it is possible to reliably determine if a language model was trained on synthetic data if that data is output by a watermarked LLM. Our new methods, specialized for radioactivity, detects with a provable confidence weak residuals of the watermark signal in the fine-tuned LLM. We link the radioactivity contamination level to the following properties: the watermark robustness, its proportion in the training set, and the fine-tuning process. For instance, if the suspect model is open-weight, we demonstrate that training on watermarked instructions can be detected with high confidence ($p$-value $< 10^{-5}$) even when as little as $5\%$ of training text is watermarked.
<div id='section'>Paperid: <span id='pid'>136, <a href='https://arxiv.org/pdf/2402.14007.pdf' target='_blank'>https://arxiv.org/pdf/2402.14007.pdf</a></span>   <span><a href='https://github.com/zwhe99/X-SIR' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14007">Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of cross-lingual consistency in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks, decreasing the AUCs to a random-guessing level without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose X-SIR as a defense method against CWRA. Code: https://github.com/zwhe99/X-SIR.
<div id='section'>Paperid: <span id='pid'>137, <a href='https://arxiv.org/pdf/2402.12688.pdf' target='_blank'>https://arxiv.org/pdf/2402.12688.pdf</a></span>   <span><a href='https://github.com/hurunyi/Robust-Wide' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyi Hu, Jie Zhang, Ting Xu, Jiwei Li, Tianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12688">Robust-Wide: Robust Watermarking against Instruction-driven Image Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Instruction-driven image editing allows users to quickly edit an image according to text instructions in a forward pass. Nevertheless, malicious users can easily exploit this technique to create fake images, which could cause a crisis of trust and harm the rights of the original image owners. Watermarking is a common solution to trace such malicious behavior. Unfortunately, instruction-driven image editing can significantly change the watermarked image at the semantic level, making current state-of-the-art watermarking methods ineffective. To remedy it, we propose Robust-Wide, the first robust watermarking methodology against instruction-driven image editing. Specifically, we follow the classic structure of deep robust watermarking, consisting of the encoder, noise layer, and decoder. To achieve robustness against semantic distortions, we introduce a novel Partial Instruction-driven Denoising Sampling Guidance (PIDSG) module, which consists of a large variety of instruction injections and substantial modifications of images at different semantic levels. With PIDSG, the encoder tends to embed the watermark into more robust and semantic-aware areas, which remains in existence even after severe image editing. Experiments demonstrate that Robust-Wide can effectively extract the watermark from the edited image with a low bit error rate of nearly 2.6% for 64-bit watermark messages. Meanwhile, it only induces a neglectable influence on the visual quality and editability of the original images. Moreover, Robust-Wide holds general robustness against different sampling configurations and other popular image editing methods such as ControlNet-InstructPix2Pix, MagicBrush, Inpainting, and DDIM Inversion. Codes and models are available at https://github.com/hurunyi/Robust-Wide.
<div id='section'>Paperid: <span id='pid'>138, <a href='https://arxiv.org/pdf/2402.05864.pdf' target='_blank'>https://arxiv.org/pdf/2402.05864.pdf</a></span>   <span><a href='https://github.com/XuandongZhao/pf-decoding' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuandong Zhao, Lei Li, Yu-Xiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05864">Permute-and-Flip: An optimally stable and watermarkable decoder for LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys stability properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-stability tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson (2023)'s Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and its Gumbel watermarked counterpart) in terms of perplexity, while retaining the same stability (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding
<div id='section'>Paperid: <span id='pid'>139, <a href='https://arxiv.org/pdf/2401.17264.pdf' target='_blank'>https://arxiv.org/pdf/2401.17264.pdf</a></span>   <span><a href='https://pierrefdz.github.io/publications/audioseal/' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/facebookresearch/audioseal' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin San Roman, Pierre Fernandez, Alexandre DÃ©fossez, Teddy Furon, Tuan Tran, Hady Elsahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.17264">Proactive Detection of Voice Cloning with Localized Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning. We present AudioSeal, the first audio watermarking technique designed specifically for localized detection of AI-generated speech. AudioSeal employs a generator/detector architecture trained jointly with a localization loss to enable localized watermark detection up to the sample level, and a novel perceptual loss inspired by auditory masking, that enables AudioSeal to achieve better imperceptibility. AudioSeal achieves state-of-the-art performance in terms of robustness to real life audio manipulations and imperceptibility based on automatic and human evaluation metrics. Additionally, AudioSeal is designed with a fast, single-pass detector, that significantly surpasses existing models in speed - achieving detection up to two orders of magnitude faster, making it ideal for large-scale and real-time applications.
<div id='section'>Paperid: <span id='pid'>140, <a href='https://arxiv.org/pdf/2401.04247.pdf' target='_blank'>https://arxiv.org/pdf/2401.04247.pdf</a></span>   <span><a href='https://github.com/zhanglijun95/ZoDiac' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijun Zhang, Xiao Liu, Antoni Viros Martin, Cindy Xiong Bearfield, Yuriy Brun, Hui Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.04247">Attack-Resilient Image Watermarking Using Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking images is critical for tracking image provenance and proving ownership. With the advent of generative models, such as stable diffusion, that can create fake but realistic images, watermarking has become particularly important to make human-created images reliably identifiable. Unfortunately, the very same stable diffusion technology can remove watermarks injected using existing methods. To address this problem, we present ZoDiac, which uses a pre-trained stable diffusion model to inject a watermark into the trainable latent space, resulting in watermarks that can be reliably detected in the latent vector even when attacked. We evaluate ZoDiac on three benchmarks, MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against state-of-the-art watermark attacks, with a watermark detection rate above 98% and a false positive rate below 6.4%, outperforming state-of-the-art watermarking methods. We hypothesize that the reciprocating denoising process in diffusion models may inherently enhance the robustness of the watermark when faced with strong attacks and validate the hypothesis. Our research demonstrates that stable diffusion is a promising approach to robust watermarking, able to withstand even stable-diffusion--based attack methods. ZoDiac is open-sourced and available at https://github.com/zhanglijun95/ZoDiac.
<div id='section'>Paperid: <span id='pid'>141, <a href='https://arxiv.org/pdf/2312.08883.pdf' target='_blank'>https://arxiv.org/pdf/2312.08883.pdf</a></span>   <span><a href='https://xuanyuzhang21.github.io/project/editguard/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanyu Zhang, Runyi Li, Jiwen Yu, Youmin Xu, Weiqi Li, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.08883">EditGuard: Versatile Image Watermarking for Tamper Localization and Copyright Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era where AI-generated content (AIGC) models can produce stunning and lifelike images, the lingering shadow of unauthorized reproductions and malicious tampering poses imminent threats to copyright integrity and information security. Current image watermarking methods, while widely accepted for safeguarding visual content, can only protect copyright and ensure traceability. They fall short in localizing increasingly realistic image tampering, potentially leading to trust crises, privacy violations, and legal disputes. To solve this challenge, we propose an innovative proactive forensics framework EditGuard, to unify copyright protection and tamper-agnostic localization, especially for AIGC-based editing methods. It can offer a meticulous embedding of imperceptible watermarks and precise decoding of tampered areas and copyright information. Leveraging our observed fragility and locality of image-into-image steganography, the realization of EditGuard can be converted into a united image-bit steganography issue, thus completely decoupling the training process from the tampering types. Extensive experiments demonstrate that our EditGuard balances the tamper localization accuracy, copyright recovery precision, and generalizability to various AIGC-based tampering methods, especially for image forgery that is difficult for the naked eye to detect. The project page is available at https://xuanyuzhang21.github.io/project/editguard/.
<div id='section'>Paperid: <span id='pid'>142, <a href='https://arxiv.org/pdf/2312.05187.pdf' target='_blank'>https://arxiv.org/pdf/2312.05187.pdf</a></span>   <span><a href='https://github.com/facebookresearch/seamless_communication' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Seamless Communication, LoÃ¯c Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, John Hoffman, Min-Jae Hwang, Hirofumi Inaguma, Christopher Klaiber, Ilia Kulikov, Pengwei Li, Daniel Licht, Jean Maillard, Ruslan Mavlyutov, Alice Rakotoarison, Kaushik Ram Sadagopan, Abinesh Ramakrishnan, Tuan Tran, Guillaume Wenzek, Yilin Yang, Ethan Ye, Ivan Evtimov, Pierre Fernandez, Cynthia Gao, Prangthip Hansanti, Elahe Kalbassi, Amanda Kallet, Artyom Kozhevnikov, Gabriel Mejia Gonzalez, Robin San Roman, Christophe Touret, Corinne Wong, Carleigh Wood, Bokai Yu, Pierre Andrews, Can Balioglu, Peng-Jen Chen, Marta R. Costa-jussÃ, Maha Elbayad, Hongyu Gong, Francisco GuzmÃ¡n, Kevin Heffernan, Somya Jain, Justine Kao, Ann Lee, Xutai Ma, Alex Mourachko, Benjamin Peloquin, Juan Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Anna Sun, Paden Tomasello, Changhan Wang, Jeff Wang, Skyler Wang, Mary Williamson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05187">Seamless: Multilingual Expressive and Streaming Speech Translation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model-SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. SeamlessM4T v2 provides the foundation on which our next two models are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one's voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. The contributions to this work are publicly released and accessible at https://github.com/facebookresearch/seamless_communication
<div id='section'>Paperid: <span id='pid'>143, <a href='https://arxiv.org/pdf/2312.03410.pdf' target='_blank'>https://arxiv.org/pdf/2312.03410.pdf</a></span>   <span><a href='https://timbrewatermarking.github.io/samples' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Jie Zhang, Tianwei Zhang, Xi Yang, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03410">Detecting Voice Cloning Attacks via Timbre Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, it is common to release audio content to the public. However, with the rise of voice cloning technology, attackers have the potential to easily impersonate a specific person by utilizing his publicly released audio without any permission. Therefore, it becomes significant to detect any potential misuse of the released audio content and protect its timbre from being impersonated. To this end, we introduce a novel concept, "Timbre Watermarking", which embeds watermark information into the target individual's speech, eventually defeating the voice cloning attacks. To ensure the watermark is robust to the voice cloning model's learning process, we design an end-to-end voice cloning-resistant detection framework. The core idea of our solution is to embed and extract the watermark in the frequency domain in a temporally invariant manner. To acquire generalization across different voice cloning attacks, we modulate their shared process and integrate it into our framework as a distortion layer. Experiments demonstrate that the proposed timbre watermarking can defend against different voice cloning attacks, exhibit strong resistance against various adaptive attacks (e.g., reconstruction-based removal attacks, watermark overwriting attacks), and achieve practicality in real-world services such as PaddleSpeech, Voice-Cloning-App, and so-vits-svc. In addition, ablation studies are also conducted to verify the effectiveness of our design. Some audio samples are available at https://timbrewatermarking.github.io/samples.
<div id='section'>Paperid: <span id='pid'>144, <a href='https://arxiv.org/pdf/2312.00273.pdf' target='_blank'>https://arxiv.org/pdf/2312.00273.pdf</a></span>   <span><a href='https://github.com/wagner-group/MarkMyWords' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Julien Piet, Chawin Sitawarin, Vivian Fang, Norman Mu, David Wagner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00273">Mark My Words: Analyzing and Evaluating Language Model Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The capabilities of large language models have grown significantly in recent years and so too have concerns about their misuse. It is important to be able to distinguish machine-generated text from human-authored content. Prior works have proposed numerous schemes to watermark text, which would benefit from a systematic evaluation framework. This work focuses on LLM output watermarking techniques - as opposed to image or model watermarks - and proposes Mark My Words, a comprehensive benchmark for them under different natural language tasks. We focus on three main metrics: quality, size (i.e., the number of tokens needed to detect a watermark), and tamper resistance (i.e., the ability to detect a watermark after perturbing marked text). Current watermarking techniques are nearly practical enough for real-world use: Kirchenbauer et al. [33]'s scheme can watermark models like Llama 2 7B-chat or Mistral-7B-Instruct with no perceivable loss in quality on natural language tasks, the watermark can be detected with fewer than 100 tokens, and their scheme offers good tamper resistance to simple perturbations. However, they struggle to efficiently watermark code generations. We publicly release our benchmark (https://github.com/wagner-group/MarkMyWords).
<div id='section'>Paperid: <span id='pid'>145, <a href='https://arxiv.org/pdf/2311.15617.pdf' target='_blank'>https://arxiv.org/pdf/2311.15617.pdf</a></span>   <span><a href='https://github.com/GTMLLab/VeryFL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihao Li, Yanyi Lai, Chuan Chen, Zibin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.15617">VeryFL: A Verify Federated Learning Framework Embedded with Blockchain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Blockchain-empowered federated learning (FL) has provoked extensive research recently. Various blockchain-based federated learning algorithm, architecture and mechanism have been designed to solve issues like single point failure and data falsification brought by centralized FL paradigm. Moreover, it is easier to allocate incentives to nodes with the help of the blockchain. Various centralized federated learning frameworks like FedML, have emerged in the community to help boost the research on FL. However, decentralized blockchain-based federated learning framework is still missing, which cause inconvenience for researcher to reproduce or verify the algorithm performance based on blockchain. Inspired by the above issues, we have designed and developed a blockchain-based federated learning framework by embedding Ethereum network. This report will present the overall structure of this framework, which proposes a code practice paradigm for the combination of FL with blockchain and, at the same time, compatible with normal FL training task. In addition to implement some blockchain federated learning algorithms on smart contract to help execute a FL training, we also propose a model ownership authentication architecture based on blockchain and model watermarking to protect the intellectual property rights of models. These mechanism on blockchain shows an underlying support of blockchain for federated learning to provide a verifiable training, aggregation and incentive distribution procedure and thus we named this framework VeryFL (A Verify Federated Learninig Framework Embedded with Blockchain). The source code is avaliable on https://github.com/GTMLLab/VeryFL.
<div id='section'>Paperid: <span id='pid'>146, <a href='https://arxiv.org/pdf/2311.13713.pdf' target='_blank'>https://arxiv.org/pdf/2311.13713.pdf</a></span>   <span><a href='https://github.com/BennyTMT/RIW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingtian Tan, Tianhao Wang, Somesh Jha
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13713">A Somewhat Robust Image Watermark against Diffusion-based Editing Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, diffusion models (DMs) have become the state-of-the-art method for image synthesis. Editing models based on DMs, known for their high fidelity and precision, have inadvertently introduced new challenges related to image copyright infringement and malicious editing. Our work is the first to formalize and address this issue. After assessing and attempting to enhance traditional image watermarking techniques, we recognize their limitations in this emerging context. In response, we develop a novel technique, RIW (Robust Invisible Watermarking), to embed invisible watermarks leveraging adversarial example techniques. Our technique ensures a high extraction accuracy of $96\%$ for the invisible watermark after editing, compared to the $0\%$ offered by conventional methods. We provide access to our code at https://github.com/BennyTMT/RIW.
<div id='section'>Paperid: <span id='pid'>147, <a href='https://arxiv.org/pdf/2311.13629.pdf' target='_blank'>https://arxiv.org/pdf/2311.13629.pdf</a></span>   <span><a href='https://github.com/mtailanian/diff-cf' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>MatÃ­as Tailanian, Marina Gardella, Ãlvaro Pardo, Pablo MusÃ©
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13629">Diffusion models meet image counter-forensics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>From its acquisition in the camera sensors to its storage, different operations are performed to generate the final image. This pipeline imprints specific traces into the image to form a natural watermark. Tampering with an image disturbs these traces; these disruptions are clues that are used by most methods to detect and locate forgeries. In this article, we assess the capabilities of diffusion models to erase the traces left by forgers and, therefore, deceive forensics methods. Such an approach has been recently introduced for adversarial purification, achieving significant performance. We show that diffusion purification methods are well suited for counter-forensics tasks. Such approaches outperform already existing counter-forensics techniques both in deceiving forensics methods and in preserving the natural look of the purified images. The source code is publicly available at https://github.com/mtailanian/diff-cf.
<div id='section'>Paperid: <span id='pid'>148, <a href='https://arxiv.org/pdf/2311.07138.pdf' target='_blank'>https://arxiv.org/pdf/2311.07138.pdf</a></span>   <span><a href='https://github.com/THU-KEG/WaterBench' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Shangqing Tu, Yuliang Sun, Yushi Bai, Jifan Yu, Lei Hou, Juanzi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07138">WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For benchmarking procedure, to ensure an apples-to-apples comparison, we first adjust each watermarking method's hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For task selection, we diversify the input and output length to form a five-category taxonomy, covering $9$ tasks. (3) For evaluation metric, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate $4$ open-source watermarks on $2$ LLMs under $2$ watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github.com/THU-KEG/WaterBench.
<div id='section'>Paperid: <span id='pid'>149, <a href='https://arxiv.org/pdf/2311.05863.pdf' target='_blank'>https://arxiv.org/pdf/2311.05863.pdf</a></span>   <span><a href='https://github.com/Pter61/vlpmarker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuanmin Tang, Jing Yu, Keke Gai, Xiangyan Qu, Yue Hu, Gang Xiong, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05863">Watermarking Vision-Language Pre-trained Models for Multi-modal Embedding as a Service</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in vision-language pre-trained models (VLPs) have significantly increased visual understanding and cross-modal analysis capabilities. Companies have emerged to provide multi-modal Embedding as a Service (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount of training data and resources for high-performance service. However, existing studies indicate that EaaS is vulnerable to model extraction attacks that induce great loss for the owners of VLPs. Protecting the intellectual property and commercial ownership of VLPs is increasingly crucial yet challenging. A major solution of watermarking model for EaaS implants a backdoor in the model by inserting verifiable trigger embeddings into texts, but it is only applicable for large language models and is unrealistic due to data and model privacy. In this paper, we propose a safe and robust backdoor-based embedding watermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding orthogonal transformation to effectively inject triggers into the VLPs without interfering with the model parameters, which achieves high-quality copyright verification and minimal impact on model performance. To enhance the watermark robustness, we further propose a collaborative copyright verification strategy based on both backdoor trigger and embedding distribution, enhancing resilience against various attacks. We increase the watermark practicality via an out-of-distribution trigger selection approach, removing access to the model training data and thus making it possible for many real-world scenarios. Our extensive experiments on various datasets indicate that the proposed watermarking approach is effective and safe for verifying the copyright of VLPs for multi-modal EaaS and robust against model extraction attacks. Our code is available at https://github.com/Pter61/vlpmarker.
<div id='section'>Paperid: <span id='pid'>150, <a href='https://arxiv.org/pdf/2310.14942.pdf' target='_blank'>https://arxiv.org/pdf/2310.14942.pdf</a></span>   <span><a href='https://github.com/JunfengGo/Domain-Watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junfeng Guo, Yiming Li, Lixu Wang, Shu-Tao Xia, Heng Huang, Cong Liu, Bo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14942">Domain Watermark: Effective and Harmless Dataset Copyright Protection is Closed at Hand</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The prosperity of deep neural networks (DNNs) is largely benefited from open-source datasets, based on which users can evaluate and improve their methods. In this paper, we revisit backdoor-based dataset ownership verification (DOV), which is currently the only feasible approach to protect the copyright of open-source datasets. We reveal that these methods are fundamentally harmful given that they could introduce malicious misclassification behaviors to watermarked DNNs by the adversaries. In this paper, we design DOV from another perspective by making watermarked models (trained on the protected dataset) correctly classify some `hard' samples that will be misclassified by the benign model. Our method is inspired by the generalization property of DNNs, where we find a \emph{hardly-generalized domain} for the original dataset (as its \emph{domain watermark}). It can be easily learned with the protected dataset containing modified samples. Specifically, we formulate the domain generation as a bi-level optimization and propose to optimize a set of visually-indistinguishable clean-label modified data with similar effects to domain-watermarked samples from the hardly-generalized domain to ensure watermark stealthiness. We also design a hypothesis-test-guided ownership verification via our domain watermark and provide the theoretical analyses of our method. Extensive experiments on three benchmark datasets are conducted, which verify the effectiveness of our method and its resistance to potential adaptive methods. The code for reproducing main experiments is available at \url{https://github.com/JunfengGo/Domain-Watermark}.
<div id='section'>Paperid: <span id='pid'>151, <a href='https://arxiv.org/pdf/2310.14724.pdf' target='_blank'>https://arxiv.org/pdf/2310.14724.pdf</a></span>   <span><a href='https://github.com/NLP2CT/LLM-generated-Text-Detection' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong, Lidia S. Chao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14724">A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future Directions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, statistics-based detectors, neural-base detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressing need to bolster detector research. We also delve into prevalent datasets, elucidating their limitations and developmental requirements. Furthermore, we analyze various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues and the lack of effective evaluation framework. Conclusively, we highlight interesting directions for future research in LLM-generated text detection to advance the implementation of responsible artificial intelligence (AI). Our aim with this survey is to provide a clear and comprehensive introduction for newcomers while also offering seasoned researchers a valuable update in the field of LLM-generated text detection. The useful resources are publicly available at: https://github.com/NLP2CT/LLM-generated-Text-Detection.
<div id='section'>Paperid: <span id='pid'>152, <a href='https://arxiv.org/pdf/2310.14532.pdf' target='_blank'>https://arxiv.org/pdf/2310.14532.pdf</a></span>   <span><a href='https://github.com/bytedance/DWSF' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengchang Guo, Qilong Zhang, Junwei Luo, Feng Guo, Wenbin Zhang, Xiaodong Su, Minglei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.14532">Practical Deep Dispersed Watermarking with Synchronization and Fusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning based blind watermarking works have gradually emerged and achieved impressive performance. However, previous deep watermarking studies mainly focus on fixed low-resolution images while paying less attention to arbitrary resolution images, especially widespread high-resolution images nowadays. Moreover, most works usually demonstrate robustness against typical non-geometric attacks (\textit{e.g.}, JPEG compression) but ignore common geometric attacks (\textit{e.g.}, Rotate) and more challenging combined attacks. To overcome the above limitations, we propose a practical deep \textbf{D}ispersed \textbf{W}atermarking with \textbf{S}ynchronization and \textbf{F}usion, called \textbf{\proposed}. Specifically, given an arbitrary-resolution cover image, we adopt a dispersed embedding scheme which sparsely and randomly selects several fixed small-size cover blocks to embed a consistent watermark message by a well-trained encoder. In the extraction stage, we first design a watermark synchronization module to locate and rectify the encoded blocks in the noised watermarked image. We then utilize a decoder to obtain messages embedded in these blocks, and propose a message fusion strategy based on similarity to make full use of the consistency among messages, thus determining a reliable message. Extensive experiments conducted on different datasets convincingly demonstrate the effectiveness of our proposed {\proposed}. Compared with state-of-the-art approaches, our blind watermarking can achieve better performance: averagely improve the bit accuracy by 5.28\% and 5.93\% against single and combined attacks, respectively, and show less file size increment and better visual quality. Our code is available at https://github.com/bytedance/DWSF.
<div id='section'>Paperid: <span id='pid'>153, <a href='https://arxiv.org/pdf/2310.11446.pdf' target='_blank'>https://arxiv.org/pdf/2310.11446.pdf</a></span>   <span><a href='https://pierrefdz.github.io/publications/invariancewm/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Fernandez, Guillaume Couairon, Teddy Furon, Matthijs Douze
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11446">Functional Invariants to Watermark Large Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of transformer-based models increases the concerns about their integrity and ownership insurance. Watermarking addresses this issue by embedding a unique identifier into the model, while preserving its performance. However, most existing approaches require to optimize the weights to imprint the watermark signal, which is not suitable at scale due to the computational cost. This paper explores watermarks with virtually no computational cost, applicable to a non-blind white-box setting (assuming access to both the original and watermarked networks). They generate functionally equivalent copies by leveraging the models' invariance, via operations like dimension permutations or scaling/unscaling. This enables to watermark models without any change in their outputs and remains stealthy. Experiments demonstrate the effectiveness of the approach and its robustness against various model transformations (fine-tuning, quantization, pruning), making it a practical solution to protect the integrity of large models.
<div id='section'>Paperid: <span id='pid'>154, <a href='https://arxiv.org/pdf/2310.07726.pdf' target='_blank'>https://arxiv.org/pdf/2310.07726.pdf</a></span>   <span><a href='https://github.com/GuanlinLee/warfare' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanlin Li, Yifei Chen, Jie Zhang, Shangwei Guo, Han Qiu, Guoyin Wang, Jiwei Li, Tianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07726">Warfare:Breaking the Watermark Protection of AI-Generated Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-Generated Content (AIGC) is rapidly expanding, with services using advanced generative models to create realistic images and fluent text. Regulating such content is crucial to prevent policy violations, such as unauthorized commercialization or unsafe content distribution. Watermarking is a promising solution for content attribution and verification, but we demonstrate its vulnerability to two key attacks: (1) Watermark removal, where adversaries erase embedded marks to evade regulation, and (2) Watermark forging, where they generate illicit content with forged watermarks, leading to misattribution. We propose Warfare, a unified attack framework leveraging a pre-trained diffusion model for content processing and a generative adversarial network for watermark manipulation. Evaluations across datasets and embedding setups show that Warfare achieves high success rates while preserving content quality. We further introduce Warfare-Plus, which enhances efficiency without compromising effectiveness. The code can be found in https://github.com/GuanlinLee/warfare.
<div id='section'>Paperid: <span id='pid'>155, <a href='https://arxiv.org/pdf/2310.06356.pdf' target='_blank'>https://arxiv.org/pdf/2310.06356.pdf</a></span>   <span><a href='https://github.com/THU-BPM/Robust_Watermark' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/THU-BPM/MarkLLM' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/THU-BPM/Robust_Watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.06356">A Semantic Invariant Robust Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness. Our code and data are available at \href{https://github.com/THU-BPM/Robust_Watermark}{https://github.com/THU-BPM/Robust\_Watermark}. Additionally, our algorithm could also be accessed through MarkLLM \citep{pan2024markllm} \footnote{https://github.com/THU-BPM/MarkLLM}.
<div id='section'>Paperid: <span id='pid'>156, <a href='https://arxiv.org/pdf/2309.04777.pdf' target='_blank'>https://arxiv.org/pdf/2309.04777.pdf</a></span>   <span><a href='https://github.com/GuanhaoGan/robust-model-watermarking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanhao Gan, Yiming Li, Dongxian Wu, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.04777">Towards Robust Model Watermark via Reducing Parametric Vulnerability</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks are valuable assets considering their commercial benefits and huge demands for costly annotation and computation resources. To protect the copyright of DNNs, backdoor-based ownership verification becomes popular recently, in which the model owner can watermark the model by embedding a specific backdoor behavior before releasing it. The defenders (usually the model owners) can identify whether a suspicious third-party model is ``stolen'' from them based on the presence of the behavior. Unfortunately, these watermarks are proven to be vulnerable to removal attacks even like fine-tuning. To further explore this vulnerability, we investigate the parameter space and find there exist many watermark-removed models in the vicinity of the watermarked one, which may be easily used by removal attacks. Inspired by this finding, we propose a mini-max formulation to find these watermark-removed models and recover their watermark behavior. Extensive experiments demonstrate that our method improves the robustness of the model watermarking against parametric changes and numerous watermark-removal attacks. The codes for reproducing our main experiments are available at \url{https://github.com/GuanhaoGan/robust-model-watermarking}.
<div id='section'>Paperid: <span id='pid'>157, <a href='https://arxiv.org/pdf/2308.14061.pdf' target='_blank'>https://arxiv.org/pdf/2308.14061.pdf</a></span>   <span><a href='https://github.com/xyfJASON/HCL' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Feng, Yifeng Xu, Guangming Lu, Wenjie Pei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14061">Hierarchical Contrastive Learning for Pattern-Generalizable Image Corruption Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Effective image restoration with large-size corruptions, such as blind image inpainting, entails precise detection of corruption region masks which remains extremely challenging due to diverse shapes and patterns of corruptions. In this work, we present a novel method for automatic corruption detection, which allows for blind corruption restoration without known corruption masks. Specifically, we develop a hierarchical contrastive learning framework to detect corrupted regions by capturing the intrinsic semantic distinctions between corrupted and uncorrupted regions. In particular, our model detects the corrupted mask in a coarse-to-fine manner by first predicting a coarse mask by contrastive learning in low-resolution feature space and then refines the uncertain area of the mask by high-resolution contrastive learning. A specialized hierarchical interaction mechanism is designed to facilitate the knowledge propagation of contrastive learning in different scales, boosting the modeling performance substantially. The detected multi-scale corruption masks are then leveraged to guide the corruption restoration. Detecting corrupted regions by learning the contrastive distinctions rather than the semantic patterns of corruptions, our model has well generalization ability across different corruption patterns. Extensive experiments demonstrate following merits of our model: 1) the superior performance over other methods on both corruption detection and various image restoration tasks including blind inpainting and watermark removal, and 2) strong generalization across different corruption patterns such as graffiti, random noise or other image content. Codes and trained weights are available at https://github.com/xyfJASON/HCL .
<div id='section'>Paperid: <span id='pid'>158, <a href='https://arxiv.org/pdf/2308.02816.pdf' target='_blank'>https://arxiv.org/pdf/2308.02816.pdf</a></span>   <span><a href='https://github.com/grasses/PromptCARE' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongwei Yao, Jian Lou, Kui Ren, Zhan Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.02816">PromptCARE: Prompt Copyright Protection by Watermark Injection and Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have witnessed a meteoric rise in popularity among the general public users over the past few months, facilitating diverse downstream tasks with human-level accuracy and proficiency. Prompts play an essential role in this success, which efficiently adapt pre-trained LLMs to task-specific applications by simply prepending a sequence of tokens to the query texts. However, designing and selecting an optimal prompt can be both expensive and demanding, leading to the emergence of Prompt-as-a-Service providers who profit by providing well-designed prompts for authorized use. With the growing popularity of prompts and their indispensable role in LLM-based services, there is an urgent need to protect the copyright of prompts against unauthorized use.
  In this paper, we propose PromptCARE, the first framework for prompt copyright protection through watermark injection and verification. Prompt watermarking presents unique challenges that render existing watermarking techniques developed for model and dataset copyright verification ineffective. PromptCARE overcomes these hurdles by proposing watermark injection and verification schemes tailor-made for prompts and NLP characteristics. Extensive experiments on six well-known benchmark datasets, using three prevalent pre-trained LLMs (BERT, RoBERTa, and Facebook OPT-1.3b), demonstrate the effectiveness, harmlessness, robustness, and stealthiness of PromptCARE.
<div id='section'>Paperid: <span id='pid'>159, <a href='https://arxiv.org/pdf/2308.00221.pdf' target='_blank'>https://arxiv.org/pdf/2308.00221.pdf</a></span>   <span><a href='https://github.com/bangawayoo/mb-lm-watermarking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>KiYoon Yoo, Wonhyuk Ahn, Nojun Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00221">Advancing Beyond Identification: Multi-bit Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We show the viability of tackling misuses of large language models beyond the identification of machine-generated text. While existing zero-bit watermark methods focus on detection only, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose Multi-bit Watermark via Position Allocation, embedding traceable multi-bit information during language model generation. Through allocating tokens onto different parts of the messages, we embed longer messages in high corruption settings without added latency. By independently embedding sub-units of messages, the proposed method outperforms the existing works in terms of robustness and latency. Leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages ($\geq$ 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time. Code is released here: https://github.com/bangawayoo/mb-lm-watermarking
<div id='section'>Paperid: <span id='pid'>160, <a href='https://arxiv.org/pdf/2308.00113.pdf' target='_blank'>https://arxiv.org/pdf/2308.00113.pdf</a></span>   <span><a href='https://github.com/facebookresearch/three_bricks' target='_blank'>  GitHub</a></span> <span><a href='https://pierrefdz.github.io/publications/threebricks/' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, Teddy Furon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.00113">Three Bricks to Consolidate Watermarks for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The task of discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing generated text to a specific model. It alters the sampling generation process so as to leave an invisible trace in the generated output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10$^{\text{-6}}$). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.
<div id='section'>Paperid: <span id='pid'>161, <a href='https://arxiv.org/pdf/2307.16230.pdf' target='_blank'>https://arxiv.org/pdf/2307.16230.pdf</a></span>   <span><a href='https://github.com/THU-BPM/MarkLLM' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/THU-BPM/unforgeable_watermark' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/THU-BPM/unforgeable_watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Aiwei Liu, Leyi Pan, Xuming Hu, Shu'ang Li, Lijie Wen, Irwin King, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.16230">An Unforgeable Publicly Verifiable Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, text watermarking algorithms for large language models (LLMs) have been proposed to mitigate the potential harms of text generated by LLMs, including fake news and copyright issues. However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection. To address this limitation, we propose an unforgeable publicly verifiable watermark algorithm named UPV that uses two different neural networks for watermark generation and detection, instead of using the same key at both stages. Meanwhile, the token embedding parameters are shared between the generation and detection networks, which makes the detection network achieve a high accuracy very efficiently. Experiments demonstrate that our algorithm attains high detection accuracy and computational efficiency through neural networks. Subsequent analysis confirms the high complexity involved in forging the watermark from the detection network. Our code is available at \href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable\_watermark}. Additionally, our algorithm could also be accessed through MarkLLM \citep{pan2024markllm} \footnote{https://github.com/THU-BPM/MarkLLM}.
<div id='section'>Paperid: <span id='pid'>162, <a href='https://arxiv.org/pdf/2307.15992.pdf' target='_blank'>https://arxiv.org/pdf/2307.15992.pdf</a></span>   <span><a href='https://github.com/lancopku/codable-watermarking-for-llm' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng, Jie Zhou, Xu Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15992">Towards Codable Watermarking for Injecting Multi-bits Information to LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns. However, we argue that existing LLM watermarking methods are encoding-inefficient and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.). In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit customizable information. First of all, we study the taxonomy of LLM watermarking technologies and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive evaluation system for CTWL: (1) watermarking success rate, (2) robustness against various corruptions, (3) coding rate of payload information, (4) encoding and decoding efficiency, (5) impacts on the quality of the generated text. To meet the requirements of these non-Pareto-improving metrics, we follow the most prominent vocabulary partition-based watermarking direction, and devise an advanced CTWL method named Balance-Marking. The core idea of our method is to use a proxy language model to split the vocabulary into probability-balanced parts, thereby effectively maintaining the quality of the watermarked text. Our code is available at https://github.com/lancopku/codable-watermarking-for-llm.
<div id='section'>Paperid: <span id='pid'>163, <a href='https://arxiv.org/pdf/2306.17439.pdf' target='_blank'>https://arxiv.org/pdf/2306.17439.pdf</a></span>   <span><a href='https://github.com/XuandongZhao/Unigram-Watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuandong Zhao, Prabhanjan Ananth, Lei Li, Yu-Xiang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.17439">Provable Robust Watermarking for AI-Generated Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study the problem of watermarking large language models (LLMs) generated text -- one of the most promising approaches for addressing the safety challenges of LLM usage. In this paper, we propose a rigorous theoretical framework to quantify the effectiveness and robustness of LLM watermarks. We propose a robust and high-quality watermark method, Unigram-Watermark, by extending an existing approach with a simplified fixed grouping strategy. We prove that our watermark method enjoys guaranteed generation quality, correctness in watermark detection, and is robust against text editing and paraphrasing. Experiments on three varying LLMs and two datasets verify that our Unigram-Watermark achieves superior detection accuracy and comparable generation quality in perplexity, thus promoting the responsible use of LLMs. Code is available at https://github.com/XuandongZhao/Unigram-Watermark.
<div id='section'>Paperid: <span id='pid'>164, <a href='https://arxiv.org/pdf/2306.04642.pdf' target='_blank'>https://arxiv.org/pdf/2306.04642.pdf</a></span>   <span><a href='https://github.com/Yingqiancui/DiffusionShield' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingqian Cui, Jie Ren, Han Xu, Pengfei He, Hui Liu, Lichao Sun, Yue Xing, Jiliang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04642">DiffusionShield: A Watermark for Copyright Protection against Generative Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Generative Diffusion Models (GDMs) have showcased their remarkable capabilities in learning and generating images. A large community of GDMs has naturally emerged, further promoting the diversified applications of GDMs in various fields. However, this unrestricted proliferation has raised serious concerns about copyright protection. For example, artists including painters and photographers are becoming increasingly concerned that GDMs could effortlessly replicate their unique creative works without authorization. In response to these challenges, we introduce a novel watermarking scheme, DiffusionShield, tailored for GDMs. DiffusionShield protects images from copyright infringement by GDMs through encoding the ownership information into an imperceptible watermark and injecting it into the images. Its watermark can be easily learned by GDMs and will be reproduced in their generated images. By detecting the watermark from generated images, copyright infringement can be exposed with evidence. Benefiting from the uniformity of the watermarks and the joint optimization method, DiffusionShield ensures low distortion of the original image, high watermark detection performance, and the ability to embed lengthy messages. We conduct rigorous and comprehensive experiments to show the effectiveness of DiffusionShield in defending against infringement by GDMs and its superiority over traditional watermarking methods. The code for DiffusionShield is accessible in https://github.com/Yingqiancui/DiffusionShield.
<div id='section'>Paperid: <span id='pid'>165, <a href='https://arxiv.org/pdf/2306.04634.pdf' target='_blank'>https://arxiv.org/pdf/2306.04634.pdf</a></span>   <span><a href='https://github.com/jwkirchenbauer/lm-watermarking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.04634">On the Reliability of Watermarks for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.
<div id='section'>Paperid: <span id='pid'>166, <a href='https://arxiv.org/pdf/2306.01953.pdf' target='_blank'>https://arxiv.org/pdf/2306.01953.pdf</a></span>   <span><a href='https://github.com/XuandongZhao/WatermarkAttacker' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuandong Zhao, Kexun Zhang, Zihao Su, Saastha Vasan, Ilya Grishchenko, Christopher Kruegel, Giovanni Vigna, Yu-Xiang Wang, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01953">Invisible Image Watermarks Are Provably Removable Using Generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invisible watermarks safeguard images' copyrights by embedding hidden messages only detectable by owners. They also prevent people from misusing images, especially those generated by AI models. We propose a family of regeneration attacks to remove these invisible watermarks. The proposed attack method first adds random noise to an image to destroy the watermark and then reconstructs the image. This approach is flexible and can be instantiated with many existing image-denoising algorithms and pre-trained generative models such as diffusion models. Through formal proofs and extensive empirical evaluations, we demonstrate that pixel-level invisible watermarks are vulnerable to this regeneration attack. Our results reveal that, across four different pixel-level watermarking schemes, the proposed method consistently achieves superior performance compared to existing attack techniques, with lower detection rates and higher image quality. However, watermarks that keep the image semantically similar can be an alternative defense against our attacks. Our finding underscores the need for a shift in research/industry emphasis from invisible watermarks to semantic-preserving watermarks. Code is available at https://github.com/XuandongZhao/WatermarkAttacker
<div id='section'>Paperid: <span id='pid'>167, <a href='https://arxiv.org/pdf/2305.20030.pdf' target='_blank'>https://arxiv.org/pdf/2305.20030.pdf</a></span>   <span><a href='https://github.com/YuxinWenRick/tree-ring-watermark,' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/YuxinWenRick/tree-ring-watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuxin Wen, John Kirchenbauer, Jonas Geiping, Tom Goldstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.20030">Tree-Ring Watermarks: Fingerprints for Diffusion Images that are Invisible and Robust</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking the outputs of generative models is a crucial technique for tracing copyright and preventing potential harm from AI-generated content. In this paper, we introduce a novel technique called Tree-Ring Watermarking that robustly fingerprints diffusion model outputs. Unlike existing methods that perform post-hoc modifications to images after sampling, Tree-Ring Watermarking subtly influences the entire sampling process, resulting in a model fingerprint that is invisible to humans. The watermark embeds a pattern into the initial noise vector used for sampling. These patterns are structured in Fourier space so that they are invariant to convolutions, crops, dilations, flips, and rotations. After image generation, the watermark signal is detected by inverting the diffusion process to retrieve the noise vector, which is then checked for the embedded signal. We demonstrate that this technique can be easily applied to arbitrary diffusion models, including text-conditioned Stable Diffusion, as a plug-in with negligible loss in FID. Our watermark is semantically hidden in the image space and is far more robust than watermarking alternatives that are currently deployed. Code is available at https://github.com/YuxinWenRick/tree-ring-watermark.
<div id='section'>Paperid: <span id='pid'>168, <a href='https://arxiv.org/pdf/2305.15060.pdf' target='_blank'>https://arxiv.org/pdf/2305.15060.pdf</a></span>   <span><a href='https://github.com/hongcheki/sweet-watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Taehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin, Gunhee Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.15060">Who Wrote this Code? Watermarking for Code Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed. However, we discover that the existing works fail to function appropriately in code generation tasks due to the task's nature of having low entropy. Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks. Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text. Our code is available in https://github.com/hongcheki/sweet-watermark.
<div id='section'>Paperid: <span id='pid'>169, <a href='https://arxiv.org/pdf/2305.08883.pdf' target='_blank'>https://arxiv.org/pdf/2305.08883.pdf</a></span>   <span><a href='https://github.com/Kiode/Text_Watermark_Language_Models' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.08883">Watermarking Text Generated by Black-Box Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark. Experiments demonstrate the effectiveness of our method on both Chinese and English datasets. Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.
<div id='section'>Paperid: <span id='pid'>170, <a href='https://arxiv.org/pdf/2305.03807.pdf' target='_blank'>https://arxiv.org/pdf/2305.03807.pdf</a></span>   <span><a href='https://github.com/zhengyuan-jiang/WEvade' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyuan Jiang, Jinghuai Zhang, Neil Zhenqiang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.03807">Evading Watermark based Detection of AI-Generated Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A generative AI model can generate extremely realistic-looking content, posing growing challenges to the authenticity of information. To address the challenges, watermark has been leveraged to detect AI-generated content. Specifically, a watermark is embedded into an AI-generated content before it is released. A content is detected as AI-generated if a similar watermark can be decoded from it. In this work, we perform a systematic study on the robustness of such watermark-based AI-generated content detection. We focus on AI-generated images. Our work shows that an attacker can post-process a watermarked image via adding a small, human-imperceptible perturbation to it, such that the post-processed image evades detection while maintaining its visual quality. We show the effectiveness of our attack both theoretically and empirically. Moreover, to evade detection, our adversarial post-processing method adds much smaller perturbations to AI-generated images and thus better maintain their visual quality than existing popular post-processing methods such as JPEG compression, Gaussian blur, and Brightness/Contrast. Our work shows the insufficiency of existing watermark-based detection of AI-generated content, highlighting the urgent needs of new methods. Our code is publicly available: https://github.com/zhengyuan-jiang/WEvade.
<div id='section'>Paperid: <span id='pid'>171, <a href='https://arxiv.org/pdf/2305.01904.pdf' target='_blank'>https://arxiv.org/pdf/2305.01904.pdf</a></span>   <span><a href='https://github.com/bangawayoo/nlp-watermarking' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, Nojun Kwak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.01904">Robust Multi-bit Natural Language Watermarking through Invariant Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed a proliferation of valuable original natural language contents found in subscription-based media outlets, web novel platforms, and outputs of large language models. However, these contents are susceptible to illegal piracy and potential misuse without proper security measures. This calls for a secure watermarking system to guarantee copyright protection through leakage tracing or ownership identification. To effectively combat piracy and protect copyrights, a multi-bit watermarking framework should be able to embed adequate bits of information and extract the watermarks in a robust manner despite possible corruption. In this work, we explore ways to advance both payload and robustness by following a well-known proposition from image watermarking and identify features in natural language that are invariant to minor corruption. Through a systematic analysis of the possible sources of errors, we further propose a corruption-resistant infill model. Our full method improves upon the previous work on robustness by +16.8% point on average on four datasets, three corruption types, and two corruption ratios. Code available at https://github.com/bangawayoo/nlp-watermarking.
<div id='section'>Paperid: <span id='pid'>172, <a href='https://arxiv.org/pdf/2305.00097.pdf' target='_blank'>https://arxiv.org/pdf/2305.00097.pdf</a></span>   <span><a href='https://github.com/Tongzhou0101/NNSplitter' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zhou, Yukui Luo, Shaolei Ren, Xiaolin Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00097">NNSplitter: An Active Defense Solution for DNN Model via Automated Weight Obfuscation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a type of valuable intellectual property (IP), deep neural network (DNN) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model IP protection scheme, namely NNSplitter, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users with the support of the trusted execution environment. Experimental results demonstrate the effectiveness of NNSplitter, e.g., by only modifying 275 out of over 11 million (i.e., 0.002%) weights, the accuracy of the obfuscated ResNet-18 model on CIFAR-10 can drop to 10%. Moreover, NNSplitter is stealthy and resilient against norm clipping and fine-tuning attacks, making it an appealing solution for DNN model protection. The code is available at: https://github.com/Tongzhou0101/NNSplitter.
<div id='section'>Paperid: <span id='pid'>173, <a href='https://arxiv.org/pdf/2304.07361.pdf' target='_blank'>https://arxiv.org/pdf/2304.07361.pdf</a></span>   <span><a href='https://github.com/nilslukas/gan-watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Lukas, Florian Kerschbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.07361">PTW: Pivotal Tuning Watermarking for Pre-Trained Image Generators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfakes refer to content synthesized using deep generators, which, when misused, have the potential to erode trust in digital media. Synthesizing high-quality deepfakes requires access to large and complex generators only a few entities can train and provide. The threat is malicious users that exploit access to the provided model and generate harmful deepfakes without risking detection. Watermarking makes deepfakes detectable by embedding an identifiable code into the generator that is later extractable from its generated images. We propose Pivotal Tuning Watermarking (PTW), a method for watermarking pre-trained generators (i) three orders of magnitude faster than watermarking from scratch and (ii) without the need for any training data. We improve existing watermarking methods and scale to generators $4 \times$ larger than related work. PTW can embed longer codes than existing methods while better preserving the generator's image quality. We propose rigorous, game-based definitions for robustness and undetectability, and our study reveals that watermarking is not robust against an adaptive white-box attacker who controls the generator's parameters. We propose an adaptive attack that can successfully remove any watermarking with access to only 200 non-watermarked images. Our work challenges the trustworthiness of watermarking for deepfake detection when the parameters of a generator are available. The source code to reproduce our experiments is available at https://github.com/nilslukas/gan-watermark.
<div id='section'>Paperid: <span id='pid'>174, <a href='https://arxiv.org/pdf/2304.03400.pdf' target='_blank'>https://arxiv.org/pdf/2304.03400.pdf</a></span>   <span><a href='https://github.com/TuBui/RoSteALS' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Tu Bui, Shruti Agarwal, Ning Yu, John Collomosse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.03400">RoSteALS: Robust Steganography using Autoencoder Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data hiding such as steganography and invisible watermarking has important applications in copyright protection, privacy-preserved communication and content provenance. Existing works often fall short in either preserving image quality, or robustness against perturbations or are too complex to train. We propose RoSteALS, a practical steganography technique leveraging frozen pretrained autoencoders to free the payload embedding from learning the distribution of cover images. RoSteALS has a light-weight secret encoder of just 300k parameters, is easy to train, has perfect secret recovery performance and comparable image quality on three benchmarks. Additionally, RoSteALS can be adapted for novel cover-less steganography applications in which the cover image can be sampled from noise or conditioned on text prompts via a denoising diffusion process. Our model and code are available at \url{https://github.com/TuBui/RoSteALS}.
<div id='section'>Paperid: <span id='pid'>175, <a href='https://arxiv.org/pdf/2303.15435.pdf' target='_blank'>https://arxiv.org/pdf/2303.15435.pdf</a></span>   <span><a href='https://github.com/facebookresearch/stable_signature' target='_blank'>  GitHub</a></span> <span><a href='https://pierrefdz.github.io/publications/stablesignature' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Pierre Fernandez, Guillaume Couairon, HervÃ© JÃ©gou, Matthijs Douze, Teddy Furon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.15435">The Stable Signature: Rooting Watermarks in Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative image modeling enables a wide range of applications but raises ethical concerns about responsible deployment. This paper introduces an active strategy combining image watermarking and Latent Diffusion Models. The goal is for all generated images to conceal an invisible watermark allowing for future detection and/or identification. The method quickly fine-tunes the latent decoder of the image generator, conditioned on a binary signature. A pre-trained watermark extractor recovers the hidden signature from any generated image and a statistical test then determines whether it comes from the generative model. We evaluate the invisibility and robustness of the watermarks on a variety of generation tasks, showing that Stable Signature works even after the images are modified. For instance, it detects the origin of an image generated from a text prompt, then cropped to keep $10\%$ of the content, with $90$+$\%$ accuracy at a false positive rate below 10$^{-6}$.
<div id='section'>Paperid: <span id='pid'>176, <a href='https://arxiv.org/pdf/2303.13408.pdf' target='_blank'>https://arxiv.org/pdf/2303.13408.pdf</a></span>   <span><a href='https://github.com/martiansideofthemoon/ai-detection-paraphrases' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Kalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, Mohit Iyyer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13408">Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Using DIPPER to paraphrase text generated by three large language models (including GPT3.5-davinci-003) successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics.
  To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.
<div id='section'>Paperid: <span id='pid'>177, <a href='https://arxiv.org/pdf/2303.11156.pdf' target='_blank'>https://arxiv.org/pdf/2303.11156.pdf</a></span>   <span><a href='https://github.com/vinusankars/Reliability-of-AI-text-detectors' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, Soheil Feizi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11156">Can AI-Generated Text be Reliably Detected?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) perform impressively well in various applications. However, the potential for misuse of these models in activities such as plagiarism, generating fake news, and spamming has raised concern about their responsible use. Consequently, the reliable detection of AI-generated text has become a critical area of research. AI text detectors have shown to be effective under their specific settings. In this paper, we stress-test the robustness of these AI text detectors in the presence of an attacker. We introduce recursive paraphrasing attack to stress test a wide range of detection schemes, including the ones using the watermarking as well as neural network-based detectors, zero shot classifiers, and retrieval-based detectors. Our experiments conducted on passages, each approximately 300 tokens long, reveal the varying sensitivities of these detectors to our attacks. Our findings indicate that while our recursive paraphrasing method can significantly reduce detection rates, it only slightly degrades text quality in many cases, highlighting potential vulnerabilities in current detection systems in the presence of an attacker. Additionally, we investigate the susceptibility of watermarked LLMs to spoofing attacks aimed at misclassifying human-written text as AI-generated. We demonstrate that an attacker can infer hidden AI text signatures without white-box access to the detection method, potentially leading to reputational risks for LLM developers. Finally, we provide a theoretical framework connecting the AUROC of the best possible detector to the Total Variation distance between human and AI text distributions. This analysis offers insights into the fundamental challenges of reliable detection as language models continue to advance. Our code is publicly available at https://github.com/vinusankars/Reliability-of-AI-text-detectors.
<div id='section'>Paperid: <span id='pid'>178, <a href='https://arxiv.org/pdf/2303.10137.pdf' target='_blank'>https://arxiv.org/pdf/2303.10137.pdf</a></span>   <span><a href='https://github.com/yunqing-me/WatermarkDM' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, Min Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10137">A Recipe for Watermarking Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models (DMs) have demonstrated advantageous potential on generative tasks. Widespread interest exists in incorporating DMs into downstream applications, such as producing or editing photorealistic images. However, practical deployment and unprecedented power of DMs raise legal issues, including copyright protection and monitoring of generated content. In this regard, watermarking has been a proven solution for copyright protection and content monitoring, but it is underexplored in the DMs literature. Specifically, DMs generate samples from longer tracks and may have newly designed multimodal structures, necessitating the modification of conventional watermarking pipelines. To this end, we conduct comprehensive analyses and derive a recipe for efficiently watermarking state-of-the-art DMs (e.g., Stable Diffusion), via training from scratch or finetuning. Our recipe is straightforward but involves empirically ablated implementation details, providing a foundation for future research on watermarking DMs. The code is available at https://github.com/yunqing-me/WatermarkDM.
<div id='section'>Paperid: <span id='pid'>179, <a href='https://arxiv.org/pdf/2302.10296.pdf' target='_blank'>https://arxiv.org/pdf/2302.10296.pdf</a></span>   <span><a href='https://github.com/cure-lab/Function-Coupled-Watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Wen, Yu Li, Wei Jiang, Qiang Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.10296">On Function-Coupled Watermarks for Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Well-performed deep neural networks (DNNs) generally require massive labelled data and computational resources for training. Various watermarking techniques are proposed to protect such intellectual properties (IPs), wherein the DNN providers implant secret information into the model so that they can later claim IP ownership by retrieving their embedded watermarks with some dedicated trigger inputs. While promising results are reported in the literature, existing solutions suffer from watermark removal attacks, such as model fine-tuning and model pruning.
  In this paper, we propose a novel DNN watermarking solution that can effectively defend against the above attacks. Our key insight is to enhance the coupling of the watermark and model functionalities such that removing the watermark would inevitably degrade the model's performance on normal inputs. To this end, unlike previous methods relying on secret features learnt from out-of-distribution data, our method only uses features learnt from in-distribution data. Specifically, on the one hand, we propose to sample inputs from the original training dataset and fuse them as watermark triggers. On the other hand, we randomly mask model weights during training so that the information of our embedded watermarks spreads in the network. By doing so, model fine-tuning/pruning would not forget our function-coupled watermarks. Evaluation results on various image classification tasks show a 100\% watermark authentication success rate under aggressive watermark removal attacks, significantly outperforming existing solutions. Code is available: https://github.com/cure-lab/Function-Coupled-Watermark.
<div id='section'>Paperid: <span id='pid'>180, <a href='https://arxiv.org/pdf/2212.04825.pdf' target='_blank'>https://arxiv.org/pdf/2212.04825.pdf</a></span>   <span><a href='https://github.com/facebookresearch/Whac-A-Mole' target='_blank'>  GitHub</a></span> <span><a href='https://github.com/facebookresearch/Whac-A-Mole' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiheng Li, Ivan Evtimov, Albert Gordo, Caner Hazirbas, Tal Hassner, Cristian Canton Ferrer, Chenliang Xu, Mark Ibrahim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.04825">A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models have been found to learn shortcuts -- unintended decision rules that are unable to generalize -- undermining models' reliability. Previous works address this problem under the tenuous assumption that only a single shortcut exists in the training data. Real-world images are rife with multiple visual cues from background to texture. Key to advancing the reliability of vision systems is understanding whether existing methods can overcome multiple shortcuts or struggle in a Whac-A-Mole game, i.e., where mitigating one shortcut amplifies reliance on others. To address this shortcoming, we propose two benchmarks: 1) UrbanCars, a dataset with precisely controlled spurious cues, and 2) ImageNet-W, an evaluation set based on ImageNet for watermark, a shortcut we discovered affects nearly every modern vision model. Along with texture and background, ImageNet-W allows us to study multiple shortcuts emerging from training on natural images. We find computer vision models, including large foundation models -- regardless of training set, architecture, and supervision -- struggle when multiple shortcuts are present. Even methods explicitly designed to combat shortcuts struggle in a Whac-A-Mole dilemma. To tackle this challenge, we propose Last Layer Ensemble, a simple-yet-effective method to mitigate multiple shortcuts without Whac-A-Mole behavior. Our results surface multi-shortcut mitigation as an overlooked challenge critical to advancing the reliability of vision systems. The datasets and code are released: https://github.com/facebookresearch/Whac-A-Mole.
<div id='section'>Paperid: <span id='pid'>181, <a href='https://arxiv.org/pdf/2210.00875.pdf' target='_blank'>https://arxiv.org/pdf/2210.00875.pdf</a></span>   <span><a href='https://github.com/THUYimingLi/Untargeted_Backdoor_Watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, Bo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.00875">Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean-label settings. We also discuss how to use the proposed untargeted backdoor watermark for dataset ownership verification. Experiments on benchmark datasets verify the effectiveness of our methods and their resistance to existing backdoor defenses. Our codes are available at \url{https://github.com/THUYimingLi/Untargeted_Backdoor_Watermark}.
<div id='section'>Paperid: <span id='pid'>182, <a href='https://arxiv.org/pdf/2209.06015.pdf' target='_blank'>https://arxiv.org/pdf/2209.06015.pdf</a></span>   <span><a href='https://github.com/THUYimingLi/DVBW' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Tao Wei, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.06015">Black-box Dataset Ownership Verification via Backdoor Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning, especially deep neural networks (DNNs), has been widely and successfully adopted in many critical applications for its high effectiveness and efficiency. The rapid development of DNNs has benefited from the existence of some high-quality datasets ($e.g.$, ImageNet), which allow researchers and developers to easily verify the performance of their methods. Currently, almost all existing released datasets require that they can only be adopted for academic or educational purposes rather than commercial purposes without permission. However, there is still no good way to ensure that. In this paper, we formulate the protection of released datasets as verifying whether they are adopted for training a (suspicious) third-party model, where defenders can only query the model while having no information about its parameters and training details. Based on this formulation, we propose to embed external patterns via backdoor watermarking for the ownership verification to protect them. Our method contains two main parts, including dataset watermarking and dataset verification. Specifically, we exploit poison-only backdoor attacks ($e.g.$, BadNets) for dataset watermarking and design a hypothesis-test-guided method for dataset verification. We also provide some theoretical analyses of our methods. Experiments on multiple benchmark datasets of different tasks are conducted, which verify the effectiveness of our method. The code for reproducing main experiments is available at \url{https://github.com/THUYimingLi/DVBW}.
<div id='section'>Paperid: <span id='pid'>183, <a href='https://arxiv.org/pdf/2208.00563.pdf' target='_blank'>https://arxiv.org/pdf/2208.00563.pdf</a></span>   <span><a href='https://github.com/ghua-ac/dnn_watermark' target='_blank'>  GitHub</a></span></div></span></div><div id = 'author'>Authors:<span id = 'author'>Guang Hua, Andrew Beng Jin Teoh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.00563">Deep Fidelity in DNN Watermarking: A Study of Backdoor Watermarking for Classification Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Backdoor watermarking is a promising paradigm to protect the copyright of deep neural network (DNN) models. In the existing works on this subject, researchers have intensively focused on watermarking robustness, while the concept of fidelity, which is concerned with the preservation of the model's original functionality, has received less attention. In this paper, focusing on deep image classification models, we show that the existing shared notion of the sole measurement of learning accuracy is inadequate to characterize backdoor fidelity. Meanwhile, we show that the analogous concept of embedding distortion in multimedia watermarking, interpreted as the total weight loss (TWL) in DNN backdoor watermarking, is also problematic for fidelity measurement. To address this challenge, we propose the concept of deep fidelity, which states that the backdoor watermarked DNN model should preserve both the feature representation and decision boundary of the unwatermarked host model. To achieve deep fidelity, we propose two loss functions termed penultimate feature loss (PFL) and softmax probability-distribution loss (SPL) to preserve feature representation, while the decision boundary is preserved by the proposed fix last layer (FixLL) treatment, inspired by the recent discovery that deep learning with a fixed classifier causes no loss of learning accuracy. With the above designs, both embedding from scratch and fine-tuning strategies are implemented to evaluate the deep fidelity of backdoor embedding, whose advantages over the existing methods are verified via experiments using ResNet18 for MNIST and CIFAR-10 classifications, and wide residual network (i.e., WRN28_10) for CIFAR-100 task. PyTorch codes are available at https://github.com/ghua-ac/dnn_watermark.
<div id='section'>Paperid: <span id='pid'>184, <a href='https://arxiv.org/pdf/2409.10958.pdf' target='_blank'>https://arxiv.org/pdf/2409.10958.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yongyang Pan, Xiaohong Liu, Siqi Luo, Yi Xin, Xiao Guo, Xiaoming Liu, Xiongkuo Min, Guangtao Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10958">Towards Effective User Attribution for Latent Diffusion Models via Watermark-Informed Blending</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advancements in multimodal large language models have enabled the creation of hyper-realistic images from textual descriptions. However, these advancements also raise significant concerns about unauthorized use, which hinders their broader distribution. Traditional watermarking methods often require complex integration or degrade image quality. To address these challenges, we introduce a novel framework Towards Effective user Attribution for latent diffusion models via Watermark-Informed Blending (TEAWIB). TEAWIB incorporates a unique ready-to-use configuration approach that allows seamless integration of user-specific watermarks into generative models. This approach ensures that each user can directly apply a pre-configured set of parameters to the model without altering the original model parameters or compromising image quality. Additionally, noise and augmentation operations are embedded at the pixel level to further secure and stabilize watermarked images. Extensive experiments validate the effectiveness of TEAWIB, showcasing the state-of-the-art performance in perceptual quality and attribution accuracy.
<div id='section'>Paperid: <span id='pid'>185, <a href='https://arxiv.org/pdf/2504.01048.pdf' target='_blank'>https://arxiv.org/pdf/2504.01048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunxue Xu, Yiwei Wang, Bryan Hooi, Yujun Cai, Songze Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.01048">How does Watermarking Affect Visual Language Models in Document Understanding?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Language Models (VLMs) have become foundational models for document understanding tasks, widely used in the processing of complex multimodal documents across domains such as finance, law, and academia. However, documents often contain noise-like information, such as watermarks, which inevitably leads us to inquire: \emph{Do watermarks degrade the performance of VLMs in document understanding?} To address this, we propose a novel evaluation framework to investigate the effect of visible watermarks on VLMs performance. We takes into account various factors, including different types of document data, the positions of watermarks within documents and variations in watermark content. Our experimental results reveal that VLMs performance can be significantly compromised by watermarks, with performance drop rates reaching up to 36\%. We discover that \emph{scattered} watermarks cause stronger interference than centralized ones, and that \emph{semantic contents} in watermarks creates greater disruption than simple visual occlusion. Through attention mechanism analysis and embedding similarity examination, we find that the performance drops are mainly attributed to that watermarks 1) force widespread attention redistribution, and 2) alter semantic representation in the embedding space. Our research not only highlights significant challenges in deploying VLMs for document understanding, but also provides insights towards developing robust inference mechanisms on watermarked documents.
<div id='section'>Paperid: <span id='pid'>186, <a href='https://arxiv.org/pdf/2311.08721.pdf' target='_blank'>https://arxiv.org/pdf/2311.08721.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Ren, Han Xu, Yiding Liu, Yingqian Cui, Shuaiqiang Wang, Dawei Yin, Jiliang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.08721">A Robust Semantics-based Watermark for Large Language Model against Paraphrasing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have show great ability in various natural language tasks. However, there are concerns that LLMs are possible to be used improperly or even illegally. To prevent the malicious usage of LLMs, detecting LLM-generated text becomes crucial in the deployment of LLM applications. Watermarking is an effective strategy to detect the LLM-generated content by encoding a pre-defined secret watermark to facilitate the detection process. However, the majority of existing watermark methods leverage the simple hashes of precedent tokens to partition vocabulary. Such watermark can be easily eliminated by paraphrase and correspondingly the detection effectiveness will be greatly compromised. Thus, to enhance the robustness against paraphrase, we propose a semantics-based watermark framework SemaMark. It leverages the semantics as an alternative to simple hashes of tokens since the paraphrase will likely preserve the semantic meaning of the sentences. Comprehensive experiments are conducted to demonstrate the effectiveness and robustness of SemaMark under different paraphrases.
<div id='section'>Paperid: <span id='pid'>187, <a href='https://arxiv.org/pdf/2510.16367.pdf' target='_blank'>https://arxiv.org/pdf/2510.16367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Li, Kejiang Chen, Jun Jiang, Jie Zhang, Qiyi Yao, Kai Zeng, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16367">EditMark: Watermarking Large Language Models based on Model Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities, but their training requires extensive data and computational resources, rendering them valuable digital assets. Therefore, it is essential to watermark LLMs to protect their copyright and trace unauthorized use or resale. Existing methods for watermarking LLMs primarily rely on training LLMs with a watermarked dataset, which entails burdensome training costs and negatively impacts the LLM's performance. In addition, their watermarked texts are not logical or natural, thereby reducing the stealthiness of the watermark. To address these issues, we propose EditMark, the first watermarking method that leverages model editing to embed a training-free, stealthy, and performance-lossless watermark for LLMs. We observe that some questions have multiple correct answers. Therefore, we assign each answer a unique watermark and update the weights of LLMs to generate corresponding questions and answers through the model editing technique. In addition, we refine the model editing technique to align with the requirements of watermark embedding. Specifically, we introduce an adaptive multi-round stable editing strategy, coupled with the injection of a noise matrix, to improve both the effectiveness and robustness of the watermark embedding. Extensive experiments indicate that EditMark can embed 32-bit watermarks into LLMs within 20 seconds (Fine-tuning: 6875 seconds) with a watermark extraction success rate of 100%, which demonstrates its effectiveness and efficiency. External experiments further demonstrate that EditMark has fidelity, stealthiness, and a certain degree of robustness against common attacks.
<div id='section'>Paperid: <span id='pid'>188, <a href='https://arxiv.org/pdf/2507.20762.pdf' target='_blank'>https://arxiv.org/pdf/2507.20762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Yuan, Chaoqun Yang, Yu Xing, Tong Chen, Nguyen Quoc Viet Hung, Hongzhi Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20762">Watermarking Large Language Model-based Time Series Forecasting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model-based Time Series Forecasting (LLMTS) has shown remarkable promise in handling complex and diverse temporal data, representing a significant step toward foundation models for time series analysis. However, this emerging paradigm introduces two critical challenges. First, the substantial commercial potential and resource-intensive development raise urgent concerns about intellectual property (IP) protection. Second, their powerful time series forecasting capabilities may be misused to produce misleading or fabricated deepfake time series data. To address these concerns, we explore watermarking the outputs of LLMTS models, that is, embedding imperceptible signals into the generated time series data that remain detectable by specialized algorithms. We propose a novel post-hoc watermarking framework, Waltz, which is broadly compatible with existing LLMTS models. Waltz is inspired by the empirical observation that time series patch embeddings are rarely aligned with a specific set of LLM tokens, which we term ``cold tokens''. Leveraging this insight, Waltz embeds watermarks by rewiring the similarity statistics between patch embeddings and cold token embeddings, and detects watermarks using similarity z-scores. To minimize potential side effects, we introduce a similarity-based embedding position identification strategy and employ projected gradient descent to constrain the watermark noise within a defined boundary. Extensive experiments using two popular LLMTS models across seven benchmark datasets demonstrate that Waltz achieves high watermark detection accuracy with minimal impact on the quality of the generated time series.
<div id='section'>Paperid: <span id='pid'>189, <a href='https://arxiv.org/pdf/2504.15026.pdf' target='_blank'>https://arxiv.org/pdf/2504.15026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijin Yang, Xin Zhang, Kejiang Chen, Kai Zeng, Qiyi Yao, Han Fang, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15026">Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. Existing methods primarily focus on ensuring that watermark embedding does not degrade the model performance. However, they often overlook critical challenges in real-world deployment scenarios, such as the complexity of watermark key management, user-defined generation parameters, and the difficulty of verification by arbitrary third parties. To address this issue, we propose Gaussian Shading++, a diffusion model watermarking method tailored for real-world deployment. We propose a double-channel design that leverages pseudorandom error-correcting codes to encode the random seed required for watermark pseudorandomization, achieving performance-lossless watermarking under a fixed watermark key and overcoming key management challenges. Additionally, we model the distortions introduced during generation and inversion as an additive white Gaussian noise channel and employ a novel soft decision decoding strategy during extraction, ensuring strong robustness even when generation parameters vary. To enable third-party verification, we incorporate public key signatures, which provide a certain level of resistance against forgery attacks even when model inversion capabilities are fully disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only maintains performance losslessness but also outperforms existing methods in terms of robustness, making it a more practical solution for real-world deployment.
<div id='section'>Paperid: <span id='pid'>190, <a href='https://arxiv.org/pdf/2412.08082.pdf' target='_blank'>https://arxiv.org/pdf/2412.08082.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongyi Zhang, Jie Zhang, Wenbo Zhou, Xinghui Zhou, Qing Guo, Weiming Zhang, Tianwei Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08082">FaceTracer: Unveiling Source Identities from Swapped Face Images and Videos for Fraud Prevention</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face-swapping techniques have advanced rapidly with the evolution of deep learning, leading to widespread use and growing concerns about potential misuse, especially in cases of fraud. While many efforts have focused on detecting swapped face images or videos, these methods are insufficient for tracing the malicious users behind fraudulent activities. Intrusive watermark-based approaches also fail to trace unmarked identities, limiting their practical utility. To address these challenges, we introduce FaceTracer, the first non-intrusive framework specifically designed to trace the identity of the source person from swapped face images or videos. Specifically, FaceTracer leverages a disentanglement module that effectively suppresses identity information related to the target person while isolating the identity features of the source person. This allows us to extract robust identity information that can directly link the swapped face back to the original individual, aiding in uncovering the actors behind fraudulent activities. Extensive experiments demonstrate FaceTracer's effectiveness across various face-swapping techniques, successfully identifying the source person in swapped content and enabling the tracing of malicious actors involved in fraudulent activities. Additionally, FaceTracer shows strong transferability to unseen face-swapping methods including commercial applications and robustness against transmission distortions and adaptive attacks.
<div id='section'>Paperid: <span id='pid'>191, <a href='https://arxiv.org/pdf/2405.04936.pdf' target='_blank'>https://arxiv.org/pdf/2405.04936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiwen Ren, Zehua Ma, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04936">SPSW: Database Watermarking Based on Fake Tuples and Sparse Priority Strategy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Databases play a crucial role in storing and managing vast amounts of data in various organizations and industries. Yet the risk of database leakage poses a significant threat to data privacy and security. To trace the source of database leakage, researchers have proposed many database watermarking schemes. Among them, fake-tuples-based database watermarking shows great potential as it does not modify the original data of the database, ensuring the seamless usability of the watermarked database. However, the existing fake-tuple-based database watermarking schemes need to insert a large number of fake tuples for the embedding of each watermark bit, resulting in low watermark transparency. Therefore, we propose a novel database watermarking scheme based on fake tuples and sparse priority strategy, named SPSW, which achieves the same watermark capacity with a lower number of inserted fake tuples compared to the existing embedding strategy. Specifically, for a database about to be watermarked, we prioritize embedding the sparsest watermark sequence, i.e., the sequence containing the most `0' bits among the currently available watermark sequences. For each bit in the sparse watermark sequence, when it is set to `1', SPSW will embed the corresponding set of fake tuples into the database. Otherwise, no modifications will be made to the database. Through theoretical analysis, the proposed sparse priority strategy not only improves transparency but also enhances the robustness of the watermark. The comparative experimental results with other database watermarking schemes further validate the superior performance of the proposed SPSW, aligning with the theoretical analysis.
<div id='section'>Paperid: <span id='pid'>192, <a href='https://arxiv.org/pdf/2404.04956.pdf' target='_blank'>https://arxiv.org/pdf/2404.04956.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04956">Gaussian Shading: Provable Performance-Lossless Image Watermarking for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. However, existing methods often compromise the model performance or require additional training, which is undesirable for operators and users. To address this issue, we propose Gaussian Shading, a diffusion model watermarking technique that is both performance-lossless and training-free, while serving the dual purpose of copyright protection and tracing of offending content. Our watermark embedding is free of model parameter modifications and thus is plug-and-play. We map the watermark to latent representations following a standard Gaussian distribution, which is indistinguishable from latent representations obtained from the non-watermarked diffusion model. Therefore we can achieve watermark embedding with lossless performance, for which we also provide theoretical proof. Furthermore, since the watermark is intricately linked with image semantics, it exhibits resilience to lossy processing and erasure attempts. The watermark can be extracted by Denoising Diffusion Implicit Models (DDIM) inversion and inverse sampling. We evaluate Gaussian Shading on multiple versions of Stable Diffusion, and the results demonstrate that Gaussian Shading not only is performance-lossless but also outperforms existing methods in terms of robustness.
<div id='section'>Paperid: <span id='pid'>193, <a href='https://arxiv.org/pdf/2312.06488.pdf' target='_blank'>https://arxiv.org/pdf/2312.06488.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Na Zhao, Kejiang Chen, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.06488">Performance-lossless Black-box Model Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of deep learning, high-value and high-cost models have become valuable assets, and related intellectual property protection technologies have become a hot topic. However, existing model watermarking work in black-box scenarios mainly originates from training-based backdoor methods, which probably degrade primary task performance. To address this, we propose a branch backdoor-based model watermarking protocol to protect model intellectual property, where a construction based on a message authentication scheme is adopted as the branch indicator after a comparative analysis with secure cryptographic technologies primitives. We prove the lossless performance of the protocol by reduction. In addition, we analyze the potential threats to the protocol and provide a secure and feasible watermarking instance for language models.
<div id='section'>Paperid: <span id='pid'>194, <a href='https://arxiv.org/pdf/2311.09535.pdf' target='_blank'>https://arxiv.org/pdf/2311.09535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuai Li, Kejiang Chen, Kunsheng Tang, Jie Zhang, Weiming Zhang, Nenghai Yu, Kai Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09535">Turning Your Strength into Watermark: Watermarking Large Language Model via Knowledge Injection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have demonstrated outstanding performance, making them valuable digital assets with significant commercial potential. Unfortunately, the LLM and its API are susceptible to intellectual property theft. Watermarking is a classic solution for copyright verification. However, most recent emerging LLM watermarking methods focus on identifying AI-generated texts rather than watermarking LLM itself. Only a few attempts are based on weight quantification and backdoor watermarking, which are not robust or covert enough, limiting their applicability in practice.
  To address this issue, we propose a novel watermarking method for LLMs based on knowledge injection and innovatively use knowledge as the watermark carrier. Specifically, in the watermark embedding stage, we first embed the watermarks into the selected knowledge to obtain the watermarked knowledge, subsequently injected into the to-be-protected LLM. In the watermark extraction stage, questions related to the watermarked knowledge are designed, for querying the suspect LLM and extracting the watermarks from its response. The experiments show that the watermark extraction success rate is close to 100% and demonstrate the effectiveness, fidelity, stealthiness, and robustness of our proposed method.
<div id='section'>Paperid: <span id='pid'>195, <a href='https://arxiv.org/pdf/2309.05940.pdf' target='_blank'>https://arxiv.org/pdf/2309.05940.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitao Feng, Jiyan He, Jie Zhang, Tianwei Zhang, Wenbo Zhou, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.05940">Catch You Everything Everywhere: Guarding Textual Inversion via Concept Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AIGC (AI-Generated Content) has achieved tremendous success in many applications such as text-to-image tasks, where the model can generate high-quality images with diverse prompts, namely, different descriptions in natural languages. More surprisingly, the emerging personalization techniques even succeed in describing unseen concepts with only a few personal images as references, and there have been some commercial platforms for sharing the valuable personalized concept. However, such an advanced technique also introduces a severe threat, where malicious users can misuse the target concept to generate highly-realistic illegal images. Therefore, it becomes necessary for the platform to trace malicious users and hold them accountable.
  In this paper, we focus on guarding the most popular lightweight personalization model, ie, Textual Inversion (TI). To achieve it, we propose the novel concept watermarking, where watermark information is embedded into the target concept and then extracted from generated images based on the watermarked concept. Specifically, we jointly train a watermark encoder and a watermark decoder with the sampler in the loop.
  It shows great resilience to different diffusion sampling processes possibly chosen by malicious users, meanwhile preserving utility for normal use. In practice, the concept owner can upload his concept with different watermarks (ie, serial numbers) to the platform, and the platform allocates different users with different serial numbers for subsequent tracing and forensics.
<div id='section'>Paperid: <span id='pid'>196, <a href='https://arxiv.org/pdf/2308.12141.pdf' target='_blank'>https://arxiv.org/pdf/2308.12141.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhe Lei, Jie Zhang, Jingtao Li, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12141">Aparecium: Revealing Secrets from Physical Photographs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is a crucial tool for safeguarding copyrights and can serve as a more aesthetically pleasing alternative to QR codes. In recent years, watermarking methods based on deep learning have proved superior robustness against complex physical distortions than traditional watermarking methods. However, they have certain limitations that render them less effective in practice. For instance, current solutions necessitate physical photographs to be rectangular for accurate localization, cannot handle physical bending or folding, and require the hidden area to be completely captured at a close distance and small angle. To overcome these challenges, we propose a novel deep watermarking framework dubbed \textit{Aparecium}. Specifically, we preprocess secrets (i.e., watermarks) into a pattern and then embed it into the cover image, which is symmetrical to the final decoding-then-extracting process. To capture the watermarked region from complex physical scenarios, a locator is also introduced. Besides, we adopt a three-stage training strategy for training convergence. Extensive experiments demonstrate that \textit{Aparecium} is not only robust against different digital distortions, but also can resist various physical distortions, such as screen-shooting and printing-shooting, even in severe cases including different shapes, curvature, folding, incompleteness, long distances, and big angles while maintaining high visual quality. Furthermore, some ablation studies are also conducted to verify our design.
<div id='section'>Paperid: <span id='pid'>197, <a href='https://arxiv.org/pdf/2212.02339.pdf' target='_blank'>https://arxiv.org/pdf/2212.02339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chang Liu, Jie Zhang, Han Fang, Zehua Ma, Weiming Zhang, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2212.02339">DeAR: A Deep-learning-based Audio Re-recording Resilient Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio watermarking is widely used for leaking source tracing. The robustness of the watermark determines the traceability of the algorithm. With the development of digital technology, audio re-recording (AR) has become an efficient and covert means to steal secrets. AR process could drastically destroy the watermark signal while preserving the original information. This puts forward a new requirement for audio watermarking at this stage, that is, to be robust to AR distortions. Unfortunately, none of the existing algorithms can effectively resist AR attacks due to the complexity of the AR process. To address this limitation, this paper proposes DeAR, a deep-learning-based audio re-recording resistant watermarking. Inspired by DNN-based image watermarking, we pioneer a deep learning framework for audio carriers, based on which the watermark signal can be effectively embedded and extracted. Meanwhile, in order to resist the AR attack, we delicately analyze the distortions that occurred in the AR process and design the corresponding distortion layer to cooperate with the proposed watermarking framework. Extensive experiments show that the proposed algorithm can resist not only common electronic channel distortions but also AR distortions. Under the premise of high-quality embedding (SNR=25.86dB), in the case of a common re-recording distance (20cm), the algorithm can effectively achieve an average bit recovery accuracy of 98.55%.
<div id='section'>Paperid: <span id='pid'>198, <a href='https://arxiv.org/pdf/2007.10240.pdf' target='_blank'>https://arxiv.org/pdf/2007.10240.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zehua Ma, Weiming Zhang, Han Fang, Xiaoyi Dong, Linfeng Geng, Nenghai Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2007.10240">Local Geometric Distortions Resilient Watermarking Scheme Based on Symmetry</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As an efficient watermark attack method, geometric distortions destroy the synchronization between watermark encoder and decoder. And the local geometric distortion is a famous challenge in the watermark field. Although a lot of geometric distortions resilient watermarking schemes have been proposed, few of them perform well against local geometric distortion like random bending attack (RBA). To address this problem, this paper proposes a novel watermark synchronization process and the corresponding watermarking scheme. In our scheme, the watermark bits are represented by random patterns. The message is encoded to get a watermark unit, and the watermark unit is flipped to generate a symmetrical watermark. Then the symmetrical watermark is embedded into the spatial domain of the host image in an additive way. In watermark extraction, we first get the theoretically mean-square error minimized estimation of the watermark. Then the auto-convolution function is applied to this estimation to detect the symmetry and get a watermark units map. According to this map, the watermark can be accurately synchronized, and then the extraction can be done. Experimental results demonstrate the excellent robustness of the proposed watermarking scheme to local geometric distortions, global geometric distortions, common image processing operations, and some kinds of combined attacks.
<div id='section'>Paperid: <span id='pid'>199, <a href='https://arxiv.org/pdf/2511.19316.pdf' target='_blank'>https://arxiv.org/pdf/2511.19316.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xincheng Wang, Hanchi Sun, Wenjun Sun, Kejun Xue, Wangqiu Zhou, Jianbo Zhang, Wei Sun, Dandan Zhu, Xiongkuo Min, Jun Jia, Zhijun Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.19316">Evaluating Dataset Watermarking for Fine-tuning Traceability of Customized Diffusion Models: A Comprehensive Benchmark and Removal Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent fine-tuning techniques for diffusion models enable them to reproduce specific image sets, such as particular faces or artistic styles, but also introduce copyright and security risks. Dataset watermarking has been proposed to ensure traceability by embedding imperceptible watermarks into training images, which remain detectable in outputs even after fine-tuning. However, current methods lack a unified evaluation framework. To address this, this paper establishes a general threat model and introduces a comprehensive evaluation framework encompassing Universality, Transmissibility, and Robustness. Experiments show that existing methods perform well in universality and transmissibility, and exhibit some robustness against common image processing operations, yet still fall short under real-world threat scenarios. To reveal these vulnerabilities, the paper further proposes a practical watermark removal method that fully eliminates dataset watermarks without affecting fine-tuning, highlighting a key challenge for future research.
<div id='section'>Paperid: <span id='pid'>200, <a href='https://arxiv.org/pdf/2506.22960.pdf' target='_blank'>https://arxiv.org/pdf/2506.22960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shreyas Dixit, Ashhar Aziz, Shashwat Bajpai, Vasu Sharma, Aman Chadha, Vinija Jain, Amitava Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22960">Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A report by the European Union Law Enforcement Agency predicts that by 2026, up to 90 percent of online content could be synthetically generated, raising concerns among policymakers, who cautioned that "Generative AI could act as a force multiplier for political disinformation. The combined effect of generative text, images, videos, and audio may surpass the influence of any single modality." In response, California's Bill AB 3211 mandates the watermarking of AI-generated images, videos, and audio. However, concerns remain regarding the vulnerability of invisible watermarking techniques to tampering and the potential for malicious actors to bypass them entirely. Generative AI-powered de-watermarking attacks, especially the newly introduced visual paraphrase attack, have shown an ability to fully remove watermarks, resulting in a paraphrase of the original image. This paper introduces PECCAVI, the first visual paraphrase attack-safe and distortion-free image watermarking technique. In visual paraphrase attacks, an image is altered while preserving its core semantic regions, termed Non-Melting Points (NMPs). PECCAVI strategically embeds watermarks within these NMPs and employs multi-channel frequency domain watermarking. It also incorporates noisy burnishing to counter reverse-engineering efforts aimed at locating NMPs to disrupt the embedded watermark, thereby enhancing durability. PECCAVI is model-agnostic. All relevant resources and codes will be open-sourced.
<div id='section'>Paperid: <span id='pid'>201, <a href='https://arxiv.org/pdf/2408.10446.pdf' target='_blank'>https://arxiv.org/pdf/2408.10446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Niyar R Barman, Krish Sharma, Ashhar Aziz, Shashwat Bajpai, Shwetangshu Biswas, Vasu Sharma, Vinija Jain, Aman Chadha, Amit Sheth, Amitava Das
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.10446">The Brittleness of AI-Generated Image Watermarking Techniques: Examining Their Robustness Against Visual Paraphrasing Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of text-to-image generation systems, exemplified by models like Stable Diffusion, Midjourney, Imagen, and DALL-E, has heightened concerns about their potential misuse. In response, companies like Meta and Google have intensified their efforts to implement watermarking techniques on AI-generated images to curb the circulation of potentially misleading visuals. However, in this paper, we argue that current image watermarking methods are fragile and susceptible to being circumvented through visual paraphrase attacks. The proposed visual paraphraser operates in two steps. First, it generates a caption for the given image using KOSMOS-2, one of the latest state-of-the-art image captioning systems. Second, it passes both the original image and the generated caption to an image-to-image diffusion system. During the denoising step of the diffusion pipeline, the system generates a visually similar image that is guided by the text caption. The resulting image is a visual paraphrase and is free of any watermarks. Our empirical findings demonstrate that visual paraphrase attacks can effectively remove watermarks from images. This paper provides a critical assessment, empirically revealing the vulnerability of existing watermarking techniques to visual paraphrase attacks. While we do not propose solutions to this issue, this paper serves as a call to action for the scientific community to prioritize the development of more robust watermarking techniques. Our first-of-its-kind visual paraphrase dataset and accompanying code are publicly available.
<div id='section'>Paperid: <span id='pid'>202, <a href='https://arxiv.org/pdf/2412.05695.pdf' target='_blank'>https://arxiv.org/pdf/2412.05695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqi Tan, Xiang Liu, Shuzhao Xie, Bin Chen, Shu-Tao Xia, Zhi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.05695">WATER-GS: Toward Copyright Protection for 3D Gaussian Splatting via Universal Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has emerged as a pivotal technique for 3D scene representation, providing rapid rendering speeds and high fidelity. As 3DGS gains prominence, safeguarding its intellectual property becomes increasingly crucial since 3DGS could be used to imitate unauthorized scene creations and raise copyright issues. Existing watermarking methods for implicit NeRFs cannot be directly applied to 3DGS due to its explicit representation and real-time rendering process, leaving watermarking for 3DGS largely unexplored. In response, we propose WATER-GS, a novel method designed to protect 3DGS copyrights through a universal watermarking strategy. First, we introduce a pre-trained watermark decoder, treating raw 3DGS generative modules as potential watermark encoders to ensure imperceptibility. Additionally, we implement novel 3D distortion layers to enhance the robustness of the embedded watermark against common real-world distortions of point cloud data. Comprehensive experiments and ablation studies demonstrate that WATER-GS effectively embeds imperceptible and robust watermarks into 3DGS without compromising rendering efficiency and quality. Our experiments indicate that the 3D distortion layers can yield up to a 20% improvement in accuracy rate. Notably, our method is adaptable to different 3DGS variants, including 3DGS compression frameworks and 2D Gaussian splatting.
<div id='section'>Paperid: <span id='pid'>203, <a href='https://arxiv.org/pdf/2508.08667.pdf' target='_blank'>https://arxiv.org/pdf/2508.08667.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ke Liu, Xuanhan Wang, Qilong Zhang, Lianli Gao, Jingkuan Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.08667">Learning Generalizable and Efficient Image Watermarking via Hierarchical Two-Stage Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep image watermarking, which refers to enable imperceptible watermark embedding and reliable extraction in cover images, has shown to be effective for copyright protection of image assets. However, existing methods face limitations in simultaneously satisfying three essential criteria for generalizable watermarking: 1) invisibility (imperceptible hide of watermarks), 2) robustness (reliable watermark recovery under diverse conditions), and 3) broad applicability (low latency in watermarking process). To address these limitations, we propose a Hierarchical Watermark Learning (HiWL), a two-stage optimization that enable a watermarking model to simultaneously achieve three criteria. In the first stage, distribution alignment learning is designed to establish a common latent space with two constraints: 1) visual consistency between watermarked and non-watermarked images, and 2) information invariance across watermark latent representations. In this way, multi-modal inputs including watermark message (binary codes) and cover images (RGB pixels) can be well represented, ensuring the invisibility of watermarks and robustness in watermarking process thereby. The second stage employs generalized watermark representation learning to establish a disentanglement policy for separating watermarks from image content in RGB space. In particular, it strongly penalizes substantial fluctuations in separated RGB watermarks corresponding to identical messages. Consequently, HiWL effectively learns generalizable latent-space watermark representations while maintaining broad applicability. Extensive experiments demonstrate the effectiveness of proposed method. In particular, it achieves 7.6\% higher accuracy in watermark extraction than existing methods, while maintaining extremely low latency (100K images processed in 8s).
<div id='section'>Paperid: <span id='pid'>204, <a href='https://arxiv.org/pdf/2507.14067.pdf' target='_blank'>https://arxiv.org/pdf/2507.14067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuliang Liu, Qi Zheng, Jesse Jiaxi Xu, Yibo Yan, Junyan Zhang, He Geng, Aiwei Liu, Peijie Jiang, Jia Liu, Yik-Cheung Tam, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.14067">VLA-Mark: A cross modal watermark for large vision-language alignment model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Vision-language models demand watermarking solutions that protect intellectual property without compromising multimodal coherence. Existing text watermarking methods disrupt visual-textual alignment through biased token selection and static strategies, leaving semantic-critical concepts vulnerable. We propose VLA-Mark, a vision-aligned framework that embeds detectable watermarks while preserving semantic fidelity through cross-modal coordination. Our approach integrates multiscale visual-textual alignment metrics, combining localized patch affinity, global semantic coherence, and contextual attention patterns, to guide watermark injection without model retraining. An entropy-sensitive mechanism dynamically balances watermark strength and semantic preservation, prioritizing visual grounding during low-uncertainty generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than conventional methods, with near-perfect detection (98.8% AUC). The framework demonstrates 96.1\% attack resilience against attacks such as paraphrasing and synonym substitution, while maintaining text-visual consistency, establishing new standards for quality-preserving multimodal watermarking
<div id='section'>Paperid: <span id='pid'>205, <a href='https://arxiv.org/pdf/2504.12108.pdf' target='_blank'>https://arxiv.org/pdf/2504.12108.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shizhan Cai, Liang Ding, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12108">Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of Large Language Models (LLMs) has intensified concerns about content traceability and potential misuse. Existing watermarking schemes for sampled text often face trade-offs between maintaining text quality and ensuring robust detection against various attacks. To address these issues, we propose a novel watermarking scheme that improves both detectability and text quality by introducing a cumulative watermark entropy threshold. Our approach is compatible with and generalizes existing sampling functions, enhancing adaptability. Experimental results across multiple LLMs show that our scheme significantly outperforms existing methods, achieving over 80\% improvements on widely-used datasets, e.g., MATH and GSM8K, while maintaining high detection accuracy.
<div id='section'>Paperid: <span id='pid'>206, <a href='https://arxiv.org/pdf/2505.19081.pdf' target='_blank'>https://arxiv.org/pdf/2505.19081.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyang Xia, Dawei Zhou, Decheng Liu, Lin Yuan, Jie Li, Nannan Wang, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19081">Towards Generalized Proactive Defense against Face Swapping with Contour-Hybrid Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Face swapping, recognized as a privacy and security concern, has prompted considerable defensive research. With the advancements in AI-generated content, the discrepancies between the real and swapped faces have become nuanced. Considering the difficulty of forged traces detection, we shift the focus to the face swapping purpose and proactively embed elaborate watermarks against unknown face swapping techniques. Given that the constant purpose is to swap the original face identity while preserving the background, we concentrate on the regions surrounding the face to ensure robust watermark generation, while embedding the contour texture and face identity information to achieve progressive image determination. The watermark is located in the facial contour and contains hybrid messages, dubbed the contour-hybrid watermark (CMark). Our approach generalizes face swapping detection without requiring any swapping techniques during training and the storage of large-scale messages in advance. Experiments conducted across 8 face swapping techniques demonstrate the superiority of our approach compared with state-of-the-art passive and proactive detectors while achieving a favorable balance between the image quality and watermark robustness.
<div id='section'>Paperid: <span id='pid'>207, <a href='https://arxiv.org/pdf/2408.01354.pdf' target='_blank'>https://arxiv.org/pdf/2408.01354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiwen Ning, Jiachi Chen, Qingyuan Zhong, Tao Zhang, Yanlin Wang, Wei Li, Jingwen Zhang, Jianxing Yu, Yuming Feng, Weizhe Zhang, Zibin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.01354">MCGMark: An Encodable and Robust Online Watermark for Tracing LLM-Generated Malicious Code</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advent of large language models (LLMs), numerous software service providers (SSPs) are dedicated to developing LLMs customized for code generation tasks, such as CodeLlama and Copilot. However, these LLMs can be leveraged by attackers to create malicious software, which may pose potential threats to the software ecosystem. For example, they can automate the creation of advanced phishing malware. To address this issue, we first conduct an empirical study and design a prompt dataset, MCGTest, which involves approximately 400 person-hours of work and consists of 406 malicious code generation tasks. Utilizing this dataset, we propose MCGMark, the first robust, code structure-aware, and encodable watermarking approach to trace LLM-generated code. We embed encodable information by controlling the token selection and ensuring the output quality based on probabilistic outliers. Additionally, we enhance the robustness of the watermark by considering the structural features of malicious code, preventing the embedding of the watermark in easily modified positions, such as comments. We validate the effectiveness and robustness of MCGMark on the DeepSeek-Coder. MCGMark achieves an embedding success rate of 88.9% within a maximum output limit of 400 tokens. Furthermore, it also demonstrates strong robustness and has minimal impact on the quality of the output code. Our approach assists SSPs in tracing and holding responsible parties accountable for malicious code generated by LLMs.
<div id='section'>Paperid: <span id='pid'>208, <a href='https://arxiv.org/pdf/2407.01251.pdf' target='_blank'>https://arxiv.org/pdf/2407.01251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huajie Chen, Tianqing Zhu, Lefeng Zhang, Bo Liu, Derui Wang, Wanlei Zhou, Minhui Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01251">QUEEN: Query Unlearning against Model Extraction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model extraction attacks currently pose a non-negligible threat to the security and privacy of deep learning models. By querying the model with a small dataset and usingthe query results as the ground-truth labels, an adversary can steal a piracy model with performance comparable to the original model. Two key issues that cause the threat are, on the one hand, accurate and unlimited queries can be obtained by the adversary; on the other hand, the adversary can aggregate the query results to train the model step by step. The existing defenses usually employ model watermarking or fingerprinting to protect the ownership. However, these methods cannot proactively prevent the violation from happening. To mitigate the threat, we propose QUEEN (QUEry unlEarNing) that proactively launches counterattacks on potential model extraction attacks from the very beginning. To limit the potential threat, QUEEN has sensitivity measurement and outputs perturbation that prevents the adversary from training a piracy model with high performance. In sensitivity measurement, QUEEN measures the single query sensitivity by its distance from the center of its cluster in the feature space. To reduce the learning accuracy of attacks, for the highly sensitive query batch, QUEEN applies query unlearning, which is implemented by gradient reverse to perturb the softmax output such that the piracy model will generate reverse gradients to worsen its performance unconsciously. Experiments show that QUEEN outperforms the state-of-the-art defenses against various model extraction attacks with a relatively low cost to the model accuracy. The artifact is publicly available at https://anonymous.4open.science/r/queen implementation-5408/.
<div id='section'>Paperid: <span id='pid'>209, <a href='https://arxiv.org/pdf/2302.08637.pdf' target='_blank'>https://arxiv.org/pdf/2302.08637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huajie Chen, Tianqing Zhu, Chi Liu, Shui Yu, Wanlei Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.08637">High-frequency Matters: An Overwriting Attack and defense for Image-processing Neural Network Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been significant advancement in the field of model watermarking techniques. However, the protection of image-processing neural networks remains a challenge, with only a limited number of methods being developed. The objective of these techniques is to embed a watermark in the output images of the target generative network, so that the watermark signal can be detected in the output of a surrogate model obtained through model extraction attacks. This promising technique, however, has certain limits. Analysis of the frequency domain reveals that the watermark signal is mainly concealed in the high-frequency components of the output. Thus, we propose an overwriting attack that involves forging another watermark in the output of the generative network. The experimental results demonstrate the efficacy of this attack in sabotaging existing watermarking schemes for image-processing networks, with an almost 100% success rate. To counter this attack, we devise an adversarial framework for the watermarking network. The framework incorporates a specially designed adversarial training step, where the watermarking network is trained to defend against the overwriting network, thereby enhancing its robustness. Additionally, we observe an overfitting phenomenon in the existing watermarking method, which can render it ineffective. To address this issue, we modify the training process to eliminate the overfitting problem.
<div id='section'>Paperid: <span id='pid'>210, <a href='https://arxiv.org/pdf/2506.13494.pdf' target='_blank'>https://arxiv.org/pdf/2506.13494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yugeng Liu, Tianshuo Cong, Michael Backes, Zheng Li, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.13494">Watermarking LLM-Generated Datasets in Downstream Tasks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have experienced rapid advancements, with applications spanning a wide range of fields, including sentiment classification, review generation, and question answering. Due to their efficiency and versatility, researchers and companies increasingly employ LLM-generated data to train their models. However, the inability to track content produced by LLMs poses a significant challenge, potentially leading to copyright infringement for the LLM owners. In this paper, we propose a method for injecting watermarks into LLM-generated datasets, enabling the tracking of downstream tasks to detect whether these datasets were produced using the original LLM. These downstream tasks can be divided into two categories. The first involves using the generated datasets at the input level, commonly for training classification tasks. The other is the output level, where model trainers use LLM-generated content as output for downstream tasks, such as question-answering tasks. We design a comprehensive set of experiments to evaluate both watermark methods. Our results indicate the high effectiveness of our watermark approach. Additionally, regarding model utility, we find that classifiers trained on the generated datasets achieve a test accuracy exceeding 0.900 in many cases, suggesting that the utility of such models remains robust. For the output-level watermark, we observe that the quality of the generated text is comparable to that produced using real-world datasets. Through our research, we aim to advance the protection of LLM copyrights, taking a significant step forward in safeguarding intellectual property in this domain.
<div id='section'>Paperid: <span id='pid'>211, <a href='https://arxiv.org/pdf/2306.07754.pdf' target='_blank'>https://arxiv.org/pdf/2306.07754.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Ma, Zhengyu Zhao, Xinlei He, Zheng Li, Michael Backes, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.07754">Generative Watermarking Against Unauthorized Subject-Driven Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large text-to-image models have shown remarkable performance in synthesizing high-quality images. In particular, the subject-driven model makes it possible to personalize the image synthesis for a specific subject, e.g., a human face or an artistic style, by fine-tuning the generic text-to-image model with a few images from that subject. Nevertheless, misuse of subject-driven image synthesis may violate the authority of subject owners. For example, malicious users may use subject-driven synthesis to mimic specific artistic styles or to create fake facial images without authorization. To protect subject owners against such misuse, recent attempts have commonly relied on adversarial examples to indiscriminately disrupt subject-driven image synthesis. However, this essentially prevents any benign use of subject-driven synthesis based on protected images.
  In this paper, we take a different angle and aim at protection without sacrificing the utility of protected images for general synthesis purposes. Specifically, we propose GenWatermark, a novel watermark system based on jointly learning a watermark generator and a detector. In particular, to help the watermark survive the subject-driven synthesis, we incorporate the synthesis process in learning GenWatermark by fine-tuning the detector with synthesized images for a specific subject. This operation is shown to largely improve the watermark detection accuracy and also ensure the uniqueness of the watermark for each individual subject. Extensive experiments validate the effectiveness of GenWatermark, especially in practical scenarios with unknown models and text prompts (74% Acc.), as well as partial data watermarking (80% Acc. for 1/4 watermarking). We also demonstrate the robustness of GenWatermark to two potential countermeasures that substantially degrade the synthesis quality.
<div id='section'>Paperid: <span id='pid'>212, <a href='https://arxiv.org/pdf/2305.12502.pdf' target='_blank'>https://arxiv.org/pdf/2305.12502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, Yang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12502">Watermarking Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability and accessibility of diffusion models (DMs) have significantly increased in recent years, making them a popular tool for analyzing and predicting the spread of information, behaviors, or phenomena through a population. Particularly, text-to-image diffusion models (e.g., DALLE 2 and Latent Diffusion Models (LDMs) have gained significant attention in recent years for their ability to generate high-quality images and perform various image synthesis tasks. Despite their widespread adoption in many fields, DMs are often susceptible to various intellectual property violations. These can include not only copyright infringement but also more subtle forms of misappropriation, such as unauthorized use or modification of the model. Therefore, DM owners must be aware of these potential risks and take appropriate steps to protect their models. In this work, we are the first to protect the intellectual property of DMs. We propose a simple but effective watermarking scheme that injects the watermark into the DMs and can be verified by the pre-defined prompts. In particular, we propose two different watermarking methods, namely NAIVEWM and FIXEDWM. The NAIVEWM method injects the watermark into the LDMs and activates it using a prompt containing the watermark. On the other hand, the FIXEDWM is considered more advanced and stealthy compared to the NAIVEWM, as it can only activate the watermark when using a prompt containing a trigger in a fixed position. We conducted a rigorous evaluation of both approaches, demonstrating their effectiveness in watermark injection and verification with minimal impact on the LDM's functionality.
<div id='section'>Paperid: <span id='pid'>213, <a href='https://arxiv.org/pdf/2504.12739.pdf' target='_blank'>https://arxiv.org/pdf/2504.12739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyi Hu, Jie Zhang, Shiqian Zhao, Nils Lukas, Jiwei Li, Qing Guo, Han Qiu, Tianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12739">Mask Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MaskMark, a simple, efficient, and flexible framework for image watermarking. MaskMark has two variants: (1) MaskMark-D, which supports global watermark embedding, watermark localization, and local watermark extraction for applications such as tamper detection; (2) MaskMark-ED, which focuses on local watermark embedding and extraction, offering enhanced robustness in small regions to support fine-grined image protection. MaskMark-D builds on the classical encoder-distortion layer-decoder training paradigm. In MaskMark-D, we introduce a simple masking mechanism during the decoding stage that enables both global and local watermark extraction. During training, the decoder is guided by various types of masks applied to watermarked images before extraction, helping it learn to localize watermarks and extract them from the corresponding local areas. MaskMark-ED extends this design by incorporating the mask into the encoding stage as well, guiding the encoder to embed the watermark in designated local regions, which improves robustness under regional attacks. Extensive experiments show that MaskMark achieves state-of-the-art performance in global and local watermark extraction, watermark localization, and multi-watermark embedding. It outperforms all existing baselines, including the recent leading model WAM for local watermarking, while preserving high visual quality of the watermarked images. In addition, MaskMark is highly efficient and adaptable. It requires only 20 hours of training on a single A6000 GPU, achieving 15x computational efficiency compared to WAM. By simply adjusting the distortion layer, MaskMark can be quickly fine-tuned to meet varying robustness requirements.
<div id='section'>Paperid: <span id='pid'>214, <a href='https://arxiv.org/pdf/2412.10049.pdf' target='_blank'>https://arxiv.org/pdf/2412.10049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyi Hu, Jie Zhang, Yiming Li, Jiwei Li, Qing Guo, Han Qiu, Tianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10049">SuperMark: Robust and Training-free Image Watermarking via Diffusion-based Super-Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In today's digital landscape, the blending of AI-generated and authentic content has underscored the need for copyright protection and content authentication. Watermarking has become a vital tool to address these challenges, safeguarding both generated and real content. Effective watermarking methods must withstand various distortions and attacks. Current deep watermarking techniques often use an encoder-noise layer-decoder architecture and include distortions to enhance robustness. However, they struggle to balance robustness and fidelity and remain vulnerable to adaptive attacks, despite extensive training. To overcome these limitations, we propose SuperMark, a robust, training-free watermarking framework. Inspired by the parallels between watermark embedding/extraction in watermarking and the denoising/noising processes in diffusion models, SuperMark embeds the watermark into initial Gaussian noise using existing techniques. It then applies pre-trained Super-Resolution (SR) models to denoise the watermarked noise, producing the final watermarked image. For extraction, the process is reversed: the watermarked image is inverted back to the initial watermarked noise via DDIM Inversion, from which the embedded watermark is extracted. This flexible framework supports various noise injection methods and diffusion-based SR models, enabling enhanced customization. The robustness of the DDIM Inversion process against perturbations allows SuperMark to achieve strong resilience to distortions while maintaining high fidelity. Experiments demonstrate that SuperMark achieves fidelity comparable to existing methods while significantly improving robustness. Under standard distortions, it achieves an average watermark extraction accuracy of 99.46%, and 89.29% under adaptive attacks. Moreover, SuperMark shows strong transferability across datasets, SR models, embedding methods, and resolutions.
<div id='section'>Paperid: <span id='pid'>215, <a href='https://arxiv.org/pdf/2409.19627.pdf' target='_blank'>https://arxiv.org/pdf/2409.19627.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pengcheng Li, Xulong Zhang, Jing Xiao, Jianzong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19627">IDEAW: Robust Neural Audio Watermarking with Invertible Dual-Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The audio watermarking technique embeds messages into audio and accurately extracts messages from the watermarked audio. Traditional methods develop algorithms based on expert experience to embed watermarks into the time-domain or transform-domain of signals. With the development of deep neural networks, deep learning-based neural audio watermarking has emerged. Compared to traditional algorithms, neural audio watermarking achieves better robustness by considering various attacks during training. However, current neural watermarking methods suffer from low capacity and unsatisfactory imperceptibility. Additionally, the issue of watermark locating, which is extremely important and even more pronounced in neural audio watermarking, has not been adequately studied. In this paper, we design a dual-embedding watermarking model for efficient locating. We also consider the impact of the attack layer on the invertible neural network in robustness training, improving the model to enhance both its reasonableness and stability. Experiments show that the proposed model, IDEAW, can withstand various attacks with higher capacity and more efficient locating ability compared to existing methods.
<div id='section'>Paperid: <span id='pid'>216, <a href='https://arxiv.org/pdf/2310.02401.pdf' target='_blank'>https://arxiv.org/pdf/2310.02401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingqian Cui, Jie Ren, Yuping Lin, Han Xu, Pengfei He, Yue Xing, Lingjuan Lyu, Wenqi Fan, Hui Liu, Jiliang Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.02401">FT-Shield: A Watermark Against Unauthorized Fine-tuning in Text-to-Image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image generative models, especially those based on latent diffusion models (LDMs), have demonstrated outstanding ability in generating high-quality and high-resolution images from textual prompts. With this advancement, various fine-tuning methods have been developed to personalize text-to-image models for specific applications such as artistic style adaptation and human face transfer. However, such advancements have raised copyright concerns, especially when the data are used for personalization without authorization. For example, a malicious user can employ fine-tuning techniques to replicate the style of an artist without consent. In light of this concern, we propose FT-Shield, a watermarking solution tailored for the fine-tuning of text-to-image diffusion models. FT-Shield addresses copyright protection challenges by designing new watermark generation and detection strategies. In particular, it introduces an innovative algorithm for watermark generation. It ensures the seamless transfer of watermarks from training images to generated outputs, facilitating the identification of copyrighted material use. To tackle the variability in fine-tuning methods and their impact on watermark detection, FT-Shield integrates a Mixture of Experts (MoE) approach for watermark detection. Comprehensive experiments validate the effectiveness of our proposed FT-Shield.
<div id='section'>Paperid: <span id='pid'>217, <a href='https://arxiv.org/pdf/2506.14474.pdf' target='_blank'>https://arxiv.org/pdf/2506.14474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eyal German, Sagiv Antebi, Edan Habler, Asaf Shabtai, Yuval Elovici
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14474">LexiMark: Robust Watermarking via Lexical Substitutions to Enhance Membership Verification of an LLM's Textual Training Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) can be trained or fine-tuned on data obtained without the owner's consent. Verifying whether a specific LLM was trained on particular data instances or an entire dataset is extremely challenging. Dataset watermarking addresses this by embedding identifiable modifications in training data to detect unauthorized use. However, existing methods often lack stealth, making them relatively easy to detect and remove. In light of these limitations, we propose LexiMark, a novel watermarking technique designed for text and documents, which embeds synonym substitutions for carefully selected high-entropy words. Our method aims to enhance an LLM's memorization capabilities on the watermarked text without altering the semantic integrity of the text. As a result, the watermark is difficult to detect, blending seamlessly into the text with no visible markers, and is resistant to removal due to its subtle, contextually appropriate substitutions that evade automated and manual detection. We evaluated our method using baseline datasets from recent studies and seven open-source models: LLaMA-1 7B, LLaMA-3 8B, Mistral 7B, Pythia 6.9B, as well as three smaller variants from the Pythia family (160M, 410M, and 1B). Our evaluation spans multiple training settings, including continued pretraining and fine-tuning scenarios. The results demonstrate significant improvements in AUROC scores compared to existing methods, underscoring our method's effectiveness in reliably verifying whether unauthorized watermarked data was used in LLM training.
<div id='section'>Paperid: <span id='pid'>218, <a href='https://arxiv.org/pdf/2409.08087.pdf' target='_blank'>https://arxiv.org/pdf/2409.08087.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benji Peng, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Junyu Liu, Qian Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.08087">Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.
<div id='section'>Paperid: <span id='pid'>219, <a href='https://arxiv.org/pdf/2511.07947.pdf' target='_blank'>https://arxiv.org/pdf/2511.07947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaxin Xiao, Qingqing Ye, Zi Liang, Haoyang Li, RongHua Li, Huadi Zheng, Haibo Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.07947">Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks. For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.
<div id='section'>Paperid: <span id='pid'>220, <a href='https://arxiv.org/pdf/2508.10065.pdf' target='_blank'>https://arxiv.org/pdf/2508.10065.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhao Sun, Yihua Zhang, Gaowen Liu, Hongtao Xie, Sijia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.10065">Invisible Watermarks, Visible Gains: Steering Machine Unlearning with Bi-Level Watermarking Design</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing demand for the right to be forgotten, machine unlearning (MU) has emerged as a vital tool for enhancing trust and regulatory compliance by enabling the removal of sensitive data influences from machine learning (ML) models. However, most MU algorithms primarily rely on in-training methods to adjust model weights, with limited exploration of the benefits that data-level adjustments could bring to the unlearning process. To address this gap, we propose a novel approach that leverages digital watermarking to facilitate MU by strategically modifying data content. By integrating watermarking, we establish a controlled unlearning mechanism that enables precise removal of specified data while maintaining model utility for unrelated tasks. We first examine the impact of watermarked data on MU, finding that MU effectively generalizes to watermarked data. Building on this, we introduce an unlearning-friendly watermarking framework, termed Water4MU, to enhance unlearning effectiveness. The core of Water4MU is a bi-level optimization (BLO) framework: at the upper level, the watermarking network is optimized to minimize unlearning difficulty, while at the lower level, the model itself is trained independently of watermarking. Experimental results demonstrate that Water4MU is effective in MU across both image classification and image generation tasks. Notably, it outperforms existing methods in challenging MU scenarios, known as "challenging forgets".
<div id='section'>Paperid: <span id='pid'>221, <a href='https://arxiv.org/pdf/2503.22330.pdf' target='_blank'>https://arxiv.org/pdf/2503.22330.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziping Dong, Chao Shuai, Zhongjie Ba, Peng Cheng, Zhan Qin, Qinglong Wang, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.22330">WMCopier: Forging Invisible Image Watermarks on Arbitrary Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invisible Image Watermarking is crucial for ensuring content provenance and accountability in generative AI. While Gen-AI providers are increasingly integrating invisible watermarking systems, the robustness of these schemes against forgery attacks remains poorly characterized. This is critical, as forging traceable watermarks onto illicit content leads to false attribution, potentially harming the reputation and legal standing of Gen-AI service providers who are not responsible for the content. In this work, we propose WMCopier, an effective watermark forgery attack that operates without requiring any prior knowledge of or access to the target watermarking algorithm. Our approach first models the target watermark distribution using an unconditional diffusion model, and then seamlessly embeds the target watermark into a non-watermarked image via a shallow inversion process. We also incorporate an iterative optimization procedure that refines the reconstructed image to further trade off the fidelity and forgery efficiency. Experimental results demonstrate that WMCopier effectively deceives both open-source and closed-source watermark systems (e.g., Amazon's system), achieving a significantly higher success rate than existing methods. Additionally, we evaluate the robustness of forged samples and discuss the potential defenses against our attack.
<div id='section'>Paperid: <span id='pid'>222, <a href='https://arxiv.org/pdf/2405.04825.pdf' target='_blank'>https://arxiv.org/pdf/2405.04825.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Shao, Yiming Li, Hongwei Yao, Yiling He, Zhan Qin, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04825">Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ownership verification is currently the most critical and widely adopted post-hoc method to safeguard model copyright. In general, model owners exploit it to identify whether a given suspicious third-party model is stolen from them by examining whether it has particular properties `inherited' from their released models. Currently, backdoor-based model watermarks are the primary and cutting-edge methods to implant such properties in the released models. However, backdoor-based methods have two fatal drawbacks, including harmfulness and ambiguity. The former indicates that they introduce maliciously controllable misclassification behaviors ($i.e.$, backdoor) to the watermarked released models. The latter denotes that malicious users can easily pass the verification by finding other misclassified samples, leading to ownership ambiguity.
  In this paper, we argue that both limitations stem from the `zero-bit' nature of existing watermarking schemes, where they exploit the status ($i.e.$, misclassified) of predictions for verification. Motivated by this understanding, we design a new watermarking paradigm, $i.e.$, Explanation as a Watermark (EaaW), that implants verification behaviors into the explanation of feature attribution instead of model predictions. Specifically, EaaW embeds a `multi-bit' watermark into the feature attribution explanation of specific trigger samples without changing the original prediction. We correspondingly design the watermark embedding and extraction algorithms inspired by explainable artificial intelligence. In particular, our approach can be used for different tasks ($e.g.$, image classification and text generation). Extensive experiments verify the effectiveness and harmlessness of our EaaW and its resistance to potential attacks.
<div id='section'>Paperid: <span id='pid'>223, <a href='https://arxiv.org/pdf/2211.07160.pdf' target='_blank'>https://arxiv.org/pdf/2211.07160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuo Shao, Wenyuan Yang, Hanlin Gu, Zhan Qin, Lixin Fan, Qiang Yang, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.07160">FedTracker: Furnishing Ownership Verification and Traceability for Federated Learning Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) is a distributed machine learning paradigm allowing multiple clients to collaboratively train a global model without sharing their local data. However, FL entails exposing the model to various participants. This poses a risk of unauthorized model distribution or resale by the malicious client, compromising the intellectual property rights of the FL group. To deter such misbehavior, it is essential to establish a mechanism for verifying the ownership of the model and as well tracing its origin to the leaker among the FL participants. In this paper, we present FedTracker, the first FL model protection framework that provides both ownership verification and traceability. FedTracker adopts a bi-level protection scheme consisting of global watermark mechanism and local fingerprint mechanism. The former authenticates the ownership of the global model, while the latter identifies which client the model is derived from. FedTracker leverages Continual Learning (CL) principles to embed the watermark in a way that preserves the utility of the FL model on both primitive task and watermark task. FedTracker also devises a novel metric to better discriminate different fingerprints. Experimental results show FedTracker is effective in ownership verification, traceability, and maintains good fidelity and robustness against various watermark removal attacks.
<div id='section'>Paperid: <span id='pid'>224, <a href='https://arxiv.org/pdf/2405.19677.pdf' target='_blank'>https://arxiv.org/pdf/2405.19677.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxi Zhang, Xiaomei Zhang, Yanjun Zhang, Leo Yu Zhang, Chao Chen, Shengshan Hu, Asif Gill, Shirui Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19677">Large Language Model Watermark Stealing With Mixed Integer Programming</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM's parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.
<div id='section'>Paperid: <span id='pid'>225, <a href='https://arxiv.org/pdf/2506.14398.pdf' target='_blank'>https://arxiv.org/pdf/2506.14398.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chia-Hua Wu, Wanying Ge, Xin Wang, Junichi Yamagishi, Yu Tsao, Hsin-Min Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.14398">A Comparative Study on Proactive and Passive Detection of Deepfake Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Solutions for defending against deepfake speech fall into two categories: proactive watermarking models and passive conventional deepfake detectors. While both address common threats, their differences in training, optimization, and evaluation prevent a unified protocol for joint evaluation and selecting the best solutions for different cases. This work proposes a framework to evaluate both model types in deepfake speech detection. To ensure fair comparison and minimize discrepancies, all models were trained and tested on common datasets, with performance evaluated using a shared metric. We also analyze their robustness against various adversarial attacks, showing that different models exhibit distinct vulnerabilities to different speech attribute distortions. Our training and evaluation code is available at Github.
<div id='section'>Paperid: <span id='pid'>226, <a href='https://arxiv.org/pdf/2504.20111.pdf' target='_blank'>https://arxiv.org/pdf/2504.20111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anubhav Jain, Yuya Kobayashi, Naoki Murata, Yuhta Takida, Takashi Shibuya, Yuki Mitsufuji, Niv Cohen, Nasir Memon, Julian Togelius
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20111">Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking techniques are vital for protecting intellectual property and preventing fraudulent use of media. Most previous watermarking schemes designed for diffusion models embed a secret key in the initial noise. The resulting pattern is often considered hard to remove and forge into unrelated images. In this paper, we propose a black-box adversarial attack without presuming access to the diffusion model weights. Our attack uses only a single watermarked example and is based on a simple observation: there is a many-to-one mapping between images and initial noises. There are regions in the clean image latent space pertaining to each watermark that get mapped to the same initial noise when inverted. Based on this intuition, we propose an adversarial attack to forge the watermark by introducing perturbations to the images such that we can enter the region of watermarked images. We show that we can also apply a similar approach for watermark removal by learning perturbations to exit this region. We report results on multiple watermarking schemes (Tree-Ring, RingID, WIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0). Our results demonstrate the effectiveness of the attack and expose vulnerabilities in the watermarking methods, motivating future research on improving them.
<div id='section'>Paperid: <span id='pid'>227, <a href='https://arxiv.org/pdf/2504.05197.pdf' target='_blank'>https://arxiv.org/pdf/2504.05197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Ren, Jiangyan Yi, Tao Wang, Jianhua Tao, Zheng Lian, Zhengqi Wen, Chenxing Li, Ruibo Fu, Ye Bai, Xiaohui Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05197">P2Mark: Plug-and-play Parameter-level Watermarking for Neural Speech Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural speech generation (NSG) has rapidly advanced as a key component of artificial intelligence-generated content, enabling the generation of high-quality, highly realistic speech for diverse applications. This development increases the risk of technique misuse and threatens social security. Audio watermarking can embed imperceptible marks into generated audio, providing a promising approach for secure NSG usage. However, current audio watermarking methods are mainly applied at the audio-level or feature-level, which are not suitable for open-sourced scenarios where source codes and model weights are released. To address this limitation, we propose a Plug-and-play Parameter-level WaterMarking (P2Mark) method for NSG. Specifically, we embed watermarks into the released model weights, offering a reliable solution for proactively tracing and protecting model copyrights in open-source scenarios. During training, we introduce a lightweight watermark adapter into the pre-trained model, allowing watermark information to be merged into the model via this adapter. This design ensures both the flexibility to modify the watermark before model release and the security of embedding the watermark within model parameters after model release. Meanwhile, we propose a gradient orthogonal projection optimization strategy to ensure the quality of the generated audio and the accuracy of watermark preservation. Experimental results on two mainstream waveform decoders in NSG (i.e., vocoder and codec) demonstrate that P2Mark achieves comparable performance to state-of-the-art audio watermarking methods that are not applicable to open-source white-box protection scenarios, in terms of watermark extraction accuracy, watermark imperceptibility, and robustness.
<div id='section'>Paperid: <span id='pid'>228, <a href='https://arxiv.org/pdf/2405.19099.pdf' target='_blank'>https://arxiv.org/pdf/2405.19099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaolong Xue, Guangyong Shang, Zhen Ma, Minghui Xu, Hechuan Guo, Kun Li, Xiuzhen Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.19099">DataSafe: Copyright Protection with PUF Watermarking and Blockchain Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital watermarking methods are commonly used to safeguard digital media copyrights by confirming ownership and deterring unauthorized use. However, without reliable third-party oversight, these methods risk security vulnerabilities during watermark extraction. Furthermore, digital media lacks tangible ownership attributes, posing challenges for secure copyright transfer and tracing. This study introduces DataSafe, a copyright protection scheme that combines physical unclonable functions (PUFs) and blockchain technology. PUF devices use their unique fingerprints for blockchain registration. Subsequently, these devices incorporate invisible watermarking techniques to embed digital watermarks into media for copyright protection. The watermark verification process is confined within the devices, preserving confidentiality during extraction, validating identities during copyright exchanges, and facilitating blockchain-based traceability of copyright transfers. The implementation of a prototype system on the LPC55S69-EVK development board is detailed, illustrating the practicality and effectiveness of the proposed solution.
<div id='section'>Paperid: <span id='pid'>229, <a href='https://arxiv.org/pdf/2509.10569.pdf' target='_blank'>https://arxiv.org/pdf/2509.10569.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leyi Pan, Sheng Guan, Zheyu Fu, Luyang Si, Zian Wang, Xuming Hu, Irwin King, Philip S. Yu, Aiwei Liu, Lijie Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10569">MarkDiffusion: An Open-Source Toolkit for Generative Watermarking of Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce MarkDiffusion, an open-source Python toolkit for generative watermarking of latent diffusion models. It comprises three key components: a unified implementation framework for streamlined watermarking algorithm integrations and user-friendly interfaces; a mechanism visualization suite that intuitively showcases added and extracted watermark patterns to aid public understanding; and a comprehensive evaluation module offering standard implementations of 24 tools across three essential aspects - detectability, robustness, and output quality - plus 8 automated evaluation pipelines. Through MarkDiffusion, we seek to assist researchers, enhance public awareness and engagement in generative watermarking, and promote consensus while advancing research and applications.
<div id='section'>Paperid: <span id='pid'>230, <a href='https://arxiv.org/pdf/2504.17309.pdf' target='_blank'>https://arxiv.org/pdf/2504.17309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junyan Zhang, Shuliang Liu, Aiwei Liu, Yubo Gao, Jungang Li, Xiaojie Gu, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17309">CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking technology is a method used to trace the usage of content generated by large language models. Sentence-level watermarking aids in preserving the semantic integrity within individual sentences while maintaining greater robustness. However, many existing sentence-level watermarking techniques depend on arbitrary segmentation or generation processes to embed watermarks, which can limit the availability of appropriate sentences. This limitation, in turn, compromises the quality of the generated response. To address the challenge of balancing high text quality with robust watermark detection, we propose CoheMark, an advanced sentence-level watermarking technique that exploits the cohesive relationships between sentences for better logical fluency. The core methodology of CoheMark involves selecting sentences through trained fuzzy c-means clustering and applying specific next sentence selection criteria. Experimental evaluations demonstrate that CoheMark achieves strong watermark strength while exerting minimal impact on text quality.
<div id='section'>Paperid: <span id='pid'>231, <a href='https://arxiv.org/pdf/2503.04636.pdf' target='_blank'>https://arxiv.org/pdf/2503.04636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijie Xu, Aiwei Liu, Xuming Hu, Lijie Wen, Hui Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04636">Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As open-source large language models (LLMs) like Llama3 become more capable, it is crucial to develop watermarking techniques to detect their potential misuse. Existing watermarking methods either add watermarks during LLM inference, which is unsuitable for open-source LLMs, or primarily target classification LLMs rather than recent generative LLMs. Adapting these watermarks to open-source LLMs for misuse detection remains an open challenge. This work defines two misuse scenarios for open-source LLMs: intellectual property (IP) violation and LLM Usage Violation. Then, we explore the application of inference-time watermark distillation and backdoor watermarking in these contexts. We propose comprehensive evaluation methods to assess the impact of various real-world further fine-tuning scenarios on watermarks and the effect of these watermarks on LLM performance. Our experiments reveal that backdoor watermarking could effectively detect IP Violation, while inference-time watermark distillation is applicable in both scenarios but less robust to further fine-tuning and has a more significant impact on LLM performance compared to backdoor watermarking. Exploring more advanced watermarking methods for open-source LLMs to detect their misuse should be an important future direction.
<div id='section'>Paperid: <span id='pid'>232, <a href='https://arxiv.org/pdf/2407.13188.pdf' target='_blank'>https://arxiv.org/pdf/2407.13188.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiyuan Ma, Guoli Jia, Biqing Qi, Bowen Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13188">Safe-SD: Safe and Traceable Stable Diffusion with Text Prompt Trigger for Invisible Generative Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, stable diffusion (SD) models have typically flourished in the field of image synthesis and personalized editing, with a range of photorealistic and unprecedented images being successfully generated. As a result, widespread interest has been ignited to develop and use various SD-based tools for visual content creation. However, the exposure of AI-created content on public platforms could raise both legal and ethical risks. In this regard, the traditional methods of adding watermarks to the already generated images (i.e. post-processing) may face a dilemma (e.g., being erased or modified) in terms of copyright protection and content monitoring, since the powerful image inversion and text-to-image editing techniques have been widely explored in SD-based methods. In this work, we propose a Safe and high-traceable Stable Diffusion framework (namely Safe-SD) to adaptively implant the graphical watermarks (e.g., QR code) into the imperceptible structure-related pixels during the generative diffusion process for supporting text-driven invisible watermarking and detection. Different from the previous high-cost injection-then-detection training framework, we design a simple and unified architecture, which makes it possible to simultaneously train watermark injection and detection in a single network, greatly improving the efficiency and convenience of use. Moreover, to further support text-driven generative watermarking and deeply explore its robustness and high-traceability, we elaborately design lambda sampling and encryption algorithm to fine-tune a latent diffuser wrapped by a VAE for balancing high-fidelity image synthesis and high-traceable watermark detection. We present our quantitative and qualitative results on two representative datasets LSUN, COCO and FFHQ, demonstrating state-of-the-art performance of Safe-SD and showing it significantly outperforms the previous approaches.
<div id='section'>Paperid: <span id='pid'>233, <a href='https://arxiv.org/pdf/2402.16397.pdf' target='_blank'>https://arxiv.org/pdf/2402.16397.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Biqing Qi, Junqi Gao, Yiang Luo, Jianxing Liu, Ligang Wu, Bowen Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16397">Investigating Deep Watermark Security: An Adversarial Transferability Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of generative neural networks has triggered an increased demand for intellectual property (IP) protection in generated content. Deep watermarking techniques, recognized for their flexibility in IP protection, have garnered significant attention. However, the surge in adversarial transferable attacks poses unprecedented challenges to the security of deep watermarking techniques-an area currently lacking systematic investigation. This study fills this gap by introducing two effective transferable attackers to assess the vulnerability of deep watermarks against erasure and tampering risks. Specifically, we initially define the concept of local sample density, utilizing it to deduce theorems on the consistency of model outputs. Upon discovering that perturbing samples towards high sample density regions (HSDR) of the target class enhances targeted adversarial transferability, we propose the Easy Sample Selection (ESS) mechanism and the Easy Sample Matching Attack (ESMA) method. Additionally, we propose the Bottleneck Enhanced Mixup (BEM) that integrates information bottleneck theory to reduce the generator's dependence on irrelevant noise. Experiments show a significant enhancement in the success rate of targeted transfer attacks for both ESMA and BEM-ESMA methods. We further conduct a comprehensive evaluation using ESMA and BEM-ESMA as measurements, considering model architecture and watermark encoding length, and achieve some impressive findings.
<div id='section'>Paperid: <span id='pid'>234, <a href='https://arxiv.org/pdf/2312.07913.pdf' target='_blank'>https://arxiv.org/pdf/2312.07913.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Xi Zhang, Lijie Wen, Irwin King, Hui Xiong, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07913">A Survey of Text Watermarking in the Era of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking algorithms are crucial for protecting the copyright of textual content. Historically, their capabilities and application scenarios were limited. However, recent advancements in large language models (LLMs) have revolutionized these techniques. LLMs not only enhance text watermarking algorithms with their advanced abilities but also create a need for employing these algorithms to protect their own copyrights or prevent potential misuse. This paper conducts a comprehensive survey of the current state of text watermarking technology, covering four main aspects: (1) an overview and comparison of different text watermarking techniques; (2) evaluation methods for text watermarking algorithms, including their detectability, impact on text or LLM quality, robustness under target or untargeted attacks; (3) potential application scenarios for text watermarking technology; (4) current challenges and future directions for text watermarking. This survey aims to provide researchers with a thorough understanding of text watermarking technology in the era of LLM, thereby promoting its further advancement.
<div id='section'>Paperid: <span id='pid'>235, <a href='https://arxiv.org/pdf/2504.12354.pdf' target='_blank'>https://arxiv.org/pdf/2504.12354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinay Shukla, Prachee Sharma, Ryan Rossi, Sungchul Kim, Tong Yu, Aditya Grover
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12354">WaterFlow: Learning Fast & Robust Watermarks using Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ability to embed watermarks in images is a fundamental problem of interest for computer vision, and is exacerbated by the rapid rise of generated imagery in recent times. Current state-of-the-art techniques suffer from computational and statistical challenges such as the slow execution speed for practical deployments. In addition, other works trade off fast watermarking speeds but suffer greatly in their robustness or perceptual quality. In this work, we propose WaterFlow (WF), a fast and extremely robust approach for high fidelity visual watermarking based on a learned latent-dependent watermark. Our approach utilizes a pretrained latent diffusion model to encode an arbitrary image into a latent space and produces a learned watermark that is then planted into the Fourier Domain of the latent. The transformation is specified via invertible flow layers that enhance the expressivity of the latent space of the pre-trained model to better preserve image quality while permitting robust and tractable detection. Most notably, WaterFlow demonstrates state-of-the-art performance on general robustness and is the first method capable of effectively defending against difficult combination attacks. We validate our findings on three widely used real and generated datasets: MS-COCO, DiffusionDB, and WikiArt.
<div id='section'>Paperid: <span id='pid'>236, <a href='https://arxiv.org/pdf/2409.00089.pdf' target='_blank'>https://arxiv.org/pdf/2409.00089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuqing Liang, Jiancheng Xiao, Wensheng Gan, Philip S. Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00089">Watermarking Techniques for Large Language Models: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement and extensive application of artificial intelligence technology, large language models (LLMs) are extensively used to enhance production, creativity, learning, and work efficiency across various domains. However, the abuse of LLMs also poses potential harm to human society, such as intellectual property rights issues, academic misconduct, false content, and hallucinations. Relevant research has proposed the use of LLM watermarking to achieve IP protection for LLMs and traceability of multimedia data output by LLMs. To our knowledge, this is the first thorough review that investigates and analyzes LLM watermarking technology in detail. This review begins by recounting the history of traditional watermarking technology, then analyzes the current state of LLM watermarking research, and thoroughly examines the inheritance and relevance of these techniques. By analyzing their inheritance and relevance, this review can provide research with ideas for applying traditional digital watermarking techniques to LLM watermarking, to promote the cross-integration and innovation of watermarking technology. In addition, this review examines the pros and cons of LLM watermarking. Considering the current multimodal development trend of LLMs, it provides a detailed analysis of emerging multimodal LLM watermarking, such as visual and audio data, to offer more reference ideas for relevant research. This review delves into the challenges and future prospects of current watermarking technologies, offering valuable insights for future LLM watermarking research and applications.
<div id='section'>Paperid: <span id='pid'>237, <a href='https://arxiv.org/pdf/2505.19663.pdf' target='_blank'>https://arxiv.org/pdf/2505.19663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yigitcan Ãzer, Woosung Choi, Joan SerrÃ, Mayank Kumar Singh, Wei-Hsiang Liao, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19663">A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the Robust Audio Watermarking Benchmark (RAW-Bench), a benchmark for evaluating deep learning-based audio watermarking methods with standardized and systematic comparisons. To simulate real-world usage, we introduce a comprehensive audio attack pipeline with various distortions such as compression, background noise, and reverberation, along with a diverse test dataset including speech, environmental sounds, and music recordings. Evaluating four existing watermarking methods on RAW-bench reveals two main insights: (i) neural compression techniques pose the most significant challenge, even when algorithms are trained with such compressions; and (ii) training with audio attacks generally improves robustness, although it is insufficient in some cases. Furthermore, we find that specific distortions, such as polarity inversion, time stretching, or reverb, seriously affect certain methods. The evaluation framework is accessible at github.com/SonyResearch/raw_bench.
<div id='section'>Paperid: <span id='pid'>238, <a href='https://arxiv.org/pdf/2409.07743.pdf' target='_blank'>https://arxiv.org/pdf/2409.07743.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mayank Kumar Singh, Naoya Takahashi, Wei-Hsiang Liao, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07743">LOCKEY: A Novel Approach to Model Authentication and Deepfake Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a novel approach to deter unauthorized deepfakes and enable user tracking in generative models, even when the user has full access to the model parameters, by integrating key-based model authentication with watermarking techniques. Our method involves providing users with model parameters accompanied by a unique, user-specific key. During inference, the model is conditioned upon the key along with the standard input. A valid key results in the expected output, while an invalid key triggers a degraded output, thereby enforcing key-based model authentication. For user tracking, the model embeds the user's unique key as a watermark within the generated content, facilitating the identification of the user's ID. We demonstrate the effectiveness of our approach on two types of models, audio codecs and vocoders, utilizing the SilentCipher watermarking method. Additionally, we assess the robustness of the embedded watermarks against various distortions, validating their reliability in various scenarios.
<div id='section'>Paperid: <span id='pid'>239, <a href='https://arxiv.org/pdf/2510.02342.pdf' target='_blank'>https://arxiv.org/pdf/2510.02342.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Zhang, Shuliang Liu, Xu Yang, Xuming Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02342">CATMark: A Context-Aware Thresholding Framework for Robust Cross-Task Watermarking in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking algorithms for Large Language Models (LLMs) effectively identify machine-generated content by embedding and detecting hidden statistical features in text. However, such embedding leads to a decline in text quality, especially in low-entropy scenarios where performance needs improvement. Existing methods that rely on entropy thresholds often require significant computational resources for tuning and demonstrate poor adaptability to unknown or cross-task generation scenarios. We propose \textbf{C}ontext-\textbf{A}ware \textbf{T}hreshold watermarking ($\myalgo$), a novel framework that dynamically adjusts watermarking intensity based on real-time semantic context. $\myalgo$ partitions text generation into semantic states using logits clustering, establishing context-aware entropy thresholds that preserve fidelity in structured content while embedding robust watermarks. Crucially, it requires no pre-defined thresholds or task-specific tuning. Experiments show $\myalgo$ improves text quality in cross-tasks without sacrificing detection accuracy.
<div id='section'>Paperid: <span id='pid'>240, <a href='https://arxiv.org/pdf/2506.23731.pdf' target='_blank'>https://arxiv.org/pdf/2506.23731.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michel Meintz, Jan DubiÅski, Franziska Boenisch, Adam Dziedzic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23731">Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image generative models have become increasingly popular, but training them requires large datasets that are costly to collect and curate. To circumvent these costs, some parties may exploit existing models by using the generated images as training data for their own models. In general, watermarking is a valuable tool for detecting unauthorized use of generated images. However, when these images are used to train a new model, watermarking can only enable detection if the watermark persists through training and remains identifiable in the outputs of the newly trained model - a property known as radioactivity. We analyze the radioactivity of watermarks in images generated by diffusion models (DMs) and image autoregressive models (IARs). We find that existing watermarking methods for DMs fail to retain radioactivity, as watermarks are either erased during encoding into the latent space or lost in the noising-denoising process (during the training in the latent space). Meanwhile, despite IARs having recently surpassed DMs in image generation quality and efficiency, no radioactive watermarking methods have been proposed for them. To overcome this limitation, we propose the first watermarking method tailored for IARs and with radioactivity in mind - drawing inspiration from techniques in large language models (LLMs), which share IARs' autoregressive paradigm. Our extensive experimental evaluation highlights our method's effectiveness in preserving radioactivity within IARs, enabling robust provenance tracking, and preventing unauthorized use of their generated images.
<div id='section'>Paperid: <span id='pid'>241, <a href='https://arxiv.org/pdf/2506.21209.pdf' target='_blank'>https://arxiv.org/pdf/2506.21209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Louis Kerner, Michel Meintz, Bihe Zhao, Franziska Boenisch, Adam Dziedzic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21209">BitMark for Infinity: Watermarking Bitwise Autoregressive Image Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>State-of-the-art text-to-image models like Infinity generate photorealistic images at an unprecedented speed. These models operate in a bitwise autoregressive manner over a discrete set of tokens that is practically infinite in size. However, their impressive generative power comes with a growing risk: as their outputs increasingly populate the Internet, they are likely to be scraped and reused as training data-potentially by the very same models. This phenomenon has been shown to lead to model collapse, where repeated training on generated content, especially from the models' own previous versions, causes a gradual degradation in performance. A promising mitigation strategy is watermarking, which embeds human-imperceptible yet detectable signals into generated images-enabling the identification of generated content. In this work, we introduce BitMark, a robust bitwise watermarking framework for Infinity. Our method embeds a watermark directly at the bit level of the token stream across multiple scales (also referred to as resolutions) during Infinity's image generation process. Our bitwise watermark subtly influences the bits to preserve visual fidelity and generation speed while remaining robust against a spectrum of removal techniques. Furthermore, it exhibits high radioactivity, i.e., when watermarked generated images are used to train another image generative model, this second model's outputs will also carry the watermark. The radioactive traces remain detectable even when only fine-tuning diffusion or image autoregressive models on images watermarked with our BitMark. Overall, our approach provides a principled step toward preventing model collapse in image generative models by enabling reliable detection of generated outputs.
<div id='section'>Paperid: <span id='pid'>242, <a href='https://arxiv.org/pdf/2512.12324.pdf' target='_blank'>https://arxiv.org/pdf/2512.12324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Meilin Li, Ji He, Yi Yu, Jia Xu, Shanzhe Lei, Yan Teng, Yingchun Wang, Xuhong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12324">UniMark: Artificial Intelligence Generated Content Identification Toolkit</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid proliferation of Artificial Intelligence Generated Content has precipitated a crisis of trust and urgent regulatory demands. However, existing identification tools suffer from fragmentation and a lack of support for visible compliance marking. To address these gaps, we introduce the \textbf{UniMark}, an open-source, unified framework for multimodal content governance. Our system features a modular unified engine that abstracts complexities across text, image, audio, and video modalities. Crucially, we propose a novel dual-operation strategy, natively supporting both \emph{Hidden Watermarking} for copyright protection and \emph{Visible Marking} for regulatory compliance. Furthermore, we establish a standardized evaluation framework with three specialized benchmarks (Image/Video/Audio-Bench) to ensure rigorous performance assessment. This toolkit bridges the gap between advanced algorithms and engineering implementation, fostering a more transparent and secure digital ecosystem.
<div id='section'>Paperid: <span id='pid'>243, <a href='https://arxiv.org/pdf/2509.24368.pdf' target='_blank'>https://arxiv.org/pdf/2509.24368.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thibaud Gloaguen, Robin Staab, Nikola JovanoviÄ, Martin Vechev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24368">Watermarking Diffusion Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce the first watermark tailored for diffusion language models (DLMs), an emergent LLM paradigm able to generate tokens in arbitrary order, in contrast to standard autoregressive language models (ARLMs) which generate tokens sequentially. While there has been much work in ARLM watermarking, a key challenge when attempting to apply these schemes directly to the DLM setting is that they rely on previously generated tokens, which are not always available with DLM generation. In this work we address this challenge by: (i) applying the watermark in expectation over the context even when some context tokens are yet to be determined, and (ii) promoting tokens which increase the watermark strength when used as context for other tokens. This is accomplished while keeping the watermark detector unchanged. Our experimental evaluation demonstrates that the DLM watermark leads to a >99% true positive rate with minimal quality impact and achieves similar robustness to existing ARLM watermarks, enabling for the first time reliable DLM watermarking.
<div id='section'>Paperid: <span id='pid'>244, <a href='https://arxiv.org/pdf/2506.23066.pdf' target='_blank'>https://arxiv.org/pdf/2506.23066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiale Meng, Yiming Li, Zheming Lu, Zewei He, Hao Luo, Tianwei Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.23066">CoreMark: Toward Robust and Universal Text Watermarking Technique</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking schemes have gained considerable attention in recent years, yet still face critical challenges in achieving simultaneous robustness, generalizability, and imperceptibility. This paper introduces a new embedding paradigm,termed CORE, which comprises several consecutively aligned black pixel segments. Its key innovation lies in its inherent noise resistance during transmission and broad applicability across languages and fonts. Based on the CORE, we present a text watermarking framework named CoreMark. Specifically, CoreMark first dynamically extracts COREs from characters. Then, the characters with stronger robustness are selected according to the lengths of COREs. By modifying the thickness of the CORE, the hidden data is embedded into the selected characters without causing significant visual distortions. Moreover, a general plug-and-play embedding strength modulator is proposed, which can adaptively enhance the robustness for small font sizes by adjusting the embedding strength according to the font size. Experimental evaluation indicates that CoreMark demonstrates outstanding generalizability across multiple languages and fonts. Compared to existing methods, CoreMark achieves significant improvements in resisting screenshot, print-scan, and print camera attacks, while maintaining satisfactory imperceptibility.
<div id='section'>Paperid: <span id='pid'>245, <a href='https://arxiv.org/pdf/2505.16723.pdf' target='_blank'>https://arxiv.org/pdf/2505.16723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thibaud Gloaguen, Robin Staab, Nikola JovanoviÄ, Martin Vechev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16723">Robust LLM Fingerprinting via Domain-Specific Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As open-source language models (OSMs) grow more capable and are widely shared and finetuned, ensuring model provenance, i.e., identifying the origin of a given model instance, has become an increasingly important issue. At the same time, existing backdoor-based model fingerprinting techniques often fall short of achieving key requirements of real-world model ownership detection. In this work, we build on the observation that while current open-source model watermarks fail to achieve reliable content traceability, they can be effectively adapted to address the challenge of model provenance. To this end, we introduce the concept of domain-specific watermarking for model fingerprinting. Rather than watermarking all generated content, we train the model to embed watermarks only within specified subdomains (e.g., particular languages or topics). This targeted approach ensures detection reliability, while improving watermark durability and quality under a range of real-world deployment settings. Our evaluations show that domain-specific watermarking enables model fingerprinting with strong statistical guarantees, controllable false positive rates, high detection power, and preserved generation quality. Moreover, we find that our fingerprints are inherently stealthy and naturally robust to real-world variability across deployment scenarios.
<div id='section'>Paperid: <span id='pid'>246, <a href='https://arxiv.org/pdf/2502.10525.pdf' target='_blank'>https://arxiv.org/pdf/2502.10525.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thibaud Gloaguen, Nikola JovanoviÄ, Robin Staab, Martin Vechev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10525">Towards Watermarking of Open-Source LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>While watermarks for closed LLMs have matured and have been included in large-scale deployments, these methods are not applicable to open-source models, which allow users full control over the decoding process. This setting is understudied yet critical, given the rising performance of open-source models. In this work, we lay the foundation for systematic study of open-source LLM watermarking. For the first time, we explicitly formulate key requirements, including durability against common model modifications such as model merging, quantization, or finetuning, and propose a concrete evaluation setup. Given the prevalence of these modifications, durability is crucial for an open-source watermark to be effective. We survey and evaluate existing methods, showing that they are not durable. We also discuss potential ways to improve their durability and highlight remaining challenges. We hope our work enables future progress on this important problem.
<div id='section'>Paperid: <span id='pid'>247, <a href='https://arxiv.org/pdf/2405.20777.pdf' target='_blank'>https://arxiv.org/pdf/2405.20777.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thibaud Gloaguen, Nikola JovanoviÄ, Robin Staab, Martin Vechev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.20777">Black-Box Detection of Language Model Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has emerged as a promising way to detect LLM-generated text, by augmenting LLM generations with later detectable signals. Recent work has proposed multiple families of watermarking schemes, several of which focus on preserving the LLM distribution. This distribution-preservation property is motivated by the fact that it is a tractable proxy for retaining LLM capabilities, as well as the inherently implied undetectability of the watermark by downstream users. Yet, despite much discourse around undetectability, no prior work has investigated the practical detectability of any of the current watermarking schemes in a realistic black-box setting. In this work we tackle this for the first time, developing rigorous statistical tests to detect the presence, and estimate parameters, of all three popular watermarking scheme families, using only a limited number of black-box queries. We experimentally confirm the effectiveness of our methods on a range of schemes and a diverse set of open-source models. Further, we validate the feasibility of our tests on real-world APIs. Our findings indicate that current watermarking schemes are more detectable than previously believed.
<div id='section'>Paperid: <span id='pid'>248, <a href='https://arxiv.org/pdf/2402.19361.pdf' target='_blank'>https://arxiv.org/pdf/2402.19361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nikola JovanoviÄ, Robin Staab, Martin Vechev
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19361">Watermark Stealing in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as hypothesized in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.
<div id='section'>Paperid: <span id='pid'>249, <a href='https://arxiv.org/pdf/2511.11295.pdf' target='_blank'>https://arxiv.org/pdf/2511.11295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichao Tang, Mingyang Li, Di Miao, Sheng Li, Zhenxing Qian, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11295">SimuFreeMark: A Noise-Simulation-Free Robust Watermarking Against Image Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of artificial intelligence generated content (AIGC) has created a pressing need for robust image watermarking that can withstand both conventional signal processing and novel semantic editing attacks. Current deep learning-based methods rely on training with hand-crafted noise simulation layers, which inherently limit their generalization to unforeseen distortions. In this work, we propose $\textbf{SimuFreeMark}$, a noise-$\underline{\text{simu}}$lation-$\underline{\text{free}}$ water$\underline{\text{mark}}$ing framework that circumvents this limitation by exploiting the inherent stability of image low-frequency components. We first systematically establish that low-frequency components exhibit significant robustness against a wide range of attacks. Building on this foundation, SimuFreeMark embeds watermarks directly into the deep feature space of the low-frequency components, leveraging a pre-trained variational autoencoder (VAE) to bind the watermark with structurally stable image representations. This design completely eliminates the need for noise simulation during training. Extensive experiments demonstrate that SimuFreeMark outperforms state-of-the-art methods across a wide range of conventional and semantic attacks, while maintaining superior visual quality.
<div id='section'>Paperid: <span id='pid'>250, <a href='https://arxiv.org/pdf/2508.17121.pdf' target='_blank'>https://arxiv.org/pdf/2508.17121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenliang Gan, Xiaoxiao Hu, Sheng Li, Zhenxing Qian, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17121">SyncGuard: Robust Audio Watermarking Capable of Countering Desynchronization Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Audio watermarking has been widely applied in copyright protection and source tracing. However, due to the inherent characteristics of audio signals, watermark localization and resistance to desynchronization attacks remain significant challenges. In this paper, we propose a learning-based scheme named SyncGuard to address these challenges. Specifically, we design a frame-wise broadcast embedding strategy to embed the watermark in arbitrary-length audio, enhancing time-independence and eliminating the need for localization during watermark extraction. To further enhance robustness, we introduce a meticulously designed distortion layer. Additionally, we employ dilated residual blocks in conjunction with dilated gated blocks to effectively capture multi-resolution time-frequency features. Extensive experimental results show that SyncGuard efficiently handles variable-length audio segments, outperforms state-of-the-art methods in robustness against various attacks, and delivers superior auditory quality.
<div id='section'>Paperid: <span id='pid'>251, <a href='https://arxiv.org/pdf/2504.19529.pdf' target='_blank'>https://arxiv.org/pdf/2504.19529.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guobiao Li, Lei Tan, Yuliang Xue, Gaozhi Liu, Zhenxing Qian, Sheng Li, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19529">Adversarial Shallow Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in digital watermarking make use of deep neural networks for message embedding and extraction. They typically follow the ``encoder-noise layer-decoder''-based architecture. By deliberately establishing a differentiable noise layer to simulate the distortion of the watermarked signal, they jointly train the deep encoder and decoder to fit the noise layer to guarantee robustness. As a result, they are usually weak against unknown distortions that are not used in their training pipeline. In this paper, we propose a novel watermarking framework to resist unknown distortions, namely Adversarial Shallow Watermarking (ASW). ASW utilizes only a shallow decoder that is randomly parameterized and designed to be insensitive to distortions for watermarking extraction. During the watermark embedding, ASW freezes the shallow decoder and adversarially optimizes a host image until its updated version (i.e., the watermarked image) stably triggers the shallow decoder to output the watermark message. During the watermark extraction, it accurately recovers the message from the watermarked image by leveraging the insensitive nature of the shallow decoder against arbitrary distortions. Our ASW is training-free, encoder-free, and noise layer-free. Experiments indicate that the watermarked images created by ASW have strong robustness against various unknown distortions. Compared to the existing ``encoder-noise layer-decoder'' approaches, ASW achieves comparable results on known distortions and better robustness on unknown distortions.
<div id='section'>Paperid: <span id='pid'>252, <a href='https://arxiv.org/pdf/2409.07556.pdf' target='_blank'>https://arxiv.org/pdf/2409.07556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Helin Wang, Meng Yu, Jiarui Hai, Chen Chen, Yuchen Hu, Rilin Chen, Najim Dehak, Dong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07556">SSR-Speech: Towards Stable, Safe and Robust Zero-shot Text-based Speech Editing and Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce SSR-Speech, a neural codec autoregressive model designed for stable, safe, and robust zero-shot textbased speech editing and text-to-speech synthesis. SSR-Speech is built on a Transformer decoder and incorporates classifier-free guidance to enhance the stability of the generation process. A watermark Encodec is proposed to embed frame-level watermarks into the edited regions of the speech so that which parts were edited can be detected. In addition, the waveform reconstruction leverages the original unedited speech segments, providing superior recovery compared to the Encodec model. Our approach achieves state-of-the-art performance in the RealEdit speech editing task and the LibriTTS text-to-speech task, surpassing previous methods. Furthermore, SSR-Speech excels in multi-span speech editing and also demonstrates remarkable robustness to background sounds. The source code and demos are released.
<div id='section'>Paperid: <span id='pid'>253, <a href='https://arxiv.org/pdf/2311.13619.pdf' target='_blank'>https://arxiv.org/pdf/2311.13619.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ge Luo, Junqiang Huang, Manman Zhang, Zhenxing Qian, Sheng Li, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.13619">Steal My Artworks for Fine-tuning? A Watermarking Framework for Detecting Art Theft Mimicry in Text-to-Image Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement in text-to-image models has led to astonishing artistic performances. However, several studios and websites illegally fine-tune these models using artists' artworks to mimic their styles for profit, which violates the copyrights of artists and diminishes their motivation to produce original works. Currently, there is a notable lack of research focusing on this issue. In this paper, we propose a novel watermarking framework that detects mimicry in text-to-image models through fine-tuning. This framework embeds subtle watermarks into digital artworks to protect their copyrights while still preserving the artist's visual expression. If someone takes watermarked artworks as training data to mimic an artist's style, these watermarks can serve as detectable indicators. By analyzing the distribution of these watermarks in a series of generated images, acts of fine-tuning mimicry using stolen victim data will be exposed. In various fine-tune scenarios and against watermark attack methods, our research confirms that analyzing the distribution of watermarks in artificially generated images reliably detects unauthorized mimicry.
<div id='section'>Paperid: <span id='pid'>254, <a href='https://arxiv.org/pdf/2502.10465.pdf' target='_blank'>https://arxiv.org/pdf/2502.10465.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhuo Chen, Jordan Vice, Naveed Akhtar, Nur Al Hasan Haldar, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10465">Image Watermarking of Generative Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embedding watermarks into the output of generative models is essential for establishing copyright and verifiable ownership over the generated content. Emerging diffusion model watermarking methods either embed watermarks in the frequency domain or offer limited versatility of the watermark patterns in the image space, which allows simplistic detection and removal of the watermarks from the generated content. To address this issue, we propose a watermarking technique that embeds watermark features into the diffusion model itself. Our technique enables training of a paired watermark extractor for a generative model that is learned through an end-to-end process. The extractor forces the generator, during training, to effectively embed versatile, imperceptible watermarks in the generated content while simultaneously ensuring their precise recovery. We demonstrate highly accurate watermark embedding/detection and show that it is also possible to distinguish between different watermarks embedded with our method to differentiate between generative models.
<div id='section'>Paperid: <span id='pid'>255, <a href='https://arxiv.org/pdf/2502.08927.pdf' target='_blank'>https://arxiv.org/pdf/2502.08927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunzhuo Chen, Naveed Akhtar, Nur Al Hasan Haldar, Ajmal Mian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08927">Dynamic watermarks in images generated by diffusion models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-fidelity text-to-image diffusion models have revolutionized visual content generation, but their widespread use raises significant ethical concerns, including intellectual property protection and the misuse of synthetic media. To address these challenges, we propose a novel multi-stage watermarking framework for diffusion models, designed to establish copyright and trace generated images back to their source. Our multi-stage watermarking technique involves embedding: (i) a fixed watermark that is localized in the diffusion model's learned noise distribution and, (ii) a human-imperceptible, dynamic watermark in generates images, leveraging a fine-tuned decoder. By leveraging the Structural Similarity Index Measure (SSIM) and cosine similarity, we adapt the watermark's shape and color to the generated content while maintaining robustness. We demonstrate that our method enables reliable source verification through watermark classification, even when the dynamic watermark is adjusted for content-specific variations. Source model verification is enabled through watermark classification. o support further research, we generate a dataset of watermarked images and introduce a methodology to evaluate the statistical impact of watermarking on generated content.Additionally, we rigorously test our framework against various attack scenarios, demonstrating its robustness and minimal impact on image quality. Our work advances the field of AI-generated content security by providing a scalable solution for model ownership verification and misuse prevention.
<div id='section'>Paperid: <span id='pid'>256, <a href='https://arxiv.org/pdf/2511.11356.pdf' target='_blank'>https://arxiv.org/pdf/2511.11356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanbo Dai, Zongjie Li, Zhenlan Ji, Shuai Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.11356">SEAL: Subspace-Anchored Watermarks for LLM Ownership</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks, demonstrating human-level performance in text generation, reasoning, and question answering. However, training such models requires substantial computational resources, large curated datasets, and sophisticated alignment procedures. As a result, they constitute highly valuable intellectual property (IP) assets that warrant robust protection mechanisms. Existing IP protection approaches suffer from critical limitations. Model fingerprinting techniques can identify model architectures but fail to establish ownership of specific model instances. In contrast, traditional backdoor-based watermarking methods embed behavioral anomalies that can be easily removed through common post-processing operations such as fine-tuning or knowledge distillation. We propose SEAL, a subspace-anchored watermarking framework that embeds multi-bit signatures directly into the model's latent representational space, supporting both white-box and black-box verification scenarios. Our approach leverages model editing techniques to align the hidden representations of selected anchor samples with predefined orthogonal bit vectors. This alignment embeds the watermark while preserving the model's original factual predictions, rendering the watermark functionally harmless and stealthy. We conduct comprehensive experiments on multiple benchmark datasets and six prominent LLMs, comparing SEAL with 11 existing fingerprinting and watermarking methods to demonstrate its superior effectiveness, fidelity, efficiency, and robustness. Furthermore, we evaluate SEAL under potential knowledgeable attacks and show that it maintains strong verification performance even when adversaries possess knowledge of the watermarking mechanism and the embedded signatures.
<div id='section'>Paperid: <span id='pid'>257, <a href='https://arxiv.org/pdf/2510.09263.pdf' target='_blank'>https://arxiv.org/pdf/2510.09263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sven Gowal, Rudy Bunel, Florian Stimberg, David Stutz, Guillermo Ortiz-Jimenez, Christina Kouridi, Mel Vecerik, Jamie Hayes, Sylvestre-Alvise Rebuffi, Paul Bernard, Chris Gamble, Miklós Z. Horváth, Fabian Kaczmarczyck, Alex Kaskasoli, Aleksandar Petrov, Ilia Shumailov, Meghana Thotakuri, Olivia Wiles, Jessica Yung, Zahra Ahmed, Victor Martin, Simon Rosen, Christopher Savčak, Armin Senoner, Nidhi Vyas, Pushmeet Kohli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09263">SynthID-Image: Image watermarking at internet scale</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce SynthID-Image, a deep learning-based system for invisibly watermarking AI-generated imagery. This paper documents the technical desiderata, threat models, and practical challenges of deploying such a system at internet scale, addressing key requirements of effectiveness, fidelity, robustness, and security. SynthID-Image has been used to watermark over ten billion images and video frames across Google's services and its corresponding verification service is available to trusted testers. For completeness, we present an experimental evaluation of an external model variant, SynthID-O, which is available through partnerships. We benchmark SynthID-O against other post-hoc watermarking methods from the literature, demonstrating state-of-the-art performance in both visual quality and robustness to common image perturbations. While this work centers on visual media, the conclusions on deployment, constraints, and threat modeling generalize to other modalities, including audio. This paper provides a comprehensive documentation for the large-scale deployment of deep learning-based media provenance systems.
<div id='section'>Paperid: <span id='pid'>258, <a href='https://arxiv.org/pdf/2510.01968.pdf' target='_blank'>https://arxiv.org/pdf/2510.01968.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luca A. Lanzendörfer, Kyle Fearne, Florian Grötschla, Roger Wattenhofer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01968">Multi-bit Audio Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present Timbru, a post-hoc audio watermarking model that achieves state-of-the-art robustness and imperceptibility trade-offs without training an embedder-detector model. Given any 44.1 kHz stereo music snippet, our method performs per-audio gradient optimization to add imperceptible perturbations in the latent space of a pretrained audio VAE, guided by a combined message and perceptual loss. The watermark can then be extracted using a pretrained CLAP model. We evaluate 16-bit watermarking on MUSDB18-HQ against AudioSeal, WavMark, and SilentCipher across common filtering, noise, compression, resampling, cropping, and regeneration attacks. Our approach attains the best average bit error rates, while preserving perceptual quality, demonstrating an efficient, dataset-free path to imperceptible audio watermarking.
<div id='section'>Paperid: <span id='pid'>259, <a href='https://arxiv.org/pdf/2409.12121.pdf' target='_blank'>https://arxiv.org/pdf/2409.12121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junzuo Zhou, Jiangyan Yi, Yong Ren, Jianhua Tao, Tao Wang, Chu Yuan Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.12121">WMCodec: End-to-End Neural Speech Codec with Deep Watermarking for Authenticity Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in speech spoofing necessitate stronger verification mechanisms in neural speech codecs to ensure authenticity. Current methods embed numerical watermarks before compression and extract them from reconstructed speech for verification, but face limitations such as separate training processes for the watermark and codec, and insufficient cross-modal information integration, leading to reduced watermark imperceptibility, extraction accuracy, and capacity. To address these issues, we propose WMCodec, the first neural speech codec to jointly train compression-reconstruction and watermark embedding-extraction in an end-to-end manner, optimizing both imperceptibility and extractability of the watermark. Furthermore, We design an iterative Attention Imprint Unit (AIU) for deeper feature integration of watermark and speech, reducing the impact of quantization noise on the watermark. Experimental results show WMCodec outperforms AudioSeal with Encodec in most quality metrics for watermark imperceptibility and consistently exceeds both AudioSeal with Encodec and reinforced TraceableSpeech in extraction accuracy of watermark. At bandwidth of 6 kbps with a watermark capacity of 16 bps, WMCodec maintains over 99% extraction accuracy under common attacks, demonstrating strong robustness.
<div id='section'>Paperid: <span id='pid'>260, <a href='https://arxiv.org/pdf/2510.23891.pdf' target='_blank'>https://arxiv.org/pdf/2510.23891.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaqi Xue, Yifei Zhao, Mansour Al Ghanim, Shangqian Gao, Ruimin Sun, Qian Lou, Mengxin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.23891">PRO: Enabling Precise and Robust Text Watermark for Open-Source LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking for large language models (LLMs) enables model owners to verify text origin and protect intellectual property. While watermarking methods for closed-source LLMs are relatively mature, extending them to open-source models remains challenging, as developers cannot control the decoding process. Consequently, owners of open-source LLMs lack practical means to verify whether text was generated by their models. A core difficulty lies in embedding watermarks directly into model weights without hurting detectability. A promising idea is to distill watermarks from a closed-source model into an open one, but this suffers from (i) poor detectability due to mismatch between learned and predefined patterns, and (ii) fragility to downstream modifications such as fine-tuning or model merging. To overcome these limitations, we propose PRO, a Precise and Robust text watermarking method for open-source LLMs. PRO jointly trains a watermark policy model with the LLM, producing patterns that are easier for the model to learn and more consistent with detection criteria. A regularization term further simulates downstream perturbations and penalizes degradation in watermark detectability, ensuring robustness under model edits. Experiments on open-source LLMs (e.g., LLaMA-3.2, LLaMA-3, Phi-2) show that PRO substantially improves both watermark detectability and resilience to model modifications.
<div id='section'>Paperid: <span id='pid'>261, <a href='https://arxiv.org/pdf/2509.07755.pdf' target='_blank'>https://arxiv.org/pdf/2509.07755.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rochana Prih Hastuti, Rian Adam Rajagede, Mansour Al Ghanim, Mengxin Zheng, Qian Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.07755">Factuality Beyond Coherence: Evaluating LLM Watermarking Methods for Medical Texts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) are adapted to sensitive domains such as medicine, their fluency raises safety risks, particularly regarding provenance and accountability. Watermarking embeds detectable patterns to mitigate these risks, yet its reliability in medical contexts remains untested. Existing benchmarks focus on detection-quality tradeoffs and overlook factual risks. In medical text, watermarking often reweights low-entropy tokens, which are highly predictable and often carry critical medical terminology. Shifting these tokens can cause inaccuracy and hallucinations, risks that prior general-domain benchmarks fail to capture. We propose a medical-focused evaluation workflow that jointly assesses factual accuracy and coherence. Using GPT-Judger and further human validation, we introduce the Factuality-Weighted Score (FWS), a composite metric prioritizing factual accuracy beyond coherence to guide watermarking deployment in medical domains. Our evaluation shows current watermarking methods substantially compromise medical factuality, with entropy shifts degrading medical entity representation. These findings underscore the need for domain-aware watermarking approaches that preserve the integrity of medical content.
<div id='section'>Paperid: <span id='pid'>262, <a href='https://arxiv.org/pdf/2502.16699.pdf' target='_blank'>https://arxiv.org/pdf/2502.16699.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mansour Al Ghanim, Jiaqi Xue, Rochana Prih Hastuti, Mengxin Zheng, Yan Solihin, Qian Lou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.16699">Evaluating the Robustness and Accuracy of Text Watermarking Under Real-World Cross-Lingual Manipulations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a study to benchmark representative watermarking methods in cross-lingual settings. The current literature mainly focuses on the evaluation of watermarking methods for the English language. However, the literature for evaluating watermarking in cross-lingual settings is scarce. This results in overlooking important adversary scenarios in which a cross-lingual adversary could be in, leading to a gray area of practicality over cross-lingual watermarking. In this paper, we evaluate four watermarking methods in four different and vocabulary rich languages. Our experiments investigate the quality of text under different watermarking procedure and the detectability of watermarks with practical translation attack scenarios. Specifically, we investigate practical scenarios that an adversary with cross-lingual knowledge could take, and evaluate whether current watermarking methods are suitable for such scenarios. Finally, from our findings, we draw key insights about watermarking in cross-lingual settings.
<div id='section'>Paperid: <span id='pid'>263, <a href='https://arxiv.org/pdf/2410.06467.pdf' target='_blank'>https://arxiv.org/pdf/2410.06467.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingjie Chen, Ruizhong Qiu, Siyu Yuan, Zhining Liu, Tianxin Wei, Hyunsik Yoo, Zhichen Zeng, Deqing Yang, Hanghang Tong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06467">WAPITI: A Watermark for Finetuned Open-Source LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking of large language models (LLMs) generation embeds an imperceptible statistical pattern within texts, making it algorithmically detectable. Watermarking is a promising method for addressing potential harm and biases from LLMs, as it enables traceability, accountability, and detection of manipulated content, helping to mitigate unintended consequences. However, for open-source models, watermarking faces two major challenges: (i) incompatibility with fine-tuned models, and (ii) vulnerability to fine-tuning attacks. In this work, we propose WAPITI, a new method that transfers watermarking from base models to fine-tuned models through parameter integration. To the best of our knowledge, we propose the first watermark for fine-tuned open-source LLMs that preserves their fine-tuned capabilities. Furthermore, our approach offers an effective defense against fine-tuning attacks. We test our method on various model architectures and watermarking strategies. Results demonstrate that our method can successfully inject watermarks and is highly compatible with fine-tuned models. Additionally, we offer an in-depth analysis of how parameter editing influences the watermark strength and overall capabilities of the resulting models.
<div id='section'>Paperid: <span id='pid'>264, <a href='https://arxiv.org/pdf/2510.18333.pdf' target='_blank'>https://arxiv.org/pdf/2510.18333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yepeng Liu, Xuandong Zhao, Dawn Song, Gregory W. Wornell, Yuheng Bu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18333">Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite progress in watermarking algorithms for large language models (LLMs), real-world deployment remains limited. We argue that this gap stems from misaligned incentives among LLM providers, platforms, and end users, which manifest as four key barriers: competitive risk, detection-tool governance, robustness concerns and attribution issues. We revisit three classes of watermarking through this lens. \emph{Model watermarking} naturally aligns with LLM provider interests, yet faces new challenges in open-source ecosystems. \emph{LLM text watermarking} offers modest provider benefit when framed solely as an anti-misuse tool, but can gain traction in narrowly scoped settings such as dataset de-contamination or user-controlled provenance. \emph{In-context watermarking} (ICW) is tailored for trusted parties, such as conference organizers or educators, who embed hidden watermarking instructions into documents. If a dishonest reviewer or student submits this text to an LLM, the output carries a detectable watermark indicating misuse. This setup aligns incentives: users experience no quality loss, trusted parties gain a detection tool, and LLM providers remain neutral by simply following watermark instructions. We advocate for a broader exploration of incentive-aligned methods, with ICW as an example, in domains where trusted parties need reliable tools to detect misuse. More broadly, we distill design principles for incentive-aligned, domain-specific watermarking and outline future research directions. Our position is that the practical adoption of LLM watermarking requires aligning stakeholder incentives in targeted application domains and fostering active community engagement.
<div id='section'>Paperid: <span id='pid'>265, <a href='https://arxiv.org/pdf/2509.10766.pdf' target='_blank'>https://arxiv.org/pdf/2509.10766.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tong Zhou, Ruyi Ding, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Yunsi Fei, Xiaolin Xu, Shaolei Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10766">A Content-dependent Watermark for Safeguarding Image Attribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of digital and AI-generated images has amplified the need for secure and verifiable methods of image attribution. While digital watermarking offers more robust protection than metadata-based approaches--which can be easily stripped--current watermarking techniques remain vulnerable to forgery, creating risks of misattribution that can damage the reputations of AI model developers and the rights of digital artists. These vulnerabilities arise from two key issues: (1) content-agnostic watermarks, which, once learned or leaked, can be transferred across images to fake attribution, and (2) reliance on detector-based verification, which is unreliable since detectors can be tricked. We present MetaSeal, a novel framework for content-dependent watermarking with cryptographic security guarantees to safeguard image attribution. Our design provides (1) forgery resistance, preventing unauthorized replication and enforcing cryptographic verification; (2) robust, self-contained protection, embedding attribution directly into images while maintaining resilience against benign transformations; and (3) evidence of tampering, making malicious alterations visually detectable. Experiments demonstrate that MetaSeal effectively mitigates forgery attempts and applies to both natural and AI-generated images, establishing a new standard for secure image attribution.
<div id='section'>Paperid: <span id='pid'>266, <a href='https://arxiv.org/pdf/2505.16934.pdf' target='_blank'>https://arxiv.org/pdf/2505.16934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yepeng Liu, Xuandong Zhao, Christopher Kruegel, Dawn Song, Yuheng Bu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.16934">In-Context Watermarks for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing use of large language models (LLMs) for sensitive applications has highlighted the need for effective watermarking techniques to ensure the provenance and accountability of AI-generated text. However, most existing watermarking methods require access to the decoding process, limiting their applicability in real-world settings. One illustrative example is the use of LLMs by dishonest reviewers in the context of academic peer review, where conference organizers have no access to the model used but still need to detect AI-generated reviews. Motivated by this gap, we introduce In-Context Watermarking (ICW), which embeds watermarks into generated text solely through prompt engineering, leveraging LLMs' in-context learning and instruction-following abilities. We investigate four ICW strategies at different levels of granularity, each paired with a tailored detection method. We further examine the Indirect Prompt Injection (IPI) setting as a specific case study, in which watermarking is covertly triggered by modifying input documents such as academic manuscripts. Our experiments validate the feasibility of ICW as a model-agnostic, practical watermarking approach. Moreover, our findings suggest that as LLMs become more capable, ICW offers a promising direction for scalable and accessible content attribution.
<div id='section'>Paperid: <span id='pid'>267, <a href='https://arxiv.org/pdf/2502.10673.pdf' target='_blank'>https://arxiv.org/pdf/2502.10673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yepeng Liu, Xuandong Zhao, Dawn Song, Yuheng Bu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10673">Dataset Protection via Watermarked Canaries in Retrieval-Augmented LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-Augmented Generation (RAG) has become an effective method for enhancing large language models (LLMs) with up-to-date knowledge. However, it poses a significant risk of IP infringement, as IP datasets may be incorporated into the knowledge database by malicious Retrieval-Augmented LLMs (RA-LLMs) without authorization. To protect the rights of the dataset owner, an effective dataset membership inference algorithm for RA-LLMs is needed. In this work, we introduce a novel approach to safeguard the ownership of text datasets and effectively detect unauthorized use by the RA-LLMs. Our approach preserves the original data completely unchanged while protecting it by inserting specifically designed canary documents into the IP dataset. These canary documents are created with synthetic content and embedded watermarks to ensure uniqueness, stealthiness, and statistical provability. During the detection process, unauthorized usage is identified by querying the canary documents and analyzing the responses of RA-LLMs for statistical evidence of the embedded watermark. Our experimental results demonstrate high query efficiency, detectability, and stealthiness, along with minimal perturbation to the original dataset, all without compromising the performance of the RAG system.
<div id='section'>Paperid: <span id='pid'>268, <a href='https://arxiv.org/pdf/2411.18479.pdf' target='_blank'>https://arxiv.org/pdf/2411.18479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tramer, Somesh Jha, Lei Li, Yu-Xiang Wang, Dawn Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.18479">SoK: Watermarking for AI-Generated Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the outputs of generative AI (GenAI) techniques improve in quality, it becomes increasingly challenging to distinguish them from human-created content. Watermarking schemes are a promising approach to address the problem of distinguishing between AI and human-generated content. These schemes embed hidden signals within AI-generated content to enable reliable detection. While watermarking is not a silver bullet for addressing all risks associated with GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by combating misinformation and deception. This paper presents a comprehensive overview of watermarking techniques for GenAI, beginning with the need for watermarking from historical and regulatory perspectives. We formalize the definitions and desired properties of watermarking schemes and examine the key objectives and threat models for existing approaches. Practical evaluation strategies are also explored, providing insights into the development of robust watermarking techniques capable of resisting various attacks. Additionally, we review recent representative works, highlight open challenges, and discuss potential directions for this emerging field. By offering a thorough understanding of watermarking in GenAI, this work aims to guide researchers in advancing watermarking methods and applications, and support policymakers in addressing the broader implications of GenAI.
<div id='section'>Paperid: <span id='pid'>269, <a href='https://arxiv.org/pdf/2411.12989.pdf' target='_blank'>https://arxiv.org/pdf/2411.12989.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sixiao Zhang, Cheng Long, Wei Yuan, Hongxu Chen, Hongzhi Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.12989">Data Watermarking for Sequential Recommender Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of large foundation models, data has become a crucial component in building high-performance AI systems. As the demand for high-quality and large-scale data continues to rise, data copyright protection is attracting increasing attention. In this work, we explore the problem of data watermarking for sequential recommender systems, where a watermark is embedded into the target dataset and can be detected in models trained on that dataset. We focus on two settings: dataset watermarking, which protects the ownership of the entire dataset, and user watermarking, which safeguards the data of individual users. We present a method named Dataset Watermarking for Recommender Systems (DWRS) to address them. We define the watermark as a sequence of consecutive items inserted into normal users' interaction sequences. We define a Receptive Field (RF) to guide the inserting process to facilitate the memorization of the watermark. Extensive experiments on five representative sequential recommendation models and three benchmark datasets demonstrate the effectiveness of DWRS in protecting data copyright while preserving model utility.
<div id='section'>Paperid: <span id='pid'>270, <a href='https://arxiv.org/pdf/2408.05500.pdf' target='_blank'>https://arxiv.org/pdf/2408.05500.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Wei, Yang Wang, Kuofeng Gao, Shuo Shao, Yiming Li, Zhibo Wang, Zhan Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05500">PointNCBW: Towards Dataset Ownership Verification for Point Clouds via Negative Clean-label Backdoor Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, point clouds have been widely used in computer vision, whereas their collection is time-consuming and expensive. As such, point cloud datasets are the valuable intellectual property of their owners and deserve protection. To detect and prevent unauthorized use of these datasets, especially for commercial or open-sourced ones that cannot be sold again or used commercially without permission, we intend to identify whether a suspicious third-party model is trained on our protected dataset under the black-box setting. We achieve this goal by designing a scalable clean-label backdoor-based dataset watermark for point clouds that ensures both effectiveness and stealthiness. Unlike existing clean-label watermark schemes, which are susceptible to the number of categories, our method could watermark samples from all classes instead of only from the target one. Accordingly, it can still preserve high effectiveness even on large-scale datasets with many classes. Specifically, we perturb selected point clouds with non-target categories in both shape-wise and point-wise manners before inserting trigger patterns without changing their labels. The features of perturbed samples are similar to those of benign samples from the target class. As such, models trained on the watermarked dataset will have a distinctive yet stealthy backdoor behavior, i.e., misclassifying samples from the target class whenever triggers appear, since the trained DNNs will treat the inserted trigger pattern as a signal to deny predicting the target label. We also design a hypothesis-test-guided dataset ownership verification based on the proposed watermark. Extensive experiments on benchmark datasets are conducted, verifying the effectiveness of our method and its resistance to potential removal methods.
<div id='section'>Paperid: <span id='pid'>271, <a href='https://arxiv.org/pdf/2407.21034.pdf' target='_blank'>https://arxiv.org/pdf/2407.21034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sixiao Zhang, Cheng Long, Wei Yuan, Hongxu Chen, Hongzhi Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.21034">Watermarking Recommender Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recommender systems embody significant commercial value and represent crucial intellectual property. However, the integrity of these systems is constantly challenged by malicious actors seeking to steal their underlying models. Safeguarding against such threats is paramount to upholding the rights and interests of the model owner. While model watermarking has emerged as a potent defense mechanism in various domains, its direct application to recommender systems remains unexplored and non-trivial. In this paper, we address this gap by introducing Autoregressive Out-of-distribution Watermarking (AOW), a novel technique tailored specifically for recommender systems. Our approach entails selecting an initial item and querying it through the oracle model, followed by the selection of subsequent items with small prediction scores. This iterative process generates a watermark sequence autoregressively, which is then ingrained into the model's memory through training. To assess the efficacy of the watermark, the model is tasked with predicting the subsequent item given a truncated watermark sequence. Through extensive experimentation and analysis, we demonstrate the superior performance and robust properties of AOW. Notably, our watermarking technique exhibits high-confidence extraction capabilities and maintains effectiveness even in the face of distillation and fine-tuning processes.
<div id='section'>Paperid: <span id='pid'>272, <a href='https://arxiv.org/pdf/2502.12176.pdf' target='_blank'>https://arxiv.org/pdf/2502.12176.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tao Fan, Hanlin Gu, Xuemei Cao, Chee Seng Chan, Qian Chen, Yiqiang Chen, Yihui Feng, Yang Gu, Jiaxiang Geng, Bing Luo, Shuoling Liu, Win Kent Ong, Chao Ren, Jiaqi Shao, Chuan Sun, Xiaoli Tang, Hong Xi Tae, Yongxin Tong, Shuyue Wei, Fan Wu, Wei Xi, Mingcong Xu, He Yang, Xin Yang, Jiangpeng Yan, Hao Yu, Han Yu, Teng Zhang, Yifei Zhang, Xiaojin Zhang, Zhenzhe Zheng, Lixin Fan, Qiang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12176">Ten Challenging Problems in Federated Foundation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Foundation Models (FedFMs) represent a distributed learning paradigm that fuses general competences of foundation models as well as privacy-preserving capabilities of federated learning. This combination allows the large foundation models and the small local domain models at the remote clients to learn from each other in a teacher-student learning setting. This paper provides a comprehensive summary of the ten challenging problems inherent in FedFMs, encompassing foundational theory, utilization of private data, continual learning, unlearning, Non-IID and graph data, bidirectional knowledge transfer, incentive mechanism design, game mechanism design, model watermarking, and efficiency. The ten challenging problems manifest in five pivotal aspects: ``Foundational Theory," which aims to establish a coherent and unifying theoretical framework for FedFMs. ``Data," addressing the difficulties in leveraging domain-specific knowledge from private data while maintaining privacy; ``Heterogeneity," examining variations in data, model, and computational resources across clients; ``Security and Privacy," focusing on defenses against malicious attacks and model theft; and ``Efficiency," highlighting the need for improvements in training, communication, and parameter efficiency. For each problem, we offer a clear mathematical definition on the objective function, analyze existing methods, and discuss the key challenges and potential solutions. This in-depth exploration aims to advance the theoretical foundations of FedFMs, guide practical implementations, and inspire future research to overcome these obstacles, thereby enabling the robust, efficient, and privacy-preserving FedFMs in various real-world applications.
<div id='section'>Paperid: <span id='pid'>273, <a href='https://arxiv.org/pdf/2305.12461.pdf' target='_blank'>https://arxiv.org/pdf/2305.12461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Li, Borui Yang, Yujie Sun, Suyu Chen, Ziyun Song, Liyao Xiang, Xinbing Wang, Chenghu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12461">Towards Tracing Code Provenance with Code Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in large language models have raised wide concern in generating abundant plausible source code without scrutiny, and thus tracing the provenance of code emerges as a critical issue. To solve the issue, we propose CodeMark, a watermarking system that hides bit strings into variables respecting the natural and operational semantics of the code. For naturalness, we novelly introduce a contextual watermarking scheme to generate watermarked variables more coherent in the context atop graph neural networks. Each variable is treated as a node on the graph and the node feature gathers neighborhood (context) information through learning. Watermarks embedded into the features are thus reflected not only by the variables but also by the local contexts. We further introduce a pre-trained model on source code as a teacher to guide more natural variable generation. Throughout the embedding, the operational semantics are preserved as only variable names are altered. Beyond guaranteeing code-specific properties, CodeMark is superior in watermarking accuracy, capacity, and efficiency due to a more diversified pattern generated. Experimental results show CodeMark outperforms the SOTA watermarking systems with a better balance of the watermarking requirements.
<div id='section'>Paperid: <span id='pid'>274, <a href='https://arxiv.org/pdf/2411.13425.pdf' target='_blank'>https://arxiv.org/pdf/2411.13425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiacheng Liang, Zian Wang, Lauren Hong, Shouling Ji, Ting Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13425">Watermark under Fire: A Robustness Evaluation of LLM Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Various watermarking methods (``watermarkers'') have been proposed to identify LLM-generated texts; yet, due to the lack of unified evaluation platforms, many critical questions remain under-explored: i) What are the strengths/limitations of various watermarkers, especially their attack robustness? ii) How do various design choices impact their robustness? iii) How to optimally operate watermarkers in adversarial environments? To fill this gap, we systematize existing LLM watermarkers and watermark removal attacks, mapping out their design spaces. We then develop WaterPark, a unified platform that integrates 10 state-of-the-art watermarkers and 12 representative attacks. More importantly, by leveraging WaterPark, we conduct a comprehensive assessment of existing watermarkers, unveiling the impact of various design choices on their attack robustness. We further explore the best practices to operate watermarkers in adversarial environments. We believe our study sheds light on current LLM watermarking techniques while WaterPark serves as a valuable testbed to facilitate future research.
<div id='section'>Paperid: <span id='pid'>275, <a href='https://arxiv.org/pdf/2411.13144.pdf' target='_blank'>https://arxiv.org/pdf/2411.13144.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naen Xu, Changjiang Li, Tianyu Du, Minxi Li, Wenjie Luo, Jiacheng Liang, Yuyuan Li, Xuhong Zhang, Meng Han, Jianwei Yin, Ting Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13144">CopyrightMeter: Revisiting Copyright Protection in Text-to-image Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image diffusion models have emerged as powerful tools for generating high-quality images from textual descriptions. However, their increasing popularity has raised significant copyright concerns, as these models can be misused to reproduce copyrighted content without authorization. In response, recent studies have proposed various copyright protection methods, including adversarial perturbation, concept erasure, and watermarking techniques. However, their effectiveness and robustness against advanced attacks remain largely unexplored. Moreover, the lack of unified evaluation frameworks has hindered systematic comparison and fair assessment of different approaches. To bridge this gap, we systematize existing copyright protection methods and attacks, providing a unified taxonomy of their design spaces. We then develop CopyrightMeter, a unified evaluation framework that incorporates 17 state-of-the-art protections and 16 representative attacks. Leveraging CopyrightMeter, we comprehensively evaluate protection methods across multiple dimensions, thereby uncovering how different design choices impact fidelity, efficacy, and resilience under attacks. Our analysis reveals several key findings: (i) most protections (16/17) are not resilient against attacks; (ii) the "best" protection varies depending on the target priority; (iii) more advanced attacks significantly promote the upgrading of protections. These insights provide concrete guidance for developing more robust protection methods, while its unified evaluation protocol establishes a standard benchmark for future copyright protection research in text-to-image generation.
<div id='section'>Paperid: <span id='pid'>276, <a href='https://arxiv.org/pdf/2404.15639.pdf' target='_blank'>https://arxiv.org/pdf/2404.15639.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Batu Guan, Yao Wan, Zhangqian Bi, Zheng Wang, Hongyu Zhang, Pan Zhou, Lichao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.15639">CodeIP: A Grammar-Guided Multi-Bit Watermark for Large Language Models of Code</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have achieved remarkable progress in code generation. It now becomes crucial to identify whether the code is AI-generated and to determine the specific model used, particularly for purposes such as protecting Intellectual Property (IP) in industry and preventing cheating in programming exercises. To this end, several attempts have been made to insert watermarks into machine-generated code. However, existing approaches are limited to inserting only a single bit of information. In this paper, we introduce CodeIP, a novel multi-bit watermarking technique that inserts additional information to preserve crucial provenance details, such as the vendor ID of an LLM, thereby safeguarding the IPs of LLMs in code generation. Furthermore, to ensure the syntactical correctness of the generated code, we propose constraining the sampling process for predicting the next token by training a type predictor. Experiments conducted on a real-world dataset across five programming languages demonstrate the effectiveness of CodeIP in watermarking LLMs for code generation while maintaining the syntactical correctness of code.
<div id='section'>Paperid: <span id='pid'>277, <a href='https://arxiv.org/pdf/2310.11237.pdf' target='_blank'>https://arxiv.org/pdf/2310.11237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linyang Li, Botian Jiang, Pengyu Wang, Ke Ren, Hang Yan, Xipeng Qiu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.11237">Watermarking LLMs with Weight Quantization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Abuse of large language models reveals high risks as large language models are being deployed at an astonishing speed. It is important to protect the model weights to avoid malicious usage that violates licenses of open-source large language models. This paper proposes a novel watermarking strategy that plants watermarks in the quantization process of large language models without pre-defined triggers during inference. The watermark works when the model is used in the fp32 mode and remains hidden when the model is quantized to int8, in this way, the users can only inference the model without further supervised fine-tuning of the model. We successfully plant the watermark into open-source large language model weights including GPT-Neo and LLaMA. We hope our proposed method can provide a potential direction for protecting model weights in the era of large language model applications.
<div id='section'>Paperid: <span id='pid'>278, <a href='https://arxiv.org/pdf/2510.21115.pdf' target='_blank'>https://arxiv.org/pdf/2510.21115.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wu, Georgios Milis, Ruibo Chen, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.21115">Robust Distortion-Free Watermark for Autoregressive Audio Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of next-token-prediction models has led to widespread adoption across modalities, enabling the creation of realistic synthetic media. In the audio domain, while autoregressive speech models have propelled conversational interactions forward, the potential for misuse, such as impersonation in phishing schemes or crafting misleading speech recordings, has also increased. Security measures such as watermarking have thus become essential to ensuring the authenticity of digital media. Traditional statistical watermarking methods used for autoregressive language models face challenges when applied to autoregressive audio models, due to the inevitable ``retokenization mismatch'' - the discrepancy between original and retokenized discrete audio token sequences. To address this, we introduce Aligned-IS, a novel, distortion-free watermark, specifically crafted for audio generation models. This technique utilizes a clustering approach that treats tokens within the same cluster equivalently, effectively countering the retokenization mismatch issue. Our comprehensive testing on prevalent audio generation platforms demonstrates that Aligned-IS not only preserves the quality of generated audio but also significantly improves the watermark detectability compared to the state-of-the-art distortion-free watermarking adaptations, establishing a new benchmark in secure audio technology applications.
<div id='section'>Paperid: <span id='pid'>279, <a href='https://arxiv.org/pdf/2509.24048.pdf' target='_blank'>https://arxiv.org/pdf/2509.24048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wu, Xuehao Cui, Ruibo Chen, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24048">Analyzing and Evaluating Unbiased Language Model Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Verifying the authenticity of AI-generated text has become increasingly important with the rapid advancement of large language models, and unbiased watermarking has emerged as a promising approach due to its ability to preserve output distribution without degrading quality. However, recent work reveals that unbiased watermarks can accumulate distributional bias over multiple generations and that existing robustness evaluations are inconsistent across studies. To address these issues, we introduce UWbench, the first open-source benchmark dedicated to the principled evaluation of unbiased watermarking methods. Our framework combines theoretical and empirical contributions: we propose a statistical metric to quantify multi-batch distribution drift, prove an impossibility result showing that no unbiased watermark can perfectly preserve the distribution under infinite queries, and develop a formal analysis of robustness against token-level modification attacks. Complementing this theory, we establish a three-axis evaluation protocol: unbiasedness, detectability, and robustness, and show that token modification attacks provide more stable robustness assessments than paraphrasing-based methods. Together, UWbench offers the community a standardized and reproducible platform for advancing the design and evaluation of unbiased watermarking algorithms.
<div id='section'>Paperid: <span id='pid'>280, <a href='https://arxiv.org/pdf/2509.24043.pdf' target='_blank'>https://arxiv.org/pdf/2509.24043.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wu, Ruibo Chen, Georgios Milis, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24043">An Ensemble Framework for Unbiased Language Model Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models become increasingly capable and widely deployed, verifying the provenance of machine-generated content is critical to ensuring trust, safety, and accountability. Watermarking techniques have emerged as a promising solution by embedding imperceptible statistical signals into the generation process. Among them, unbiased watermarking is particularly attractive due to its theoretical guarantee of preserving the language model's output distribution, thereby avoiding degradation in fluency or detectability through distributional shifts. However, existing unbiased watermarking schemes often suffer from weak detection power and limited robustness, especially under short text lengths or distributional perturbations. In this work, we propose ENS, a novel ensemble framework that enhances the detectability and robustness of logits-based unbiased watermarks while strictly preserving their unbiasedness. ENS sequentially composes multiple independent watermark instances, each governed by a distinct key, to amplify the watermark signal. We theoretically prove that the ensemble construction remains unbiased in expectation and demonstrate how it improves the signal-to-noise ratio for statistical detectors. Empirical evaluations on multiple LLM families show that ENS substantially reduces the number of tokens needed for reliable detection and increases resistance to smoothing and paraphrasing attacks without compromising generation quality.
<div id='section'>Paperid: <span id='pid'>281, <a href='https://arxiv.org/pdf/2509.05753.pdf' target='_blank'>https://arxiv.org/pdf/2509.05753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ching-Chun Chang, Isao Echizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05753">Tell-Tale Watermarks for Explanatory Reasoning in Synthetic Media Forensics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of synthetic media has blurred the boundary between reality and fabrication under the evolving power of artificial intelligence, fueling an infodemic that erodes public trust in cyberspace. For digital imagery, a multitude of editing applications further complicates the forensic analysis, including semantic edits that alter content, photometric adjustments that recalibrate colour characteristics, and geometric projections that reshape viewpoints. Collectively, these transformations manipulate and control perceptual interpretation of digital imagery. This susceptibility calls for forensic enquiry into reconstructing the chain of events, thereby revealing deeper evidential insight into the presence or absence of criminal intent. This study seeks to address an inverse problem of tracing the underlying generation chain that gives rise to the observed synthetic media. A tell-tale watermarking system is developed for explanatory reasoning over the nature and extent of transformations across the lifecycle of synthetic media. Tell-tale watermarks are tailored to different classes of transformations, responding in a manner that is neither strictly robust nor fragile but instead interpretable. These watermarks function as reference clues that evolve under the same transformation dynamics as the carrier media, leaving interpretable traces when subjected to transformations. Explanatory reasoning is then performed to infer the most plausible account across the combinatorial parameter space of composite transformations. Experimental evaluations demonstrate the validity of tell-tale watermarking with respect to fidelity, synchronicity and traceability.
<div id='section'>Paperid: <span id='pid'>282, <a href='https://arxiv.org/pdf/2506.11371.pdf' target='_blank'>https://arxiv.org/pdf/2506.11371.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wu, Xuehao Cui, Ruibo Chen, Georgios Milis, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11371">A Watermark for Auto-Regressive Image Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid evolution of image generation models has revolutionized visual content creation, enabling the synthesis of highly realistic and contextually accurate images for diverse applications. However, the potential for misuse, such as deepfake generation, image based phishing attacks, and fabrication of misleading visual evidence, underscores the need for robust authenticity verification mechanisms. While traditional statistical watermarking techniques have proven effective for autoregressive language models, their direct adaptation to image generation models encounters significant challenges due to a phenomenon we term retokenization mismatch, a disparity between original and retokenized sequences during the image generation process. To overcome this limitation, we propose C-reweight, a novel, distortion-free watermarking method explicitly designed for image generation models. By leveraging a clustering-based strategy that treats tokens within the same cluster equivalently, C-reweight mitigates retokenization mismatch while preserving image fidelity. Extensive evaluations on leading image generation platforms reveal that C-reweight not only maintains the visual quality of generated images but also improves detectability over existing distortion-free watermarking techniques, setting a new standard for secure and trustworthy image synthesis.
<div id='section'>Paperid: <span id='pid'>283, <a href='https://arxiv.org/pdf/2502.11268.pdf' target='_blank'>https://arxiv.org/pdf/2502.11268.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11268">Improved Unbiased Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into language model-generated text without distorting the quality. In this paper, we introduce MCmark, a family of unbiased, Multi-Channel-based watermarks. MCmark works by partitioning the model's vocabulary into segments and promoting token probabilities within a selected segment based on a watermark key. We demonstrate that MCmark not only preserves the original distribution of the language model but also offers significant improvements in detectability and robustness over existing unbiased watermarks. Our experiments with widely-used language models demonstrate an improvement in detectability of over 10% using MCmark, compared to existing state-of-the-art unbiased watermarks. This advancement underscores MCmark's potential in enhancing the practical application of watermarking in AI-generated texts.
<div id='section'>Paperid: <span id='pid'>284, <a href='https://arxiv.org/pdf/2502.10440.pdf' target='_blank'>https://arxiv.org/pdf/2502.10440.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junfeng Guo, Yiming Li, Ruibo Chen, Yihan Wu, Chenxi Liu, Yanshuo Chen, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10440">Towards Copyright Protection for Knowledge Bases of Retrieval-augmented Language Models via Reasoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are increasingly integrated into real-world personalized applications through retrieval-augmented generation (RAG) mechanisms to supplement their responses with domain-specific knowledge. However, the valuable and often proprietary nature of the knowledge bases used in RAG introduces the risk of unauthorized usage by adversaries. Existing methods that can be generalized as watermarking techniques to protect these knowledge bases typically involve poisoning or backdoor attacks. However, these methods require altering the LLM's results of verification samples, inevitably making these watermarks susceptible to anomaly detection and even introducing new security risks. To address these challenges, we propose \name{} for `harmless' copyright protection of knowledge bases. Instead of manipulating LLM's final output, \name{} implants distinct yet benign verification behaviors in the space of chain-of-thought (CoT) reasoning, maintaining the correctness of the final answer. Our method has three main stages: (1) Generating CoTs: For each verification question, we generate two `innocent' CoTs, including a target CoT for building watermark behaviors; (2) Optimizing Watermark Phrases and Target CoTs: Inspired by our theoretical analysis, we optimize them to minimize retrieval errors under the \emph{black-box} and \emph{text-only} setting of suspicious LLM, ensuring that only watermarked verification queries can retrieve their correspondingly target CoTs contained in the knowledge base; (3) Ownership Verification: We exploit a pairwise Wilcoxon test to verify whether a suspicious LLM is augmented with the protected knowledge base by comparing its responses to watermarked and benign verification queries. Our experiments on diverse benchmarks demonstrate that \name{} effectively protects knowledge bases and its resistance to adaptive attacks.
<div id='section'>Paperid: <span id='pid'>285, <a href='https://arxiv.org/pdf/2501.09284.pdf' target='_blank'>https://arxiv.org/pdf/2501.09284.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giyeong Oh, Saejin Kim, Woohyun Cho, Sangkyu Lee, Jiwan Chung, Dokyung Song, Youngjae Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.09284">SEAL: Entangled White-box Watermarks on Low-Rank Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, LoRA and its variants have become the de facto strategy for training and sharing task-specific versions of large pretrained models, thanks to their efficiency and simplicity. However, the issue of copyright protection for LoRA weights, especially through watermark-based techniques, remains underexplored. To address this gap, we propose SEAL (SEcure wAtermarking on LoRA weights), the universal whitebox watermarking for LoRA. SEAL embeds a secret, non-trainable matrix between trainable LoRA weights, serving as a passport to claim ownership. SEAL then entangles the passport with the LoRA weights through training, without extra loss for entanglement, and distributes the finetuned weights after hiding the passport. When applying SEAL, we observed no performance degradation across commonsense reasoning, textual/visual instruction tuning, and text-to-image synthesis tasks. We demonstrate that SEAL is robust against a variety of known attacks: removal, obfuscation, and ambiguity attacks.
<div id='section'>Paperid: <span id='pid'>286, <a href='https://arxiv.org/pdf/2410.13808.pdf' target='_blank'>https://arxiv.org/pdf/2410.13808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13808">De-mark: Watermark Removal in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking techniques offer a promising way to identify machine-generated content via embedding covert information into the contents generated from language models (LMs). However, the robustness of the watermarking schemes has not been well explored. In this paper, we present De-mark, an advanced framework designed to remove n-gram-based watermarks effectively. Our method utilizes a novel querying strategy, termed random selection probing, which aids in assessing the strength of the watermark and identifying the red-green list within the n-gram watermark. Experiments on popular LMs, such as Llama3 and ChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark removal and exploitation tasks.
<div id='section'>Paperid: <span id='pid'>287, <a href='https://arxiv.org/pdf/2410.13805.pdf' target='_blank'>https://arxiv.org/pdf/2410.13805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruibo Chen, Yihan Wu, Yanshuo Chen, Chenxi Liu, Junfeng Guo, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.13805">A Watermark for Order-Agnostic Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Statistical watermarking techniques are well-established for sequentially decoded language models (LMs). However, these techniques cannot be directly applied to order-agnostic LMs, as the tokens in order-agnostic LMs are not generated sequentially. In this work, we introduce Pattern-mark, a pattern-based watermarking framework specifically designed for order-agnostic LMs. We develop a Markov-chain-based watermark generator that produces watermark key sequences with high-frequency key patterns. Correspondingly, we propose a statistical pattern-based detection algorithm that recovers the key sequence during detection and conducts statistical tests based on the count of high-frequency patterns. Our extensive evaluations on order-agnostic LMs, such as ProteinMPNN and CMLM, demonstrate Pattern-mark's enhanced detection efficiency, generation quality, and robustness, positioning it as a superior watermarking technique for order-agnostic LMs.
<div id='section'>Paperid: <span id='pid'>288, <a href='https://arxiv.org/pdf/2409.01541.pdf' target='_blank'>https://arxiv.org/pdf/2409.01541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Erjin Bao, Ching-Chun Chang, Hanrui Wang, Isao Echizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01541">Agentic Copyright Watermarking against Adversarial Evidence Forgery with Purification-Agnostic Curriculum Proxy Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the proliferation of AI agents in various domains, protecting the ownership of AI models has become crucial due to the significant investment in their development. Unauthorized use and illegal distribution of these models pose serious threats to intellectual property, necessitating effective copyright protection measures. Model watermarking has emerged as a key technique to address this issue, embedding ownership information within models to assert rightful ownership during copyright disputes. This paper presents several contributions to model watermarking: a self-authenticating black-box watermarking protocol using hash techniques, a study on evidence forgery attacks using adversarial perturbations, a proposed defense involving a purification step to counter adversarial attacks, and a purification-agnostic curriculum proxy learning method to enhance watermark robustness and model performance. Experimental results demonstrate the effectiveness of these approaches in improving the security, reliability, and performance of watermarked models.
<div id='section'>Paperid: <span id='pid'>289, <a href='https://arxiv.org/pdf/2406.02603.pdf' target='_blank'>https://arxiv.org/pdf/2406.02603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wu, Ruibo Chen, Zhengmian Hu, Yanshuo Chen, Junfeng Guo, Hongyang Zhang, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02603">Distortion-free Watermarks are not Truly Distortion-free under Watermark Key Collisions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language model (LM) watermarking techniques inject a statistical signal into LM-generated content by substituting the random sampling process with pseudo-random sampling, using watermark keys as the random seed. Among these statistical watermarking approaches, distortion-free watermarks are particularly crucial because they embed watermarks into LM-generated content without compromising generation quality. However, one notable limitation of pseudo-random sampling compared to true-random sampling is that, under the same watermark keys (i.e., key collision), the results of pseudo-random sampling exhibit correlations. This limitation could potentially undermine the distortion-free property. Our studies reveal that key collisions are inevitable due to the limited availability of watermark keys, and existing distortion-free watermarks exhibit a significant distribution bias toward the original LM distribution in the presence of key collisions. Moreover, achieving a perfect distortion-free watermark is impossible as no statistical signal can be embedded under key collisions. To reduce the distribution bias caused by key collisions, we introduce a new family of distortion-free watermarks--beta-watermark. Experimental results support that the beta-watermark can effectively reduce the distribution bias under key collisions.
<div id='section'>Paperid: <span id='pid'>290, <a href='https://arxiv.org/pdf/2511.04949.pdf' target='_blank'>https://arxiv.org/pdf/2511.04949.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tharindu Fernando, Clinton Fookes, Sridha Sridharan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04949">DeepForgeSeal: Latent Space-Driven Semi-Fragile Watermarking for Deepfake Detection Using Multi-Agent Adversarial Reinforcement Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advances in generative AI have led to increasingly realistic deepfakes, posing growing challenges for law enforcement and public trust. Existing passive deepfake detectors struggle to keep pace, largely due to their dependence on specific forgery artifacts, which limits their ability to generalize to new deepfake types. Proactive deepfake detection using watermarks has emerged to address the challenge of identifying high-quality synthetic media. However, these methods often struggle to balance robustness against benign distortions with sensitivity to malicious tampering. This paper introduces a novel deep learning framework that harnesses high-dimensional latent space representations and the Multi-Agent Adversarial Reinforcement Learning (MAARL) paradigm to develop a robust and adaptive watermarking approach. Specifically, we develop a learnable watermark embedder that operates in the latent space, capturing high-level image semantics, while offering precise control over message encoding and extraction. The MAARL paradigm empowers the learnable watermarking agent to pursue an optimal balance between robustness and fragility by interacting with a dynamic curriculum of benign and malicious image manipulations simulated by an adversarial attacker agent. Comprehensive evaluations on the CelebA and CelebA-HQ benchmarks reveal that our method consistently outperforms state-of-the-art approaches, achieving improvements of over 4.5% on CelebA and more than 5.3% on CelebA-HQ under challenging manipulation scenarios.
<div id='section'>Paperid: <span id='pid'>291, <a href='https://arxiv.org/pdf/2505.02824.pdf' target='_blank'>https://arxiv.org/pdf/2505.02824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kuofeng Gao, Yufei Zhu, Yiming Li, Jiawang Bai, Yong Yang, Zhifeng Li, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.02824">Towards Dataset Copyright Evasion Attack against Personalized Text-to-Image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image (T2I) diffusion models have rapidly advanced, enabling high-quality image generation conditioned on textual prompts. However, the growing trend of fine-tuning pre-trained models for personalization raises serious concerns about unauthorized dataset usage. To combat this, dataset ownership verification (DOV) has emerged as a solution, embedding watermarks into the fine-tuning datasets using backdoor techniques. These watermarks remain inactive under benign samples but produce owner-specified outputs when triggered. Despite the promise of DOV for T2I diffusion models, its robustness against copyright evasion attacks (CEA) remains unexplored. In this paper, we explore how attackers can bypass these mechanisms through CEA, allowing models to circumvent watermarks even when trained on watermarked datasets. We propose the first copyright evasion attack (i.e., CEAT2I) specifically designed to undermine DOV in T2I diffusion models. Concretely, our CEAT2I comprises three stages: watermarked sample detection, trigger identification, and efficient watermark mitigation. A key insight driving our approach is that T2I models exhibit faster convergence on watermarked samples during the fine-tuning, evident through intermediate feature deviation. Leveraging this, CEAT2I can reliably detect the watermarked samples. Then, we iteratively ablate tokens from the prompts of detected watermarked samples and monitor shifts in intermediate features to pinpoint the exact trigger tokens. Finally, we adopt a closed-form concept erasure method to remove the injected watermark. Extensive experiments show that our CEAT2I effectively evades DOV mechanisms while preserving model performance.
<div id='section'>Paperid: <span id='pid'>292, <a href='https://arxiv.org/pdf/2407.02411.pdf' target='_blank'>https://arxiv.org/pdf/2407.02411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinmin Li, Kuofeng Gao, Yang Bai, Jingyun Zhang, Shu-Tao Xia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02411">Video Watermarking: Safeguarding Your Video from (Unauthorized) Annotations by Video-based LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of video-based Large Language Models (LLMs) has significantly enhanced video understanding. However, it has also raised some safety concerns regarding data protection, as videos can be more easily annotated, even without authorization. This paper introduces Video Watermarking, a novel technique to protect videos from unauthorized annotations by such video-based LLMs, especially concerning the video content and description, in response to specific queries. By imperceptibly embedding watermarks into key video frames with multi-modal flow-based losses, our method preserves the viewing experience while preventing misuse by video-based LLMs. Extensive experiments show that Video Watermarking significantly reduces the comprehensibility of videos with various video-based LLMs, demonstrating both stealth and robustness. In essence, our method provides a solution for securing video content, ensuring its integrity and confidentiality in the face of evolving video-based LLMs technologies.
<div id='section'>Paperid: <span id='pid'>293, <a href='https://arxiv.org/pdf/2510.02902.pdf' target='_blank'>https://arxiv.org/pdf/2510.02902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linyu Wu, Linhao Zhong, Wenjie Qu, Yuexin Li, Yue Liu, Shengfang Zhai, Chunhua Shen, Jiaheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02902">DMark: Order-Agnostic Watermarking for Diffusion Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion large language models (dLLMs) offer faster generation than autoregressive models while maintaining comparable quality, but existing watermarking methods fail on them due to their non-sequential decoding. Unlike autoregressive models that generate tokens left-to-right, dLLMs can finalize tokens in arbitrary order, breaking the causal design underlying traditional watermarks. We present DMark, the first watermarking framework designed specifically for dLLMs. DMark introduces three complementary strategies to restore watermark detectability: predictive watermarking uses model-predicted tokens when actual context is unavailable; bidirectional watermarking exploits both forward and backward dependencies unique to diffusion decoding; and predictive-bidirectional watermarking combines both approaches to maximize detection strength. Experiments across multiple dLLMs show that DMark achieves 92.0-99.5% detection rates at 1% false positive rate while maintaining text quality, compared to only 49.6-71.2% for naive adaptations of existing methods. DMark also demonstrates robustness against text manipulations, establishing that effective watermarking is feasible for non-autoregressive language models.
<div id='section'>Paperid: <span id='pid'>294, <a href='https://arxiv.org/pdf/2402.12948.pdf' target='_blank'>https://arxiv.org/pdf/2402.12948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiayi Fu, Xuandong Zhao, Ruihan Yang, Yuansen Zhang, Jiangjie Chen, Yanghua Xiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12948">GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the GumbelMax-trick-based watermark(GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we propose a new type of GM watermark, the Logits-Addition watermark, and its three variants, specifically designed to enhance diversity. Among these, the GumbelSoft watermark (a softmax variant of the Logits-Addition watermark) demonstrates superior performance in high diversity settings, with its AUROC score outperforming those of the two alternative variants by 0.1 to 0.3 and surpassing other decoding-based watermarking methods by a minimum of 0.1.
<div id='section'>Paperid: <span id='pid'>295, <a href='https://arxiv.org/pdf/2310.10669.pdf' target='_blank'>https://arxiv.org/pdf/2310.10669.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengmian Hu, Lichang Chen, Xidong Wu, Yihan Wu, Hongyang Zhang, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.10669">Unbiased Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent advancements in large language models (LLMs) have sparked a growing apprehension regarding the potential misuse. One approach to mitigating this risk is to incorporate watermarking techniques into LLMs, allowing for the tracking and attribution of model outputs. This study examines a crucial aspect of watermarking: how significantly watermarks impact the quality of model-generated outputs. Previous studies have suggested a trade-off between watermark strength and output quality. However, our research demonstrates that it is possible to integrate watermarks without affecting the output probability distribution with appropriate implementation. We refer to this type of watermark as an unbiased watermark. This has significant implications for the use of LLMs, as it becomes impossible for users to discern whether a service provider has incorporated watermarks or not. Furthermore, the presence of watermarks does not compromise the performance of the model in downstream tasks, ensuring that the overall utility of the language model is preserved. Our findings contribute to the ongoing discussion around responsible AI development, suggesting that unbiased watermarks can serve as an effective means of tracking and attributing model outputs without sacrificing output quality.
<div id='section'>Paperid: <span id='pid'>296, <a href='https://arxiv.org/pdf/2310.07710.pdf' target='_blank'>https://arxiv.org/pdf/2310.07710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihan Wu, Zhengmian Hu, Junfeng Guo, Hongyang Zhang, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.07710">A Resilient and Accessible Distribution-Preserving Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking techniques offer a promising way to identify machine-generated content via embedding covert information into the contents generated from language models. A challenge in the domain lies in preserving the distribution of original generated content after watermarking. Our research extends and improves upon existing watermarking framework, placing emphasis on the importance of a \textbf{Di}stribution-\textbf{P}reserving (DiP) watermark. Contrary to the current strategies, our proposed DiPmark simultaneously preserves the original token distribution during watermarking (distribution-preserving), is detectable without access to the language model API and prompts (accessible), and is provably robust to moderate changes of tokens (resilient). DiPmark operates by selecting a random set of tokens prior to the generation of a word, then modifying the token distribution through a distribution-preserving reweight function to enhance the probability of these selected tokens during the sampling process. Extensive empirical evaluation on various language models and tasks demonstrates our approach's distribution-preserving property, accessibility, and resilience, making it a effective solution for watermarking tasks that demand impeccable quality preservation.
<div id='section'>Paperid: <span id='pid'>297, <a href='https://arxiv.org/pdf/2404.08221.pdf' target='_blank'>https://arxiv.org/pdf/2404.08221.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Archer Amon, Zhipeng Yin, Zichong Wang, Avash Palikhe, Wenbin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.08221">Uncertain Boundaries: Multidisciplinary Approaches to Copyright Issues in Generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI is becoming increasingly prevalent in creative fields, sparking urgent debates over how current copyright laws can keep pace with technological innovation. Recent controversies of AI models generating near-replicas of copyrighted material highlight the need to adapt current legal frameworks and develop technical methods to mitigate copyright infringement risks. This task requires understanding the intersection between computational concepts such as large-scale data scraping and probabilistic content generation, legal definitions of originality and fair use, and economic impacts on IP rights holders. However, most existing research on copyright in AI takes a purely computer science or law-based approach, leaving a gap in coordinating these approaches that only multidisciplinary efforts can effectively address. To bridge this gap, our survey adopts a comprehensive approach synthesizing insights from law, policy, economics, and computer science. It begins by discussing the foundational goals and considerations that should be applied to copyright in generative AI, followed by methods for detecting and assessing potential violations in AI system outputs. Next, it explores various regulatory options influenced by legal, policy, and economic frameworks to manage and mitigate copyright concerns associated with generative AI and reconcile the interests of IP rights holders with that of generative AI producers. The discussion then introduces techniques to safeguard individual creative works from unauthorized replication, such as watermarking and cryptographic protections. Finally, it describes advanced training strategies designed to prevent AI models from reproducing protected content. In doing so, we highlight key opportunities for action and offer actionable strategies that creators, developers, and policymakers can use in navigating the evolving copyright landscape.
<div id='section'>Paperid: <span id='pid'>298, <a href='https://arxiv.org/pdf/2512.06774.pdf' target='_blank'>https://arxiv.org/pdf/2512.06774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Longjie Zhao, Ziming Hong, Zhenyang Ren, Runnan Chen, Mingming Gong, Tongliang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06774">RDSplat: Robust Watermarking Against Diffusion Editing for 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has enabled the creation of digital assets and downstream applications, underscoring the need for robust copyright protection via digital watermarking. However, existing 3DGS watermarking methods remain highly vulnerable to diffusion-based editing, which can easily erase embedded provenance. This challenge highlights the urgent need for 3DGS watermarking techniques that are intrinsically resilient to diffusion-based editing. In this paper, we introduce RDSplat, a Robust watermarking paradigm against Diffusion editing for 3D Gaussian Splatting. RDSplat embeds watermarks into 3DGS components that diffusion-based editing inherently preserve, achieved through (i) proactively targeting low-frequency Gaussians and (ii) adversarial training with a diffusion proxy. Specifically, we introduce a multi-domain framework that operates natively in 3DGS space and embeds watermarks into diffusion-editing-preserved low-frequency Gaussians via coordinated covariance regularization and 2D filtering. In addition, we exploit the low-pass filtering behavior of diffusion-based editing by using Gaussian blur as an efficient training surrogate, enabling adversarial fine-tuning that further enhances watermark robustness against diffusion-based editing. Empirically, comprehensive quantitative and qualitative evaluations on three benchmark datasets demonstrate that RDSplat not only maintains superior robustness under diffusion-based editing, but also preserves watermark invisibility, achieving state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>299, <a href='https://arxiv.org/pdf/2503.00531.pdf' target='_blank'>https://arxiv.org/pdf/2503.00531.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyi Li, Xuanyu Zhang, Chuhan Tong, Zhipei Xu, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.00531">GaussianSeal: Rooting Adaptive Watermarks for 3D Gaussian Generation Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of AIGC technologies, the modalities generated by models have expanded from images and videos to 3D objects, leading to an increasing number of works focused on 3D Gaussian Splatting (3DGS) generative models. Existing research on copyright protection for generative models has primarily concentrated on watermarking in image and text modalities, with little exploration into the copyright protection of 3D object generative models. In this paper, we propose the first bit watermarking framework for 3DGS generative models, named GaussianSeal, to enable the decoding of bits as copyright identifiers from the rendered outputs of generated 3DGS. By incorporating adaptive bit modulation modules into the generative model and embedding them into the network blocks in an adaptive way, we achieve high-precision bit decoding with minimal training overhead while maintaining the fidelity of the model's outputs. Experiments demonstrate that our method outperforms post-processing watermarking approaches for 3DGS objects, achieving superior performance of watermark decoding accuracy and preserving the quality of the generated results.
<div id='section'>Paperid: <span id='pid'>300, <a href='https://arxiv.org/pdf/2412.01615.pdf' target='_blank'>https://arxiv.org/pdf/2412.01615.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanyu Zhang, Zecheng Tang, Zhipei Xu, Runyi Li, Youmin Xu, Bin Chen, Feng Gao, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.01615">OmniGuard: Hybrid Manipulation Localization via Augmented Versatile Deep Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid growth of generative AI and its widespread application in image editing, new risks have emerged regarding the authenticity and integrity of digital content. Existing versatile watermarking approaches suffer from trade-offs between tamper localization precision and visual quality. Constrained by the limited flexibility of previous framework, their localized watermark must remain fixed across all images. Under AIGC-editing, their copyright extraction accuracy is also unsatisfactory. To address these challenges, we propose OmniGuard, a novel augmented versatile watermarking approach that integrates proactive embedding with passive, blind extraction for robust copyright protection and tamper localization. OmniGuard employs a hybrid forensic framework that enables flexible localization watermark selection and introduces a degradation-aware tamper extraction network for precise localization under challenging conditions. Additionally, a lightweight AIGC-editing simulation layer is designed to enhance robustness across global and local editing. Extensive experiments show that OmniGuard achieves superior fidelity, robustness, and flexibility. Compared to the recent state-of-the-art approach EditGuard, our method outperforms it by 4.25dB in PSNR of the container image, 20.7% in F1-Score under noisy conditions, and 14.8% in average bit accuracy.
<div id='section'>Paperid: <span id='pid'>301, <a href='https://arxiv.org/pdf/2405.16596.pdf' target='_blank'>https://arxiv.org/pdf/2405.16596.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runyi Li, Xuanyu Zhang, Zhipei Xu, Yongbing Zhang, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.16596">Protect-Your-IP: Scalable Source-Tracing and Attribution against Personalized Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advent of personalized generation models, users can more readily create images resembling existing content, heightening the risk of violating portrait rights and intellectual property (IP). Traditional post-hoc detection and source-tracing methods for AI-generated content (AIGC) employ proactive watermark approaches; however, these are less effective against personalized generation models. Moreover, attribution techniques for AIGC rely on passive detection but often struggle to differentiate AIGC from authentic images, presenting a substantial challenge. Integrating these two processes into a cohesive framework not only meets the practical demands for protection and forensics but also improves the effectiveness of attribution tasks. Inspired by this insight, we propose a unified approach for image copyright source-tracing and attribution, introducing an innovative watermarking-attribution method that blends proactive and passive strategies. We embed copyright watermarks into protected images and train a watermark decoder to retrieve copyright information from the outputs of personalized models, using this watermark as an initial step for confirming if an image is AIGC-generated. To pinpoint specific generation techniques, we utilize powerful visual backbone networks for classification. Additionally, we implement an incremental learning strategy to adeptly attribute new personalized models without losing prior knowledge, thereby enhancing the model's adaptability to novel generation methods. We have conducted experiments using various celebrity portrait series sourced online, and the results affirm the efficacy of our method in source-tracing and attribution tasks, as well as its robustness against knowledge forgetting.
<div id='section'>Paperid: <span id='pid'>302, <a href='https://arxiv.org/pdf/2404.16824.pdf' target='_blank'>https://arxiv.org/pdf/2404.16824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuanyu Zhang, Youmin Xu, Runyi Li, Jiwen Yu, Weiqi Li, Zhipei Xu, Jian Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16824">V2A-Mark: Versatile Deep Visual-Audio Watermarking for Manipulation Localization and Copyright Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated video has revolutionized short video production, filmmaking, and personalized media, making video local editing an essential tool. However, this progress also blurs the line between reality and fiction, posing challenges in multimedia forensics. To solve this urgent issue, V2A-Mark is proposed to address the limitations of current video tampering forensics, such as poor generalizability, singular function, and single modality focus. Combining the fragility of video-into-video steganography with deep robust watermarking, our method can embed invisible visual-audio localization watermarks and copyright watermarks into the original video frames and audio, enabling precise manipulation localization and copyright protection. We also design a temporal alignment and fusion module and degradation prompt learning to enhance the localization accuracy and decoding robustness. Meanwhile, we introduce a sample-level audio localization method and a cross-modal copyright extraction mechanism to couple the information of audio and video frames. The effectiveness of V2A-Mark has been verified on a visual-audio tampering dataset, emphasizing its superiority in localization precision and copyright accuracy, crucial for the sustainable development of video editing in the AIGC video era.
<div id='section'>Paperid: <span id='pid'>303, <a href='https://arxiv.org/pdf/2305.18339.pdf' target='_blank'>https://arxiv.org/pdf/2305.18339.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, Tom H. Luan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18339">A Survey on ChatGPT: AI-Generated Contents, Challenges, and Solutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread use of large artificial intelligence (AI) models such as ChatGPT, AI-generated content (AIGC) has garnered increasing attention and is leading a paradigm shift in content creation and knowledge representation. AIGC uses generative large AI algorithms to assist or replace humans in creating massive, high-quality, and human-like content at a faster pace and lower cost, based on user-provided prompts. Despite the recent significant progress in AIGC, security, privacy, ethical, and legal challenges still need to be addressed. This paper presents an in-depth survey of working principles, security and privacy threats, state-of-the-art solutions, and future challenges of the AIGC paradigm. Specifically, we first explore the enabling technologies, general architecture of AIGC, and discuss its working modes and key characteristics. Then, we investigate the taxonomy of security and privacy threats to AIGC and highlight the ethical and societal implications of GPT and AIGC technologies. Furthermore, we review the state-of-the-art AIGC watermarking approaches for regulatable AIGC paradigms regarding the AIGC model and its produced content. Finally, we identify future challenges and open research directions related to AIGC.
<div id='section'>Paperid: <span id='pid'>304, <a href='https://arxiv.org/pdf/2301.10226.pdf' target='_blank'>https://arxiv.org/pdf/2301.10226.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.10226">A Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of "green" tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.
<div id='section'>Paperid: <span id='pid'>305, <a href='https://arxiv.org/pdf/2411.11478.pdf' target='_blank'>https://arxiv.org/pdf/2411.11478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kui Ren, Ziqi Yang, Li Lu, Jian Liu, Yiming Li, Jie Wan, Xiaodi Zhao, Xianheng Feng, Shuo Shao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11478">SoK: On the Role and Future of AIGC Watermarking in the Era of Gen-AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of AI technology, particularly in generating AI-generated content (AIGC), has transformed numerous fields, e.g., art video generation, but also brings new risks, including the misuse of AI for misinformation and intellectual property theft. To address these concerns, AIGC watermarks offer an effective solution to mitigate malicious activities. However, existing watermarking surveys focus more on traditional watermarks, overlooking AIGC-specific challenges. In this work, we propose a systematic investigation into AIGC watermarking and provide the first formal definition of AIGC watermarking. Different from previous surveys, we provide a taxonomy based on the core properties of the watermark which are summarized through comprehensive literature from various AIGC modalities. Derived from the properties, we discuss the functionality and security threats of AIGC watermarking. In the end, we thoroughly investigate the AIGC governance of different countries and practitioners. We believe this taxonomy better aligns with the practical demands for watermarking in the era of GenAI, thus providing a clearer summary of existing work and uncovering potential future research directions for the community.
<div id='section'>Paperid: <span id='pid'>306, <a href='https://arxiv.org/pdf/2309.03466.pdf' target='_blank'>https://arxiv.org/pdf/2309.03466.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Lu, Wenxuan Li, Mi Zhang, Xudong Pan, Min Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03466">Neural Dehydration: Effective Erasure of Black-box Watermarks from DNNs with Limited Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To protect the intellectual property of well-trained deep neural networks (DNNs), black-box watermarks, which are embedded into the prediction behavior of DNN models on a set of specially-crafted samples and extracted from suspect models using only API access, have gained increasing popularity in both academy and industry. Watermark robustness is usually implemented against attackers who steal the protected model and obfuscate its parameters for watermark removal. However, current robustness evaluations are primarily performed under moderate attacks or unrealistic settings. Existing removal attacks could only crack a small subset of the mainstream black-box watermarks, and fall short in four key aspects: incomplete removal, reliance on prior knowledge of the watermark, performance degradation, and high dependency on data.
  In this paper, we propose a watermark-agnostic removal attack called \textsc{Neural Dehydration} (\textit{abbrev.} \textsc{Dehydra}), which effectively erases all ten mainstream black-box watermarks from DNNs, with only limited or even no data dependence. In general, our attack pipeline exploits the internals of the protected model to recover and unlearn the watermark message. We further design target class detection and recovered sample splitting algorithms to reduce the utility loss and achieve data-free watermark removal on five of the watermarking schemes. We conduct comprehensive evaluation of \textsc{Dehydra} against ten mainstream black-box watermarks on three benchmark datasets and DNN architectures. Compared with existing removal attacks, \textsc{Dehydra} achieves strong removal effectiveness across all the covered watermarks, preserving at least $90\%$ of the stolen model utility, under the data-limited settings, i.e., less than $2\%$ of the training data or even data-free.
<div id='section'>Paperid: <span id='pid'>307, <a href='https://arxiv.org/pdf/2303.09732.pdf' target='_blank'>https://arxiv.org/pdf/2303.09732.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Yan, Xudong Pan, Mi Zhang, Min Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09732">Rethinking White-Box Watermarks on Deep Learning Models under Neural Structural Obfuscation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Copyright protection for deep neural networks (DNNs) is an urgent need for AI corporations. To trace illegally distributed model copies, DNN watermarking is an emerging technique for embedding and verifying secret identity messages in the prediction behaviors or the model internals. Sacrificing less functionality and involving more knowledge about the target DNN, the latter branch called \textit{white-box DNN watermarking} is believed to be accurate, credible and secure against most known watermark removal attacks, with emerging research efforts in both the academy and the industry.
  In this paper, we present the first systematic study on how the mainstream white-box DNN watermarks are commonly vulnerable to neural structural obfuscation with \textit{dummy neurons}, a group of neurons which can be added to a target model but leave the model behavior invariant. Devising a comprehensive framework to automatically generate and inject dummy neurons with high stealthiness, our novel attack intensively modifies the architecture of the target model to inhibit the success of watermark verification. With extensive evaluation, our work for the first time shows that nine published watermarking schemes require amendments to their verification procedures.
<div id='section'>Paperid: <span id='pid'>308, <a href='https://arxiv.org/pdf/2511.04711.pdf' target='_blank'>https://arxiv.org/pdf/2511.04711.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyuan Yang, Yichen Sun, Changzheng Chen, Zhixuan Chu, Jiaheng Zhang, Yiming Li, Dacheng Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.04711">SWAP: Towards Copyright Auditing of Soft Prompts via Sequential Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large-scale vision-language models, especially CLIP, have demonstrated remarkable performance across diverse downstream tasks. Soft prompts, as carefully crafted modules that efficiently adapt vision-language models to specific tasks, necessitate effective copyright protection. In this paper, we investigate model copyright protection by auditing whether suspicious third-party models incorporate protected soft prompts. While this can be viewed as a special case of model ownership auditing, our analysis shows that existing techniques are ineffective due to prompt learning's unique characteristics. Non-intrusive auditing is inherently prone to false positives when independent models share similar data distributions with victim models. Intrusive approaches also fail: backdoor methods designed for CLIP cannot embed functional triggers, while extending traditional DNN backdoor techniques to prompt learning suffers from harmfulness and ambiguity challenges. We find that these failures in intrusive auditing stem from the same fundamental reason: watermarking operates within the same decision space as the primary task yet pursues opposing objectives. Motivated by these findings, we propose sequential watermarking for soft prompts (SWAP), which implants watermarks into a different and more complex space. SWAP encodes watermarks through a specific order of defender-specified out-of-distribution classes, inspired by the zero-shot prediction capability of CLIP. This watermark, which is embedded in a more complex space, keeps the original prediction label unchanged, making it less opposed to the primary task. We further design a hypothesis-test-guided verification protocol for SWAP and provide theoretical analyses of success conditions. Extensive experiments on 11 datasets demonstrate SWAP's effectiveness, harmlessness, and robustness against potential adaptive attacks.
<div id='section'>Paperid: <span id='pid'>309, <a href='https://arxiv.org/pdf/2509.17993.pdf' target='_blank'>https://arxiv.org/pdf/2509.17993.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxin Yang, Bangzhen Liu, Xuemiao Xu, Cheng Xu, Yuyang Yu, Zikai Huang, Yi Wang, Shengfeng He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17993">StableGuard: Towards Unified Copyright Protection and Tamper Localization in Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of diffusion models has enhanced the realism of AI-generated content but also raised concerns about misuse, necessitating robust copyright protection and tampering localization. Although recent methods have made progress toward unified solutions, their reliance on post hoc processing introduces considerable application inconvenience and compromises forensic reliability. We propose StableGuard, a novel framework that seamlessly integrates a binary watermark into the diffusion generation process, ensuring copyright protection and tampering localization in Latent Diffusion Models through an end-to-end design. We develop a Multiplexing Watermark VAE (MPW-VAE) by equipping a pretrained Variational Autoencoder (VAE) with a lightweight latent residual-based adapter, enabling the generation of paired watermarked and watermark-free images. These pairs, fused via random masks, create a diverse dataset for training a tampering-agnostic forensic network. To further enhance forensic synergy, we introduce a Mixture-of-Experts Guided Forensic Network (MoE-GFN) that dynamically integrates holistic watermark patterns, local tampering traces, and frequency-domain cues for precise watermark verification and tampered region detection. The MPW-VAE and MoE-GFN are jointly optimized in a self-supervised, end-to-end manner, fostering a reciprocal training between watermark embedding and forensic accuracy. Extensive experiments demonstrate that StableGuard consistently outperforms state-of-the-art methods in image fidelity, watermark verification, and tampering localization.
<div id='section'>Paperid: <span id='pid'>310, <a href='https://arxiv.org/pdf/2507.05512.pdf' target='_blank'>https://arxiv.org/pdf/2507.05512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gehao Zhang, Eugene Bagdasarian, Juan Zhai, Shiqing Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.05512">Disappearing Ink: Obfuscation Breaks N-gram Code Watermarks in Theory and Practice</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Distinguishing AI-generated code from human-written code is becoming crucial for tasks such as authorship attribution, content tracking, and misuse detection. Based on this, N-gram-based watermarking schemes have emerged as prominent, which inject secret watermarks to be detected during the generation.
  However, their robustness in code content remains insufficiently evaluated. Most claims rely solely on defenses against simple code transformations or code optimizations as a simulation of attack, creating a questionable sense of robustness. In contrast, more sophisticated schemes already exist in the software engineering world, e.g., code obfuscation, which significantly alters code while preserving functionality. Although obfuscation is commonly used to protect intellectual property or evade software scanners, the robustness of code watermarking techniques against such transformations remains largely unexplored.
  In this work, we formally model the code obfuscation and prove the impossibility of N-gram-based watermarking's robustness with only one intuitive and experimentally verified assumption, distribution consistency, satisfied. Given the original false positive rate of the watermarking detection, the ratio that the detector failed on the watermarked code after obfuscation will increase to 1 - fpr.
  The experiments have been performed on three SOTA watermarking schemes, two LLMs, two programming languages, four code benchmarks, and four obfuscators. Among them, all watermarking detectors show coin-flipping detection abilities on obfuscated codes (AUROC tightly surrounds 0.5). Among all models, watermarking schemes, and datasets, both programming languages own obfuscators that can achieve attack effects with no detection AUROC higher than 0.6 after the attack. Based on the theoretical and practical observations, we also proposed a potential path of robust code watermarking.
<div id='section'>Paperid: <span id='pid'>311, <a href='https://arxiv.org/pdf/2506.03067.pdf' target='_blank'>https://arxiv.org/pdf/2506.03067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingzhe Li, Gehao Zhang, Zhenting Wang, Shiqing Ma, Siqi Pan, Richard Cartwright, Juan Zhai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.03067">EDITOR: Effective and Interpretable Prompt Inversion for Text-to-Image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image generation models~(e.g., Stable Diffusion) have achieved significant advancements, enabling the creation of high-quality and realistic images based on textual descriptions. Prompt inversion, the task of identifying the textual prompt used to generate a specific artifact, holds significant potential for applications including data attribution, model provenance, and watermarking validation. Recent studies introduced a delayed projection scheme to optimize for prompts representative of the vocabulary space, though challenges in semantic fluency and efficiency remain. Advanced image captioning models or visual large language models can generate highly interpretable prompts, but they often lack in image similarity. In this paper, we propose a prompt inversion technique called \sys for text-to-image diffusion models, which includes initializing embeddings using a pre-trained image captioning model, refining them through reverse-engineering in the latent space, and converting them to texts using an embedding-to-text model. Our experiments on the widely-used datasets, such as MS COCO, LAION, and Flickr, show that our method outperforms existing methods in terms of image similarity, textual alignment, prompt interpretability and generalizability. We further illustrate the application of our generated prompts in tasks such as cross-concept image synthesis, concept manipulation, evolutionary multi-concept generation and unsupervised segmentation.
<div id='section'>Paperid: <span id='pid'>312, <a href='https://arxiv.org/pdf/2401.16820.pdf' target='_blank'>https://arxiv.org/pdf/2401.16820.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjie Qu, Wengrui Zheng, Tianyang Tao, Dong Yin, Yanze Jiang, Zhihua Tian, Wei Zou, Jinyuan Jia, Jiaheng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.16820">Provably Robust Multi-bit Watermarking for AI-generated Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated remarkable capabilities of generating texts resembling human language. However, they can be misused by criminals to create deceptive content, such as fake news and phishing emails, which raises ethical concerns. Watermarking is a key technique to address these concerns, which embeds a message (e.g., a bit string) into a text generated by an LLM. By embedding the user ID (represented as a bit string) into generated texts, we can trace generated texts to the user, known as content source tracing. The major limitation of existing watermarking techniques is that they achieve sub-optimal performance for content source tracing in real-world scenarios. The reason is that they cannot accurately or efficiently extract a long message from a generated text. We aim to address the limitations.
  In this work, we introduce a new watermarking method for LLM-generated text grounded in pseudo-random segment assignment. We also propose multiple techniques to further enhance the robustness of our watermarking algorithm. We conduct extensive experiments to evaluate our method. Our experimental results show that our method substantially outperforms existing baselines in both accuracy and robustness on benchmark datasets. For instance, when embedding a message of length 20 into a 200-token generated text, our method achieves a match rate of $97.6\%$, while the state-of-the-art work Yoo et al. only achieves $49.2\%$. Additionally, we prove that our watermark can tolerate edits within an edit distance of 17 on average for each paragraph under the same setting.
<div id='section'>Paperid: <span id='pid'>313, <a href='https://arxiv.org/pdf/2305.13257.pdf' target='_blank'>https://arxiv.org/pdf/2305.13257.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixin Liu, Hongsheng Hu, Xun Chen, Xuyun Zhang, Lichao Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.13257">Watermarking Text Data on Large Language Models for Dataset Copyright</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Substantial research works have shown that deep models, e.g., pre-trained models, on the large corpus can learn universal language representations, which are beneficial for downstream NLP tasks. However, these powerful models are also vulnerable to various privacy attacks, while much sensitive information exists in the training dataset. The attacker can easily steal sensitive information from public models, e.g., individuals' email addresses and phone numbers. In an attempt to address these issues, particularly the unauthorized use of private data, we introduce a novel watermarking technique via a backdoor-based membership inference approach named TextMarker, which can safeguard diverse forms of private information embedded in the training text data. Specifically, TextMarker only requires data owners to mark a small number of samples for data copyright protection under the black-box access assumption to the target model. Through extensive evaluation, we demonstrate the effectiveness of TextMarker on various real-world datasets, e.g., marking only 0.1% of the training dataset is practically sufficient for effective membership inference with negligible effect on model utility. We also discuss potential countermeasures and show that TextMarker is stealthy enough to bypass them.
<div id='section'>Paperid: <span id='pid'>314, <a href='https://arxiv.org/pdf/2505.18471.pdf' target='_blank'>https://arxiv.org/pdf/2505.18471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guoheng Sun, Ziyao Wang, Xuandong Zhao, Bowei Tian, Zheyu Shen, Yexiao He, Jinming Xing, Ang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.18471">Invisible Tokens, Visible Bills: The Urgent Need to Audit Hidden Operations in Opaque LLM Services</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern large language model (LLM) services increasingly rely on complex, often abstract operations, such as multi-step reasoning and multi-agent collaboration, to generate high-quality outputs. While users are billed based on token consumption and API usage, these internal steps are typically not visible. We refer to such systems as Commercial Opaque LLM Services (COLS). This position paper highlights emerging accountability challenges in COLS: users are billed for operations they cannot observe, verify, or contest. We formalize two key risks: \textit{quantity inflation}, where token and call counts may be artificially inflated, and \textit{quality downgrade}, where providers might quietly substitute lower-cost models or tools. Addressing these risks requires a diverse set of auditing strategies, including commitment-based, predictive, behavioral, and signature-based methods. We further explore the potential of complementary mechanisms such as watermarking and trusted execution environments to enhance verifiability without compromising provider confidentiality. We also propose a modular three-layer auditing framework for COLS and users that enables trustworthy verification across execution, secure logging, and user-facing auditability without exposing proprietary internals. Our aim is to encourage further research and policy development toward transparency, auditability, and accountability in commercial LLM services.
<div id='section'>Paperid: <span id='pid'>315, <a href='https://arxiv.org/pdf/2205.09026.pdf' target='_blank'>https://arxiv.org/pdf/2205.09026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Simone Soderi, Alessandro Brighente, Federico Turrin, Mauro Conti
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2205.09026">VLC Physical Layer Security through RIS-aided Jamming Receiver for 6G Wireless Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visible Light Communication (VLC) is one the most promising enabling technology for future 6G networks to overcome Radio-Frequency (RF)-based communication limitations thanks to a broader bandwidth, higher data rate, and greater efficiency. However, from the security perspective, VLCs suffer from all known wireless communication security threats (e.g., eavesdropping and integrity attacks). For this reason, security researchers are proposing innovative Physical Layer Security (PLS) solutions to protect such communication. Among the different solutions, the novel Reflective Intelligent Surface (RIS) technology coupled with VLCs has been successfully demonstrated in recent work to improve the VLC communication capacity. However, to date, the literature still lacks analysis and solutions to show the PLS capability of RIS-based VLC communication. In this paper, we combine watermarking and jamming primitives through the Watermark Blind Physical Layer Security (WBPLSec) algorithm to secure VLC communication at the physical layer. Our solution leverages RIS technology to improve the security properties of the communication. By using an optimization framework, we can calculate RIS phases to maximize the WBPLSec jamming interference schema over a predefined area in the room. In particular, compared to a scenario without RIS, our solution improves the performance in terms of secrecy capacity without any assumption about the adversary's location. We validate through numerical evaluations the positive impact of RIS-aided solution to increase the secrecy capacity of the legitimate jamming receiver in a VLC indoor scenario. Our results show that the introduction of RIS technology extends the area where secure communication occurs and that by increasing the number of RIS elements the outage probability decreases.
<div id='section'>Paperid: <span id='pid'>316, <a href='https://arxiv.org/pdf/2506.06018.pdf' target='_blank'>https://arxiv.org/pdf/2506.06018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyi Zhu, Zaitang Li, Renyi Yang, Robert Birke, Pin-Yu Chen, Tsung-Yi Ho, Lydia Y. Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06018">Optimization-Free Universal Watermark Forgery with Regenerative Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking becomes one of the pivotal solutions to trace and verify the origin of synthetic images generated by artificial intelligence models, but it is not free of risks. Recent studies demonstrate the capability to forge watermarks from a target image onto cover images via adversarial optimization without knowledge of the target generative model and watermark schemes. In this paper, we uncover a greater risk of an optimization-free and universal watermark forgery that harnesses existing regenerative diffusion models. Our proposed forgery attack, PnP (Plug-and-Plant), seamlessly extracts and integrates the target watermark via regenerating the image, without needing any additional optimization routine. It allows for universal watermark forgery that works independently of the target image's origin or the watermarking model used. We explore the watermarked latent extracted from the target image and visual-textual context of cover images as priors to guide sampling of the regenerative process. Extensive evaluation on 24 scenarios of model-data-watermark combinations demonstrates that PnP can successfully forge the watermark (up to 100% detectability and user attribution), and maintain the best visual perception. By bypassing model retraining and enabling adaptability to any image, our approach significantly broadens the scope of forgery attacks, presenting a greater challenge to the security of current watermarking techniques for diffusion models and the authority of watermarking schemes in synthetic data generation and governance.
<div id='section'>Paperid: <span id='pid'>317, <a href='https://arxiv.org/pdf/2402.19054.pdf' target='_blank'>https://arxiv.org/pdf/2402.19054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Xu, Yunlin Tan, Cheng Zhang, Kai Chi, Peng Sun, Wenyuan Yang, Ju Ren, Hongbo Jiang, Yaoxue Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.19054">RobWE: Robust Watermark Embedding for Personalized Federated Learning Model Ownership Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embedding watermarks into models has been widely used to protect model ownership in federated learning (FL). However, existing methods are inadequate for protecting the ownership of personalized models acquired by clients in personalized FL (PFL). This is due to the aggregation of the global model in PFL, resulting in conflicts over clients' private watermarks. Moreover, malicious clients may tamper with embedded watermarks to facilitate model leakage and evade accountability. This paper presents a robust watermark embedding scheme, named RobWE, to protect the ownership of personalized models in PFL. We first decouple the watermark embedding of personalized models into two parts: head layer embedding and representation layer embedding. The head layer belongs to clients' private part without participating in model aggregation, while the representation layer is the shared part for aggregation. For representation layer embedding, we employ a watermark slice embedding operation, which avoids watermark embedding conflicts. Furthermore, we design a malicious watermark detection scheme enabling the server to verify the correctness of watermarks before aggregating local models. We conduct an exhaustive experimental evaluation of RobWE. The results demonstrate that RobWE significantly outperforms the state-of-the-art watermark embedding schemes in FL in terms of fidelity, reliability, and robustness.
<div id='section'>Paperid: <span id='pid'>318, <a href='https://arxiv.org/pdf/2508.11548.pdf' target='_blank'>https://arxiv.org/pdf/2508.11548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenhua Xu, Xubin Yue, Zhebo Wang, Qichen Liu, Xixiang Zhao, Jingxuan Zhang, Wenjun Zeng, Wengpeng Xing, Dezhang Kong, Changting Lin, Meng Han
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11548">Copyright Protection for Large Language Models: A Survey of Methods, Challenges, and Trends</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Copyright protection for large language models is of critical importance, given their substantial development costs, proprietary value, and potential for misuse. Existing surveys have predominantly focused on techniques for tracing LLM-generated content-namely, text watermarking-while a systematic exploration of methods for protecting the models themselves (i.e., model watermarking and model fingerprinting) remains absent. Moreover, the relationships and distinctions among text watermarking, model watermarking, and model fingerprinting have not been comprehensively clarified. This work presents a comprehensive survey of the current state of LLM copyright protection technologies, with a focus on model fingerprinting, covering the following aspects: (1) clarifying the conceptual connection from text watermarking to model watermarking and fingerprinting, and adopting a unified terminology that incorporates model watermarking into the broader fingerprinting framework; (2) providing an overview and comparison of diverse text watermarking techniques, highlighting cases where such methods can function as model fingerprinting; (3) systematically categorizing and comparing existing model fingerprinting approaches for LLM copyright protection; (4) presenting, for the first time, techniques for fingerprint transfer and fingerprint removal; (5) summarizing evaluation metrics for model fingerprints, including effectiveness, harmlessness, robustness, stealthiness, and reliability; and (6) discussing open challenges and future research directions. This survey aims to offer researchers a thorough understanding of both text watermarking and model fingerprinting technologies in the era of LLMs, thereby fostering further advances in protecting their intellectual property.
<div id='section'>Paperid: <span id='pid'>319, <a href='https://arxiv.org/pdf/2507.18034.pdf' target='_blank'>https://arxiv.org/pdf/2507.18034.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan An, Guang Hua, Hangcheng Cao, Zhengru Fang, Guowen Xu, Susanto Rahardja, Yuguang Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18034">Removing Box-Free Watermarks for Image-to-Image Models via Query-Based Reverse Engineering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intellectual property of deep generative networks (GNets) can be protected using a cascaded hiding network (HNet) which embeds watermarks (or marks) into GNet outputs, known as box-free watermarking. Although both GNet and HNet are encapsulated in a black box (called operation network, or ONet), with only the generated and marked outputs from HNet being released to end users and deemed secure, in this paper, we reveal an overlooked vulnerability in such systems. Specifically, we show that the hidden GNet outputs can still be reliably estimated via query-based reverse engineering, leaking the generated and unmarked images, despite the attacker's limited knowledge of the system. Our first attempt is to reverse-engineer an inverse model for HNet under the stringent black-box condition, for which we propose to exploit the query process with specially curated input images. While effective, this method yields unsatisfactory image quality. To improve this, we subsequently propose an alternative method leveraging the equivalent additive property of box-free model watermarking and reverse-engineering a forward surrogate model of HNet, with better image quality preservation. Extensive experimental results on image processing and image generation tasks demonstrate that both attacks achieve impressive watermark removal success rates (100%) while also maintaining excellent image quality (reaching the highest PSNR of 34.69 dB), substantially outperforming existing attacks, highlighting the urgent need for robust defensive strategies to mitigate the identified vulnerability in box-free model watermarking.
<div id='section'>Paperid: <span id='pid'>320, <a href='https://arxiv.org/pdf/2502.20924.pdf' target='_blank'>https://arxiv.org/pdf/2502.20924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan An, Guang Hua, Zhengru Fang, Guowen Xu, Susanto Rahardja, Yuguang Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.20924">Decoder Gradient Shield: Provable and High-Fidelity Prevention of Gradient-Based Box-Free Watermark Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intellectual property of deep image-to-image models can be protected by the so-called box-free watermarking. It uses an encoder and a decoder, respectively, to embed into and extract from the model's output images invisible copyright marks. Prior works have improved watermark robustness, focusing on the design of better watermark encoders. In this paper, we reveal an overlooked vulnerability of the unprotected watermark decoder which is jointly trained with the encoder and can be exploited to train a watermark removal network. To defend against such an attack, we propose the decoder gradient shield (DGS) as a protection layer in the decoder API to prevent gradient-based watermark removal with a closed-form solution. The fundamental idea is inspired by the classical adversarial attack, but is utilized for the first time as a defensive mechanism in the box-free model watermarking. We then demonstrate that DGS can reorient and rescale the gradient directions of watermarked queries and stop the watermark remover's training loss from converging to the level without DGS, while retaining decoder output image quality. Experimental results verify the effectiveness of proposed method. Code of paper will be made available upon acceptance.
<div id='section'>Paperid: <span id='pid'>321, <a href='https://arxiv.org/pdf/2312.12049.pdf' target='_blank'>https://arxiv.org/pdf/2312.12049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Mu, Yu Wang, Zhengan Huang, Junzuo Lai, Yehong Zhang, Hui Wang, Yue Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.12049">EncryIP: A Practical Encryption-Based Framework for Model Intellectual Property Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly growing digital economy, protecting intellectual property (IP) associated with digital products has become increasingly important. Within this context, machine learning (ML) models, being highly valuable digital assets, have gained significant attention for IP protection. This paper introduces a practical encryption-based framework called \textit{EncryIP}, which seamlessly integrates a public-key encryption scheme into the model learning process. This approach enables the protected model to generate randomized and confused labels, ensuring that only individuals with accurate secret keys, signifying authorized users, can decrypt and reveal authentic labels. Importantly, the proposed framework not only facilitates the protected model to multiple authorized users without requiring repetitive training of the original ML model with IP protection methods but also maintains the model's performance without compromising its accuracy. Compared to existing methods like watermark-based, trigger-based, and passport-based approaches, \textit{EncryIP} demonstrates superior effectiveness in both training protected models and efficiently detecting the unauthorized spread of ML models.
<div id='section'>Paperid: <span id='pid'>322, <a href='https://arxiv.org/pdf/2303.09858.pdf' target='_blank'>https://arxiv.org/pdf/2303.09858.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingxing Wei, Bangzheng Pu, Shiji Zhao, Chen Chi, Huazhu Fu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09858">Preventing Unauthorized AI Over-Analysis by Medical Image Adversarial Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of deep learning has facilitated the integration of Artificial Intelligence (AI) into clinical practices, particularly in computer-aided diagnosis. Given the pivotal role of medical images in various diagnostic procedures, it becomes imperative to ensure the responsible and secure utilization of AI techniques. However, the unauthorized utilization of AI for image analysis raises significant concerns regarding patient privacy and potential infringement on the proprietary rights of data custodians. Consequently, the development of pragmatic and cost-effective strategies that safeguard patient privacy and uphold medical image copyrights emerges as a critical necessity. In direct response to this pressing demand, we present a pioneering solution named Medical Image Adversarial watermarking (MIAD-MARK). Our approach introduces watermarks that strategically mislead unauthorized AI diagnostic models, inducing erroneous predictions without compromising the integrity of the visual content. Importantly, our method integrates an authorization protocol tailored for legitimate users, enabling the removal of the MIAD-MARK through encryption-generated keys. Through extensive experiments, we validate the efficacy of MIAD-MARK across three prominent medical image datasets. The empirical outcomes demonstrate the substantial impact of our approach, notably reducing the accuracy of standard AI diagnostic models to a mere 8.57% under white box conditions and 45.83% in the more challenging black box scenario. Additionally, our solution effectively mitigates unauthorized exploitation of medical images even in the presence of sophisticated watermark removal networks. Notably, those AI diagnosis networks exhibit a meager average accuracy of 38.59% when applied to images protected by MIAD-MARK, underscoring the robustness of our safeguarding mechanism.
<div id='section'>Paperid: <span id='pid'>323, <a href='https://arxiv.org/pdf/2512.12090.pdf' target='_blank'>https://arxiv.org/pdf/2512.12090.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samar Fares, Nurbek Tastan, Karthik Nandakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.12090">SPDMark: Selective Parameter Displacement for Robust Video Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.
<div id='section'>Paperid: <span id='pid'>324, <a href='https://arxiv.org/pdf/2511.22262.pdf' target='_blank'>https://arxiv.org/pdf/2511.22262.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkai Huang, Yijia Guo, Gaolei Li, Lei Ma, Hang Zhang, Liwen Hu, Jiazheng Wang, Jianhua Li, Tiejun Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22262">Can Protective Watermarking Safeguard the Copyright of 3D Gaussian Splatting?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has emerged as a powerful representation for 3D scenes, widely adopted due to its exceptional efficiency and high-fidelity visual quality. Given the significant value of 3DGS assets, recent works have introduced specialized watermarking schemes to ensure copyright protection and ownership verification. However, can existing 3D Gaussian watermarking approaches genuinely guarantee robust protection of the 3D assets? In this paper, for the first time, we systematically explore and validate possible vulnerabilities of 3DGS watermarking frameworks. We demonstrate that conventional watermark removal techniques designed for 2D images do not effectively generalize to the 3DGS scenario due to the specialized rendering pipeline and unique attributes of each gaussian primitives. Motivated by this insight, we propose GSPure, the first watermark purification framework specifically for 3DGS watermarking representations. By analyzing view-dependent rendering contributions and exploiting geometrically accurate feature clustering, GSPure precisely isolates and effectively removes watermark-related Gaussian primitives while preserving scene integrity. Extensive experiments demonstrate that our GSPure achieves the best watermark purification performance, reducing watermark PSNR by up to 16.34dB while minimizing degradation to original scene fidelity with less than 1dB PSNR loss. Moreover, it consistently outperforms existing methods in both effectiveness and generalization.
<div id='section'>Paperid: <span id='pid'>325, <a href='https://arxiv.org/pdf/2510.00293.pdf' target='_blank'>https://arxiv.org/pdf/2510.00293.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samar Fares, Nurbek Tastan, Noor Hussein, Karthik Nandakumar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00293">MOLM: Mixture of LoRA Markers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models can generate photorealistic images at scale. This raises urgent concerns about the ability to detect synthetically generated images and attribute these images to specific sources. While watermarking has emerged as a possible solution, existing methods remain fragile to realistic distortions, susceptible to adaptive removal, and expensive to update when the underlying watermarking key changes. We propose a general watermarking framework that formulates the encoding problem as key-dependent perturbation of the parameters of a generative model. Within this framework, we introduce Mixture of LoRA Markers (MOLM), a routing-based instantiation in which binary keys activate lightweight LoRA adapters inside residual and attention blocks. This design avoids key-specific re-training and achieves the desired properties such as imperceptibility, fidelity, verifiability, and robustness. Experiments on Stable Diffusion and FLUX show that MOLM preserves image quality while achieving robust key recovery against distortions, compression and regeneration, averaging attacks, and black-box adversarial attacks on the extractor.
<div id='section'>Paperid: <span id='pid'>326, <a href='https://arxiv.org/pdf/2509.19812.pdf' target='_blank'>https://arxiv.org/pdf/2509.19812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yang Cui, Peter Pan, Lei He, Sheng Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.19812">Efficient Speech Watermarking for Speech Synthesis via Progressive Knowledge Distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid advancement of speech generative models, unauthorized voice cloning poses significant privacy and security risks. Speech watermarking offers a viable solution for tracing sources and preventing misuse. Current watermarking technologies fall mainly into two categories: DSP-based methods and deep learning-based methods. DSP-based methods are efficient but vulnerable to attacks, whereas deep learning-based methods offer robust protection at the expense of significantly higher computational cost. To improve the computational efficiency and enhance the robustness, we propose PKDMark, a lightweight deep learning-based speech watermarking method that leverages progressive knowledge distillation (PKD). Our approach proceeds in two stages: (1) training a high-performance teacher model using an invertible neural network-based architecture, and (2) transferring the teacher's capabilities to a compact student model through progressive knowledge distillation. This process reduces computational costs by 93.6% while maintaining high level of robust performance and imperceptibility. Experimental results demonstrate that our distilled model achieves an average detection F1 score of 99.6% with a PESQ of 4.30 in advanced distortions, enabling efficient speech watermarking for real-time speech synthesis applications.
<div id='section'>Paperid: <span id='pid'>327, <a href='https://arxiv.org/pdf/2509.13982.pdf' target='_blank'>https://arxiv.org/pdf/2509.13982.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boyu Zhang, Ping He, Tianyu Du, Xuhong Zhang, Lei Yun, Kingsum Chow, Jianwei Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.13982">CLMTracing: Black-box User-level Watermarking for Code Language Model Tracing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread adoption of open-source code language models (code LMs), intellectual property (IP) protection has become an increasingly critical concern. While current watermarking techniques have the potential to identify the code LM to protect its IP, they have limitations when facing the more practical and complex demand, i.e., offering the individual user-level tracing in the black-box setting. This work presents CLMTracing, a black-box code LM watermarking framework employing the rule-based watermarks and utility-preserving injection method for user-level model tracing. CLMTracing further incorporates a parameter selection algorithm sensitive to the robust watermark and adversarial training to enhance the robustness against watermark removal attacks. Comprehensive evaluations demonstrate CLMTracing is effective across multiple state-of-the-art (SOTA) code LMs, showing significant harmless improvements compared to existing SOTA baselines and strong robustness against various removal attacks.
<div id='section'>Paperid: <span id='pid'>328, <a href='https://arxiv.org/pdf/2504.21044.pdf' target='_blank'>https://arxiv.org/pdf/2504.21044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbo Gao, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.21044">AGATE: Stealthy Black-box Watermarking for Multimodal Model Copyright Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancement in large-scale Artificial Intelligence (AI) models offering multimodal services have become foundational in AI systems, making them prime targets for model theft. Existing methods select Out-of-Distribution (OoD) data as backdoor watermarks and retrain the original model for copyright protection. However, existing methods are susceptible to malicious detection and forgery by adversaries, resulting in watermark evasion. In this work, we propose Model-\underline{ag}nostic Black-box Backdoor W\underline{ate}rmarking Framework (AGATE) to address stealthiness and robustness challenges in multimodal model copyright protection. Specifically, we propose an adversarial trigger generation method to generate stealthy adversarial triggers from ordinary dataset, providing visual fidelity while inducing semantic shifts. To alleviate the issue of anomaly detection among model outputs, we propose a post-transform module to correct the model output by narrowing the distance between adversarial trigger image embedding and text embedding. Subsequently, a two-phase watermark verification is proposed to judge whether the current model infringes by comparing the two results with and without the transform module. Consequently, we consistently outperform state-of-the-art methods across five datasets in the downstream tasks of multimodal image-text retrieval and image classification. Additionally, we validated the robustness of AGATE under two adversarial attack scenarios.
<div id='section'>Paperid: <span id='pid'>329, <a href='https://arxiv.org/pdf/2504.11774.pdf' target='_blank'>https://arxiv.org/pdf/2504.11774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Keke Gai, Ziyue Shen, Jing Yu, Liehuang Zhu, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11774">PCDiff: Proactive Control for Ownership Protection in Diffusion Models with Watermark Compatibility</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the growing demand for protecting the intellectual property (IP) of text-to-image diffusion models, we propose PCDiff -- a proactive access control framework that redefines model authorization by regulating generation quality. At its core, PCDIFF integrates a trainable fuser module and hierarchical authentication layers into the decoder architecture, ensuring that only users with valid encrypted credentials can generate high-fidelity images. In the absence of valid keys, the system deliberately degrades output quality, effectively preventing unauthorized exploitation.Importantly, while the primary mechanism enforces active access control through architectural intervention, its decoupled design retains compatibility with existing watermarking techniques. This satisfies the need of model owners to actively control model ownership while preserving the traceability capabilities provided by traditional watermarking approaches.Extensive experimental evaluations confirm a strong dependency between credential verification and image quality across various attack scenarios. Moreover, when combined with typical post-processing operations, PCDIFF demonstrates powerful performance alongside conventional watermarking methods. This work shifts the paradigm from passive detection to proactive enforcement of authorization, laying the groundwork for IP management of diffusion models.
<div id='section'>Paperid: <span id='pid'>330, <a href='https://arxiv.org/pdf/2502.13345.pdf' target='_blank'>https://arxiv.org/pdf/2502.13345.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13345">Secure and Efficient Watermarking for Latent Diffusion Models in Model Distribution Scenarios</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent diffusion models have exhibited considerable potential in generative tasks. Watermarking is considered to be an alternative to safeguard the copyright of generative models and prevent their misuse. However, in the context of model distribution scenarios, the accessibility of models to large scale of model users brings new challenges to the security, efficiency and robustness of existing watermark solutions. To address these issues, we propose a secure and efficient watermarking solution. A new security mechanism is designed to prevent watermark leakage and watermark escape, which considers watermark randomness and watermark-model association as two constraints for mandatory watermark injection. To reduce the time cost of training the security module, watermark injection and the security mechanism are decoupled, ensuring that fine-tuning VAE only accomplishes the security mechanism without the burden of learning watermark patterns. A watermark distribution-based verification strategy is proposed to enhance the robustness against diverse attacks in the model distribution scenarios. Experimental results prove that our watermarking consistently outperforms existing six baselines on effectiveness and robustness against ten image processing attacks and adversarial attacks, while enhancing security in the distribution scenarios.
<div id='section'>Paperid: <span id='pid'>331, <a href='https://arxiv.org/pdf/2412.03121.pdf' target='_blank'>https://arxiv.org/pdf/2412.03121.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yijia Guo, Wenkai Huang, Yang Li, Gaolei Li, Hang Zhang, Liwen Hu, Jianhua Li, Tiejun Huang, Lei Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03121">Splats in Splats: Embedding Invisible 3D Watermark within Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian splatting (3DGS) has demonstrated impressive 3D reconstruction performance with explicit scene representations. Given the widespread application of 3DGS in 3D reconstruction and generation tasks, there is an urgent need to protect the copyright of 3DGS assets. However, existing copyright protection techniques for 3DGS overlook the usability of 3D assets, posing challenges for practical deployment. Here we describe WaterGS, the first 3DGS watermarking framework that embeds 3D content in 3DGS itself without modifying any attributes of the vanilla 3DGS. To achieve this, we take a deep insight into spherical harmonics (SH) and devise an importance-graded SH coefficient encryption strategy to embed the hidden SH coefficients. Furthermore, we employ a convolutional autoencoder to establish a mapping between the original Gaussian primitives' opacity and the hidden Gaussian primitives' opacity. Extensive experiments indicate that WaterGS significantly outperforms existing 3D steganography techniques, with 5.31% higher scene fidelity and 3X faster rendering speed, while ensuring security, robustness, and user experience. Codes and data will be released at https://water-gs.github.io.
<div id='section'>Paperid: <span id='pid'>332, <a href='https://arxiv.org/pdf/2411.11688.pdf' target='_blank'>https://arxiv.org/pdf/2411.11688.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu, Qi Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11688">Watermarking Visual Concepts for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The personalization techniques of diffusion models succeed in generating images with specific concepts. This ability also poses great threats to copyright protection and network security since malicious users can generate unauthorized content and disinformation relevant to a target concept. Model watermarking is an effective solution to trace the malicious generated images and safeguard their copyright. However, existing model watermarking techniques merely achieve image-level tracing without concept traceability. When tracing infringing or harmful concepts, current approaches execute image concept detection and model tracing sequentially, where performance is critically constrained by concept detection accuracy. In this paper, we propose a lightweight concept watermarking framework that efficiently binds target concepts to model watermarks, supporting simultaneous concept identification and model tracing via single-stage watermark verification. To further enhance the robustness of concept watermarking, we propose an adversarial perturbation injection method collaboratively embedded with watermarks during image generation, avoiding watermark removal by model purification attacks. Experimental results demonstrate that ConceptWM significantly outperforms state-of-the-art watermarking methods, improving detection accuracy by 6.3%-19.3% across diverse datasets including COCO and StableDiffusionDB. Additionally, ConceptWM possesses a critical capability absent in other watermarking methods: it sustains a 21.7% FID/CLIP degradation under adversarial fine-tuning of Stable Diffusion models on WikiArt and CelebA-HQ, demonstrating its capability to mitigate model misuse.
<div id='section'>Paperid: <span id='pid'>333, <a href='https://arxiv.org/pdf/2409.16056.pdf' target='_blank'>https://arxiv.org/pdf/2409.16056.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuguang Yao, Anil Jain, Sijia Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.16056">Adversarial Watermarking for Face Recognition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is an essential technique for embedding an identifier (i.e., watermark message) within digital images to assert ownership and monitor unauthorized alterations. In face recognition systems, watermarking plays a pivotal role in ensuring data integrity and security. However, an adversary could potentially interfere with the watermarking process, significantly impairing recognition performance. We explore the interaction between watermarking and adversarial attacks on face recognition models. Our findings reveal that while watermarking or input-level perturbation alone may have a negligible effect on recognition accuracy, the combined effect of watermarking and perturbation can result in an adversarial watermarking attack, significantly degrading recognition performance. Specifically, we introduce a novel threat model, the adversarial watermarking attack, which remains stealthy in the absence of watermarking, allowing images to be correctly recognized initially. However, once watermarking is applied, the attack is activated, causing recognition failures. Our study reveals a previously unrecognized vulnerability: adversarial perturbations can exploit the watermark message to evade face recognition systems. Evaluated on the CASIA-WebFace dataset, our proposed adversarial watermarking attack reduces face matching accuracy by 67.2% with an $\ell_\infty$ norm-measured perturbation strength of ${2}/{255}$ and by 95.9% with a strength of ${4}/{255}$.
<div id='section'>Paperid: <span id='pid'>334, <a href='https://arxiv.org/pdf/2405.02696.pdf' target='_blank'>https://arxiv.org/pdf/2405.02696.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liangqi Lei, Keke Gai, Jing Yu, Liehuang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02696">DiffuseTrace: A Transparent and Flexible Watermarking Scheme for Latent Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Latent Diffusion Models (LDMs) enable a wide range of applications but raise ethical concerns regarding illegal utilization. Adding watermarks to generative model outputs is a vital technique employed for copyright tracking and mitigating potential risks associated with Artificial Intelligence (AI)-generated contents. However, post-processed watermarking methods are unable to withstand generative watermark attacks and there exists a trade-off between image fidelity and watermark strength. Therefore, we propose a novel technique called DiffuseTrace. DiffuseTrace does not rely on fine-tuning of the diffusion model components. The multi-bit watermark is a embedded into the image space semantically without compromising image quality. The watermark component can be utilized as a plug-in in arbitrary diffusion models. We validate through experiments the effectiveness and flexibility of DiffuseTrace. Under 8 types of image processing watermark attacks and 3 types of generative watermark attacks, DiffuseTrace maintains watermark detection rate of 99% and attribution accuracy of over 94%.
<div id='section'>Paperid: <span id='pid'>335, <a href='https://arxiv.org/pdf/2404.18890.pdf' target='_blank'>https://arxiv.org/pdf/2404.18890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuguang Yao, Steven Grosz, Sijia Liu, Anil Jain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18890">Hide and Seek: How Does Watermarking Impact Face Recognition?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent progress in generative models has revolutionized the synthesis of highly realistic images, including face images. This technological development has undoubtedly helped face recognition, such as training data augmentation for higher recognition accuracy and data privacy. However, it has also introduced novel challenges concerning the responsible use and proper attribution of computer generated images. We investigate the impact of digital watermarking, a technique for embedding ownership signatures into images, on the effectiveness of face recognition models. We propose a comprehensive pipeline that integrates face image generation, watermarking, and face recognition to systematically examine this question. The proposed watermarking scheme, based on an encoder-decoder architecture, successfully embeds and recovers signatures from both real and synthetic face images while preserving their visual fidelity. Through extensive experiments, we unveil that while watermarking enables robust image attribution, it results in a slight decline in face recognition accuracy, particularly evident for face images with challenging poses and expressions. Additionally, we find that directly training face recognition models on watermarked images offers only a limited alleviation of this performance decline. Our findings underscore the intricate trade off between watermarking and face recognition accuracy. This work represents a pivotal step towards the responsible utilization of generative models in face recognition and serves to initiate discussions regarding the broader implications of watermarking in biometrics.
<div id='section'>Paperid: <span id='pid'>336, <a href='https://arxiv.org/pdf/2509.24823.pdf' target='_blank'>https://arxiv.org/pdf/2509.24823.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedetta Tondi, Andrea Costanzo, Mauro Barni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24823">Of-SemWat: High-payload text embedding for semantic watermarking of AI-generated images with arbitrary size</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a high-payload image watermarking method for textual embedding, where a semantic description of the image - which may also correspond to the input text prompt-, is embedded inside the image. In order to be able to robustly embed high payloads in large-scale images - such as those produced by modern AI generators - the proposed approach builds upon a traditional watermarking scheme that exploits orthogonal and turbo codes for improved robustness, and integrates frequency-domain embedding and perceptual masking techniques to enhance watermark imperceptibility. Experiments show that the proposed method is extremely robust against a wide variety of image processing, and the embedded text can be retrieved also after traditional and AI inpainting, permitting to unveil the semantic modification the image has undergone via image-text mismatch analysis.
<div id='section'>Paperid: <span id='pid'>337, <a href='https://arxiv.org/pdf/2509.02447.pdf' target='_blank'>https://arxiv.org/pdf/2509.02447.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinrui Zhong, Xinze Feng, Jingwei Zuo, Fanjiang Ye, Yi Mu, Junfeng Guo, Heng Huang, Myungjin Lee, Yuke Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.02447">An Efficient and Adaptive Watermark Detection System with Tile-based Error Correction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Efficient and reliable detection of generated images is critical for the responsible deployment of generative models. Existing approaches primarily focus on improving detection accuracy and robustness under various image transformations and adversarial manipulations, yet they largely overlook the efficiency challenges of watermark detection across large-scale image collections. To address this gap, we propose QRMark, an efficient and adaptive end-to-end method for detecting embedded image watermarks. The core idea of QRMark is to combine QR Code inspired error correction with tailored tiling techniques to improve detection efficiency while preserving accuracy and robustness. At the algorithmic level, QRMark employs a Reed-Solomon error correction mechanism to mitigate the accuracy degradation introduced by tiling. At the system level, QRMark implements a resource-aware stream allocation policy that adaptively assigns more streams to GPU-intensive stages of the detection pipeline. It further employs a tile-based workload interleaving strategy to overlap data-loading overhead with computation and schedules kernels across stages to maximize efficiency. End-to-end evaluations show that QRMark achieves an average 2.43x inference speedup over the sequential baseline.
<div id='section'>Paperid: <span id='pid'>338, <a href='https://arxiv.org/pdf/2410.20202.pdf' target='_blank'>https://arxiv.org/pdf/2410.20202.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongdong Lin, Yue Li, Benedetta Tondi, Bin Li, Mauro Barni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20202">An Efficient Watermarking Method for Latent Diffusion Models via Low-Rank Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid proliferation of deep neural networks (DNNs) is driving a surge in model watermarking technologies, as the trained deep models themselves serve as intellectual properties. The core of existing model watermarking techniques involves modifying or tuning the models' weights. However, with the emergence of increasingly complex models, ensuring the efficiency of watermarking process is essential to manage the growing computational demands. Prioritizing efficiency not only optimizes resource utilization, making the watermarking process more applicable, but also minimizes potential impacts on model performance. In this letter, we propose an efficient watermarking method for latent diffusion models (LDMs) which is based on Low-Rank Adaptation (LoRA). We specifically choose to add trainable low-rank matrices to the existing weight matrices of the models to embed watermark, while keeping the original weights frozen. Moreover, we also propose a dynamic loss weight tuning algorithm to balance the generative task with the watermark embedding task, ensuring that the model can be watermarked with a limited impact on the quality of the generated images. Experimental results show that the proposed method ensures fast watermark embedding and maintains a very low bit error rate of the watermark, a high-quality of the generated image, and a zero false negative rate (FNR) for verification.
<div id='section'>Paperid: <span id='pid'>339, <a href='https://arxiv.org/pdf/2409.14700.pdf' target='_blank'>https://arxiv.org/pdf/2409.14700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dung Daniel Ngo, Daniel Scott, Saheed Obitayo, Archan Ray, Akshay Seshadri, Niraj Kumar, Vamsi K. Potluru, Marco Pistoia, Manuela Veloso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14700">Adaptive and Robust Watermark for Generative Tabular Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent development in generative models has demonstrated its ability to create high-quality synthetic data. However, the pervasiveness of synthetic content online also brings forth growing concerns that it can be used for malicious purpose. To ensure the authenticity of the data, watermarking techniques have recently emerged as a promising solution due to their strong statistical guarantees. In this paper, we propose a flexible and robust watermarking mechanism for generative tabular data. Specifically, a data provider with knowledge of the downstream tasks can partition the feature space into pairs of (key, value) columns. Within each pair, the data provider first uses elements in the key column to generate a randomized set of ``green'' intervals, then encourages elements of the value column to be in one of these ``green'' intervals. We show theoretically and empirically that the watermarked datasets (i) have negligible impact on the data quality and downstream utility, (ii) can be efficiently detected, (iii) are robust against multiple attacks commonly observed in data science, and (iv) maintain strong security against adversary attempting to learn the underlying watermark scheme.
<div id='section'>Paperid: <span id='pid'>340, <a href='https://arxiv.org/pdf/2401.08573.pdf' target='_blank'>https://arxiv.org/pdf/2401.08573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bang An, Mucong Ding, Tahseen Rabbani, Aakriti Agrawal, Yuancheng Xu, Chenghao Deng, Sicheng Zhu, Abdirisak Mohamed, Yuxin Wen, Tom Goldstein, Furong Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08573">WAVES: Benchmarking the Robustness of Image Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the burgeoning age of generative AI, watermarks act as identifiers of provenance and artificial content. We present WAVES (Watermark Analysis Via Enhanced Stress-testing), a benchmark for assessing image watermark robustness, overcoming the limitations of current evaluation methods. WAVES integrates detection and identification tasks and establishes a standardized evaluation protocol comprised of a diverse range of stress tests. The attacks in WAVES range from traditional image distortions to advanced, novel variations of diffusive, and adversarial attacks. Our evaluation examines two pivotal dimensions: the degree of image quality degradation and the efficacy of watermark detection after attacks. Our novel, comprehensive evaluation reveals previously undetected vulnerabilities of several modern watermarking algorithms. We envision WAVES as a toolkit for the future development of robust watermarks. The project is available at https://wavesbench.github.io/
<div id='section'>Paperid: <span id='pid'>341, <a href='https://arxiv.org/pdf/2311.05478.pdf' target='_blank'>https://arxiv.org/pdf/2311.05478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwei Fei, Zhihua Xia, Benedetta Tondi, Mauro Barni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.05478">Robust Retraining-free GAN Fingerprinting via Personalized Normalization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, there has been significant growth in the commercial applications of generative models, licensed and distributed by model developers to users, who in turn use them to offer services. In this scenario, there is a need to track and identify the responsible user in the presence of a violation of the license agreement or any kind of malicious usage. Although there are methods enabling Generative Adversarial Networks (GANs) to include invisible watermarks in the images they produce, generating a model with a different watermark, referred to as a fingerprint, for each user is time- and resource-consuming due to the need to retrain the model to include the desired fingerprint. In this paper, we propose a retraining-free GAN fingerprinting method that allows model developers to easily generate model copies with the same functionality but different fingerprints. The generator is modified by inserting additional Personalized Normalization (PN) layers whose parameters (scaling and bias) are generated by two dedicated shallow networks (ParamGen Nets) taking the fingerprint as input. A watermark decoder is trained simultaneously to extract the fingerprint from the generated images. The proposed method can embed different fingerprints inside the GAN by just changing the input of the ParamGen Nets and performing a feedforward pass, without finetuning or retraining. The performance of the proposed method in terms of robustness against both model-level and image-level attacks is also superior to the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>342, <a href='https://arxiv.org/pdf/2310.16919.pdf' target='_blank'>https://arxiv.org/pdf/2310.16919.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianwei Fei, Zhihua Xia, Benedetta Tondi, Mauro Barni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16919">Wide Flat Minimum Watermarking for Robust Ownership Verification of GANs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a novel multi-bit box-free watermarking method for the protection of Intellectual Property Rights (IPR) of GANs with improved robustness against white-box attacks like fine-tuning, pruning, quantization, and surrogate model attacks. The watermark is embedded by adding an extra watermarking loss term during GAN training, ensuring that the images generated by the GAN contain an invisible watermark that can be retrieved by a pre-trained watermark decoder. In order to improve the robustness against white-box model-level attacks, we make sure that the model converges to a wide flat minimum of the watermarking loss term, in such a way that any modification of the model parameters does not erase the watermark. To do so, we add random noise vectors to the parameters of the generator and require that the watermarking loss term is as invariant as possible with respect to the presence of noise. This procedure forces the generator to converge to a wide flat minimum of the watermarking loss. The proposed method is architectureand dataset-agnostic, thus being applicable to many different generation tasks and models, as well as to CNN-based image processing architectures. We present the results of extensive experiments showing that the presence of the watermark has a negligible impact on the quality of the generated images, and proving the superior robustness of the watermark against model modification and surrogate model attacks.
<div id='section'>Paperid: <span id='pid'>343, <a href='https://arxiv.org/pdf/2211.13737.pdf' target='_blank'>https://arxiv.org/pdf/2211.13737.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongdong Lin, Benedetta Tondi, Bin Li, Mauro Barni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13737">CycleGANWM: A CycleGAN watermarking method for ownership verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the proliferation and widespread use of deep neural networks (DNN), their Intellectual Property Rights (IPR) protection has become increasingly important. This paper presents a novel model watermarking method for an unsupervised image-to-image translation (I2IT) networks, named CycleGAN, which leverage the image translation visual quality and watermark embedding. In this method, a watermark decoder is trained initially. Then the decoder is frozen and used to extract the watermark bits when training the CycleGAN watermarking model. The CycleGAN watermarking (CycleGANWM) is trained with specific loss functions and optimized to get a good performance on both I2IT task and watermark embedding. For watermark verification, this work uses statistical significance test to identify the ownership of the model from the extract watermark bits. We evaluate the robustness of the model against image post-processing and improve it by fine-tuning the model with adding data augmentation on the output images before extracting the watermark bits. We also carry out surrogate model attack under black-box access of the model. The experimental results prove that the proposed method is effective and robust to some image post-processing, and it is able to resist surrogate model attack.
<div id='section'>Paperid: <span id='pid'>344, <a href='https://arxiv.org/pdf/2208.10973.pdf' target='_blank'>https://arxiv.org/pdf/2208.10973.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Benedetta Tondi, Andrea Costanzo, Mauro Barni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.10973">Robust and Large-Payload DNN Watermarking via Fixed, Distribution-Optimized, Weights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The design of an effective multi-bit watermarking algorithm hinges upon finding a good trade-off between the three fundamental requirements forming the watermarking trade-off triangle, namely, robustness against network modifications, payload, and unobtrusiveness, ensuring minimal impact on the performance of the watermarked network. In this paper, we first revisit the nature of the watermarking trade-off triangle for the DNN case, then we exploit our findings to propose a white-box, multi-bit watermarking method achieving very large payload and strong robustness against network modification. In the proposed system, the weights hosting the watermark are set prior to training, making sure that their amplitude is large enough to bear the target payload and survive network modifications, notably retraining, and are left unchanged throughout the training process. The distribution of the weights carrying the watermark is theoretically optimised to ensure the secrecy of the watermark and make sure that the watermarked weights are indistinguishable from the non-watermarked ones. The proposed method can achieve outstanding performance, with no significant impact on network accuracy, including robustness against network modifications, retraining and transfer learning, while ensuring a payload which is out of reach of state of the art methods achieving a lower - or at most comparable - robustness.
<div id='section'>Paperid: <span id='pid'>345, <a href='https://arxiv.org/pdf/2503.11071.pdf' target='_blank'>https://arxiv.org/pdf/2503.11071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenguang Liu, Chao Shuai, Shaojing Fan, Ziping Dong, Jinwu Hu, Zhongjie Ba, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11071">Harnessing Frequency Spectrum Insights for Image Copyright Protection Against Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have achieved remarkable success in novel view synthesis, but their reliance on large, diverse, and often untraceable Web datasets has raised pressing concerns about image copyright protection. Current methods fall short in reliably identifying unauthorized image use, as they struggle to generalize across varied generation tasks and fail when the training dataset includes images from multiple sources with few identifiable (watermarked or poisoned) samples. In this paper, we present novel evidence that diffusion-generated images faithfully preserve the statistical properties of their training data, particularly reflected in their spectral features. Leveraging this insight, we introduce \emph{CoprGuard}, a robust frequency domain watermarking framework to safeguard against unauthorized image usage in diffusion model training and fine-tuning. CoprGuard demonstrates remarkable effectiveness against a wide range of models, from naive diffusion models to sophisticated text-to-image models, and is robust even when watermarked images comprise a mere 1\% of the training dataset. This robust and versatile approach empowers content owners to protect their intellectual property in the era of AI-driven image generation.
<div id='section'>Paperid: <span id='pid'>346, <a href='https://arxiv.org/pdf/2502.06418.pdf' target='_blank'>https://arxiv.org/pdf/2502.06418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongjie Ba, Yitao Zhang, Peng Cheng, Bin Gong, Xinyu Zhang, Qinglong Wang, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.06418">Robust Watermarks Leak: Channel-Aware Feature Extraction Enables Adversarial Watermark Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking plays a key role in the provenance and detection of AI-generated content. While existing methods prioritize robustness against real-world distortions (e.g., JPEG compression and noise addition), we reveal a fundamental tradeoff: such robust watermarks inherently improve the redundancy of detectable patterns encoded into images, creating exploitable information leakage. To leverage this, we propose an attack framework that extracts leakage of watermark patterns through multi-channel feature learning using a pre-trained vision model. Unlike prior works requiring massive data or detector access, our method achieves both forgery and detection evasion with a single watermarked image. Extensive experiments demonstrate that our method achieves a 60\% success rate gain in detection evasion and 51\% improvement in forgery accuracy compared to state-of-the-art methods while maintaining visual fidelity. Our work exposes the robustness-stealthiness paradox: current "robust" watermarks sacrifice security for distortion resistance, providing insights for future watermark design.
<div id='section'>Paperid: <span id='pid'>347, <a href='https://arxiv.org/pdf/2501.17356.pdf' target='_blank'>https://arxiv.org/pdf/2501.17356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar Petrov, Shruti Agarwal, Philip H. S. Torr, Adel Bibi, John Collomosse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.17356">On the Coexistence and Ensembling of Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking, the practice of embedding imperceptible information into media such as images, videos, audio, and text, is essential for intellectual property protection, content provenance and attribution. The growing complexity of digital ecosystems necessitates watermarks for different uses to be embedded in the same media. However, to detect and decode all watermarks, they need to coexist well with one another. We perform the first study of coexistence of deep image watermarking methods and, contrary to intuition, we find that various open-source watermarks can coexist with only minor impacts on image quality and decoding robustness. The coexistence of watermarks also opens the avenue for ensembling watermarking methods. We show how ensembling can increase the overall message capacity and enable new trade-offs between capacity, accuracy, robustness and image quality, without needing to retrain the base models.
<div id='section'>Paperid: <span id='pid'>348, <a href='https://arxiv.org/pdf/2501.02446.pdf' target='_blank'>https://arxiv.org/pdf/2501.02446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kun Wang, Kaiyan Chang, Mengdi Wang, Xinqi Zou, Haobo Xu, Yinhe Han, Ying Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02446">RTLMarker: Protecting LLM-Generated RTL Copyright via a Hardware Watermarking Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances of large language models in the field of Verilog generation have raised several ethical and security concerns, such as code copyright protection and dissemination of malicious code. Researchers have employed watermarking techniques to identify codes generated by large language models. However, the existing watermarking works fail to protect RTL code copyright due to the significant syntactic and semantic differences between RTL code and software code in languages such as Python. This paper proposes a hardware watermarking framework RTLMarker that embeds watermarks into RTL code and deeper into the synthesized netlist. We propose a set of rule-based Verilog code transformations , ensuring the watermarked RTL code's syntactic and semantic correctness. In addition, we consider an inherent tradeoff between watermark transparency and watermark effectiveness and jointly optimize them. The results demonstrate RTLMarker's superiority over the baseline in RTL code watermarking.
<div id='section'>Paperid: <span id='pid'>349, <a href='https://arxiv.org/pdf/2410.23718.pdf' target='_blank'>https://arxiv.org/pdf/2410.23718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiufeng Huang, Ruiqi Li, Yiu-ming Cheung, Ka Chun Cheung, Simon See, Renjie Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.23718">GaussianMarker: Uncertainty-Aware Copyright Protection of 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has become a crucial method for acquiring 3D assets. To protect the copyright of these assets, digital watermarking techniques can be applied to embed ownership information discreetly within 3DGS models. However, existing watermarking methods for meshes, point clouds, and implicit radiance fields cannot be directly applied to 3DGS models, as 3DGS models use explicit 3D Gaussians with distinct structures and do not rely on neural networks. Naively embedding the watermark on a pre-trained 3DGS can cause obvious distortion in rendered images. In our work, we propose an uncertainty-based method that constrains the perturbation of model parameters to achieve invisible watermarking for 3DGS. At the message decoding stage, the copyright messages can be reliably extracted from both 3D Gaussians and 2D rendered images even under various forms of 3D and 2D distortions. We conduct extensive experiments on the Blender, LLFF and MipNeRF-360 datasets to validate the effectiveness of our proposed method, demonstrating state-of-the-art performance on both message decoding accuracy and view synthesis quality.
<div id='section'>Paperid: <span id='pid'>350, <a href='https://arxiv.org/pdf/2406.09026.pdf' target='_blank'>https://arxiv.org/pdf/2406.09026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Yang, Hai Ci, Yiren Song, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.09026">Steganalysis on Digital Watermarking: Is Your Defense Truly Impervious?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital watermarking techniques are crucial for copyright protection and source identification of images, especially in the era of generative AI models. However, many existing watermarking methods, particularly content-agnostic approaches that embed fixed patterns regardless of image content, are vulnerable to steganalysis attacks that can extract and remove the watermark with minimal perceptual distortion. In this work, we categorize watermarking algorithms into content-adaptive and content-agnostic ones, and demonstrate how averaging a collection of watermarked images could reveal the underlying watermark pattern. We then leverage this extracted pattern for effective watermark removal under both graybox and blackbox settings, even when the collection contains multiple watermark patterns. For some algorithms like Tree-Ring watermarks, the extracted pattern can also forge convincing watermarks on clean images. Our quantitative and qualitative evaluations across twelve watermarking methods highlight the threat posed by steganalysis to content-agnostic watermarks and the importance of designing watermarking techniques resilient to such analytical attacks. We propose security guidelines calling for using content-adaptive watermarking strategies and performing security evaluation against steganalysis. We also suggest multi-key assignments as potential mitigations against steganalysis vulnerabilities.
<div id='section'>Paperid: <span id='pid'>351, <a href='https://arxiv.org/pdf/2406.08337.pdf' target='_blank'>https://arxiv.org/pdf/2406.08337.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hai Ci, Yiren Song, Pei Yang, Jinheng Xie, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.08337">WMAdapter: Adding WaterMark Control to Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is crucial for protecting the copyright of AI-generated images. We propose WMAdapter, a diffusion model watermark plugin that takes user-specified watermark information and allows for seamless watermark imprinting during the diffusion generation process. WMAdapter is efficient and robust, with a strong emphasis on high generation quality. To achieve this, we make two key designs: (1) We develop a contextual adapter structure that is lightweight and enables effective knowledge transfer from heavily pretrained post-hoc watermarking models. (2) We introduce an extra finetuning step and design a hybrid finetuning strategy to further improve image quality and eliminate tiny artifacts. Empirical results demonstrate that WMAdapter offers strong flexibility, exceptional image generation quality and competitive watermark robustness.
<div id='section'>Paperid: <span id='pid'>352, <a href='https://arxiv.org/pdf/2402.16889.pdf' target='_blank'>https://arxiv.org/pdf/2402.16889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Desu, Xuanli He, Qiongkai Xu, Wei Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16889">Generative Models are Self-Watermarked: Declaring Model Authentication through Re-Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As machine- and AI-generated content proliferates, protecting the intellectual property of generative models has become imperative, yet verifying data ownership poses formidable challenges, particularly in cases of unauthorized reuse of generated data. The challenge of verifying data ownership is further amplified by using Machine Learning as a Service (MLaaS), which often functions as a black-box system.
  Our work is dedicated to detecting data reuse from even an individual sample. Traditionally, watermarking has been leveraged to detect AI-generated content. However, unlike watermarking techniques that embed additional information as triggers into models or generated content, potentially compromising output quality, our approach identifies latent fingerprints inherently present within the outputs through re-generation. We propose an explainable verification procedure that attributes data ownership through re-generation, and further amplifies these fingerprints in the generative models through iterative data re-generation. This methodology is theoretically grounded and demonstrates viability and robustness using recent advanced text and image generative models. Our methodology is significant as it goes beyond protecting the intellectual property of APIs and addresses important issues such as the spread of misinformation and academic misconduct. It provides a useful tool to ensure the integrity of sources and authorship, expanding its application in different scenarios where authenticity and ownership verification are essential.
<div id='section'>Paperid: <span id='pid'>353, <a href='https://arxiv.org/pdf/2312.07930.pdf' target='_blank'>https://arxiv.org/pdf/2312.07930.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baihe Huang, Hanlin Zhu, Banghua Zhu, Kannan Ramchandran, Michael I. Jordan, Jason D. Lee, Jiantao Jiao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.07930">Towards Optimal Statistical Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-offs between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $Î(h^{-1} \log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where the user is allowed to perform a class of perturbations on the generated texts, and characterize the optimal Type II error of robust UMP tests via a linear programming problem. To the best of our knowledge, this is the first systematic statistical treatment on the watermarking problem with near-optimal rates in the i.i.d. setting, which might be of interest for future works.
<div id='section'>Paperid: <span id='pid'>354, <a href='https://arxiv.org/pdf/2508.21727.pdf' target='_blank'>https://arxiv.org/pdf/2508.21727.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiazheng Xing, Hai Ci, Hongbin Xu, Hangjie Yuan, Yong Liu, Mike Zheng Shou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21727">OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time Optimization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking diffusion-generated images is crucial for copyright protection and user tracking. However, current diffusion watermarking methods face significant limitations: zero-bit watermarking systems lack the capacity for large-scale user tracking, while multi-bit methods are highly sensitive to certain image transformations or generative attacks, resulting in a lack of comprehensive robustness. In this paper, we propose OptMark, an optimization-based approach that embeds a robust multi-bit watermark into the intermediate latents of the diffusion denoising process. OptMark strategically inserts a structural watermark early to resist generative attacks and a detail watermark late to withstand image transformations, with tailored regularization terms to preserve image quality and ensure imperceptibility. To address the challenge of memory consumption growing linearly with the number of denoising steps during optimization, OptMark incorporates adjoint gradient methods, reducing memory usage from O(N) to O(1). Experimental results demonstrate that OptMark achieves invisible multi-bit watermarking while ensuring robust resilience against valuemetric transformations, geometric transformations, editing, and regeneration attacks.
<div id='section'>Paperid: <span id='pid'>355, <a href='https://arxiv.org/pdf/2505.12296.pdf' target='_blank'>https://arxiv.org/pdf/2505.12296.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyu Deng, Yanna Jiang, Guangsheng Yu, Qin Wang, Xu Wang, Baihe Ma, Wei Ni, Ren Ping Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.12296">PoLO: Proof-of-Learning and Proof-of-Ownership at Once with Chained Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models are increasingly shared and outsourced, raising requirements of verifying training effort (Proof-of-Learning, PoL) to ensure claimed performance and establishing ownership (Proof-of-Ownership, PoO) for transactions. When models are trained by untrusted parties, PoL and PoO must be enforced together to enable protection, attribution, and compensation. However, existing studies typically address them separately, which not only weakens protection against forgery and privacy breaches but also leads to high verification overhead.
  We propose PoLO, a unified framework that simultaneously achieves PoL and PoO using chained watermarks. PoLO splits the training process into fine-grained training shards and embeds a dedicated watermark in each shard. Each watermark is generated using the hash of the preceding shard, certifying the training process of the preceding shard. The chained structure makes it computationally difficult to forge any individual part of the whole training process. The complete set of watermarks serves as the PoL, while the final watermark provides the PoO. PoLO offers more efficient and privacy-preserving verification compared to the vanilla PoL solutions that rely on gradient-based trajectory tracing and inadvertently expose training data during verification, while maintaining the same level of ownership assurance of watermark-based PoO schemes. Our evaluation shows that PoLO achieves 99% watermark detection accuracy for ownership verification, while preserving data privacy and cutting verification costs to just 1.5-10% of traditional methods. Forging PoLO demands 1.1-4x more resources than honest proof generation, with the original proof retaining over 90% detection accuracy even after attacks.
<div id='section'>Paperid: <span id='pid'>356, <a href='https://arxiv.org/pdf/2404.05607.pdf' target='_blank'>https://arxiv.org/pdf/2404.05607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guokai Zhang, Lanjun Wang, Yuting Su, An-An Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.05607">A Training-Free Plug-and-Play Watermark Framework for Stable Diffusion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, the family of Stable Diffusion (SD) models has gained prominence for its high quality outputs and scalability. This has also raised security concerns on social media, as malicious users can create and disseminate harmful content. Existing approaches involve training components or entire SDs to embed a watermark in generated images for traceability and responsibility attribution. However, in the era of AI-generated content (AIGC), the rapid iteration of SDs renders retraining with watermark models costly. To address this, we propose a training-free plug-and-play watermark framework for SDs. Without modifying any components of SDs, we embed diverse watermarks in the latent space, adapting to the denoising process. Our experimental findings reveal that our method effectively harmonizes image quality and watermark invisibility. Furthermore, it performs robustly under various attacks. We also have validated that our method is generalized to multiple versions of SDs, even without retraining the watermark model.
<div id='section'>Paperid: <span id='pid'>357, <a href='https://arxiv.org/pdf/2312.05738.pdf' target='_blank'>https://arxiv.org/pdf/2312.05738.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlong Mao, Huiyi Tang, Yi Zhang, Fengxia Liu, Zhiyong Zheng, Shanxiang Lyu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.05738">FedReverse: Multiparty Reversible Deep Neural Network Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of Deep Neural Networks (DNN) in commercial applications is expanding rapidly. Simultaneously, the increasing complexity and cost of training DNN models have intensified the urgency surrounding the protection of intellectual property associated with these trained models. In this regard, DNN watermarking has emerged as a crucial safeguarding technique. This paper presents FedReverse, a novel multiparty reversible watermarking approach for robust copyright protection while minimizing performance impact. Unlike existing methods, FedReverse enables collaborative watermark embedding from multiple parties after model training, ensuring individual copyright claims. In addition, FedReverse is reversible, enabling complete watermark removal with unanimous client consent. FedReverse demonstrates perfect covering, ensuring that observations of watermarked content do not reveal any information about the hidden watermark. Additionally, it showcases resistance against Known Original Attacks (KOA), making it highly challenging for attackers to forge watermarks or infer the key. This paper further evaluates FedReverse through comprehensive simulations involving Multi-layer Perceptron (MLP) and Convolutional Neural Networks (CNN) trained on the MNIST dataset. The simulations demonstrate FedReverse's robustness, reversibility, and minimal impact on model accuracy across varying embedding parameters and multiple client scenarios.
<div id='section'>Paperid: <span id='pid'>358, <a href='https://arxiv.org/pdf/2309.03815.pdf' target='_blank'>https://arxiv.org/pdf/2309.03815.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>An-An Liu, Guokai Zhang, Yuting Su, Ning Xu, Yongdong Zhang, Lanjun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.03815">T2IW: Joint Text to Image & Watermark Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent developments in text-conditioned image generative models have revolutionized the production of realistic results. Unfortunately, this has also led to an increase in privacy violations and the spread of false information, which requires the need for traceability, privacy protection, and other security measures. However, existing text-to-image paradigms lack the technical capabilities to link traceable messages with image generation. In this study, we introduce a novel task for the joint generation of text to image and watermark (T2IW). This T2IW scheme ensures minimal damage to image quality when generating a compound image by forcing the semantic feature and the watermark signal to be compatible in pixels. Additionally, by utilizing principles from Shannon information theory and non-cooperative game theory, we are able to separate the revealed image and the revealed watermark from the compound image. Furthermore, we strengthen the watermark robustness of our approach by subjecting the compound image to various post-processing attacks, with minimal pixel distortion observed in the revealed watermark. Extensive experiments have demonstrated remarkable achievements in image quality, watermark invisibility, and watermark robustness, supported by our proposed set of evaluation metrics.
<div id='section'>Paperid: <span id='pid'>359, <a href='https://arxiv.org/pdf/2511.13329.pdf' target='_blank'>https://arxiv.org/pdf/2511.13329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shufan Yang, Zifeng Cheng, Zhiwei Jiang, Yafeng Yin, Cong Wang, Shiping Ge, Yuchen Fu, Qing Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13329">RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.
<div id='section'>Paperid: <span id='pid'>360, <a href='https://arxiv.org/pdf/2502.02068.pdf' target='_blank'>https://arxiv.org/pdf/2502.02068.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruisi Zhang, Neusha Javidnia, Nojan Sheybani, Farinaz Koushanfar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02068">Robust and Secure Code Watermarking for Large Language Models via ML/Crypto Codesign</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces RoSeMary, the first-of-its-kind ML/Crypto codesign watermarking framework that regulates LLM-generated code to avoid intellectual property rights violations and inappropriate misuse in software development. High-quality watermarks adhering to the detectability-fidelity-robustness tri-objective are limited due to codes' low-entropy nature. Watermark verification, however, often needs to reveal the signature and requires re-encoding new ones for code reuse, which potentially compromising the system's usability. To overcome these challenges, RoSeMary obtains high-quality watermarks by training the watermark insertion and extraction modules end-to-end to ensure (i) unaltered watermarked code functionality and (ii) enhanced detectability and robustness leveraging pre-trained CodeT5 as the insertion backbone to enlarge the code syntactic and variable rename transformation search space. In the deployment, RoSeMary uses zero-knowledge proofs for secure verification without revealing the underlying signatures. Extensive evaluations demonstrated RoSeMary achieves high detection accuracy while preserving the code functionality. RoSeMary is also robust against attacks and provides efficient secure watermark verification.
<div id='section'>Paperid: <span id='pid'>361, <a href='https://arxiv.org/pdf/2402.04435.pdf' target='_blank'>https://arxiv.org/pdf/2402.04435.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enyan Dai, Minhua Lin, Suhang Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.04435">PreGIP: Watermarking the Pretraining of Graph Neural Networks for Deep Intellectual Property Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pretraining on Graph Neural Networks (GNNs) has shown great power in facilitating various downstream tasks. As pretraining generally requires huge amount of data and computational resources, the pretrained GNNs are high-value Intellectual Properties (IP) of the legitimate owner. However, adversaries may illegally copy and deploy the pretrained GNN models for their downstream tasks. Though initial efforts have been made to watermark GNN classifiers for IP protection, these methods require the target classification task for watermarking, and thus are not applicable to self-supervised pretraining of GNN models. Hence, in this work, we propose a novel framework named PreGIP to watermark the pretraining of GNN encoder for IP protection while maintain the high-quality of the embedding space. PreGIP incorporates a task-free watermarking loss to watermark the embedding space of pretrained GNN encoder. A finetuning-resistant watermark injection is further deployed. Theoretical analysis and extensive experiments show the effectiveness of {\method} in IP protection and maintaining high-performance for downstream tasks.
<div id='section'>Paperid: <span id='pid'>362, <a href='https://arxiv.org/pdf/2310.00646.pdf' target='_blank'>https://arxiv.org/pdf/2310.00646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jingtan Wang, Xinyang Lu, Zitong Zhao, Zhongxiang Dai, Chuan-Sheng Foo, See-Kiong Ng, Bryan Kian Hsiang Low
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00646">Source Attribution for Large Language Model-Generated Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The impressive performances of Large Language Models (LLMs) and their immense potential for commercialization have given rise to serious concerns over the Intellectual Property (IP) of their training data. In particular, the synthetic texts generated by LLMs may infringe the IP of the data being used to train the LLMs. To this end, it is imperative to be able to perform source attribution by identifying the data provider who contributed to the generation of a synthetic text by an LLM. In this paper, we show that this problem can be tackled by watermarking, i.e., by enabling an LLM to generate synthetic texts with embedded watermarks that contain information about their source(s). We identify the key properties of such watermarking frameworks (e.g., source attribution accuracy, robustness against adversaries), and propose a source attribution framework that satisfies these key properties due to our algorithmic designs. Our framework enables an LLM to learn an accurate mapping from the generated texts to data providers, which sets the foundation for effective source attribution. Extensive empirical evaluations show that our framework achieves effective source attribution.
<div id='section'>Paperid: <span id='pid'>363, <a href='https://arxiv.org/pdf/2310.00076.pdf' target='_blank'>https://arxiv.org/pdf/2310.00076.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar, Atoosa Chegini, Wenxiao Wang, Soheil Feizi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00076">Robustness of AI-Image Detectors: Fundamental Limits and Practical Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In light of recent advancements in generative AI models, it has become essential to distinguish genuine content from AI-generated one to prevent the malicious usage of fake materials as authentic ones and vice versa. Various techniques have been introduced for identifying AI-generated images, with watermarking emerging as a promising approach. In this paper, we analyze the robustness of various AI-image detectors including watermarking and classifier-based deepfake detectors. For watermarking methods that introduce subtle image perturbations (i.e., low perturbation budget methods), we reveal a fundamental trade-off between the evasion error rate (i.e., the fraction of watermarked images detected as non-watermarked ones) and the spoofing error rate (i.e., the fraction of non-watermarked images detected as watermarked ones) upon an application of diffusion purification attack. To validate our theoretical findings, we also provide empirical evidence demonstrating that diffusion purification effectively removes low perturbation budget watermarks by applying minimal changes to images. The diffusion purification attack is ineffective for high perturbation watermarking methods where notable changes are applied to images. In this case, we develop a model substitution adversarial attack that can successfully remove watermarks. Moreover, we show that watermarking methods are vulnerable to spoofing attacks where the attacker aims to have real images identified as watermarked ones, damaging the reputation of the developers. In particular, with black-box access to the watermarking method, a watermarked noise image can be generated and added to real images, causing them to be incorrectly classified as watermarked. Finally, we extend our theory to characterize a fundamental trade-off between the robustness and reliability of classifier-based deep fake detectors and demonstrate it through experiments.
<div id='section'>Paperid: <span id='pid'>364, <a href='https://arxiv.org/pdf/2309.06779.pdf' target='_blank'>https://arxiv.org/pdf/2309.06779.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nojan Sheybani, Zahra Ghodsi, Ritvik Kapila, Farinaz Koushanfar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.06779">ZKROWNN: Zero Knowledge Right of Ownership for Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training contemporary AI models requires investment in procuring learning data and computing resources, making the models intellectual property of the owners. Popular model watermarking solutions rely on key input triggers for detection; the keys have to be kept private to prevent discovery, forging, and removal of the hidden signatures. We present ZKROWNN, the first automated end-to-end framework utilizing Zero-Knowledge Proofs (ZKP) that enable an entity to validate their ownership of a model, while preserving the privacy of the watermarks. ZKROWNN permits a third party client to verify model ownership in less than a second, requiring as little as a few KBs of communication.
<div id='section'>Paperid: <span id='pid'>365, <a href='https://arxiv.org/pdf/2309.01786.pdf' target='_blank'>https://arxiv.org/pdf/2309.01786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyang Yu, Junyuan Hong, Haobo Zhang, Haotao Wang, Zhangyang Wang, Jiayu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.01786">Safe and Robust Watermark Injection with a Single OoD Image</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training a high-performance deep neural network requires large amounts of data and computational resources. Protecting the intellectual property (IP) and commercial ownership of a deep model is challenging yet increasingly crucial. A major stream of watermarking strategies implants verifiable backdoor triggers by poisoning training samples, but these are often unrealistic due to data privacy and safety concerns and are vulnerable to minor model changes such as fine-tuning. To overcome these challenges, we propose a safe and robust backdoor-based watermark injection technique that leverages the diverse knowledge from a single out-of-distribution (OoD) image, which serves as a secret key for IP verification. The independence of training data makes it agnostic to third-party promises of IP security. We induce robustness via random perturbation of model parameters during watermark injection to defend against common watermark removal attacks, including fine-tuning, pruning, and model extraction. Our experimental results demonstrate that the proposed watermarking approach is not only time- and sample-efficient without training data, but also robust against the watermark removal attacks above.
<div id='section'>Paperid: <span id='pid'>366, <a href='https://arxiv.org/pdf/2304.14613.pdf' target='_blank'>https://arxiv.org/pdf/2304.14613.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Sun, Tianpeng Liu, Panhe Hu, Qing Liao, Shaojing Fu, Nenghai Yu, Deke Guo, Yongxiang Liu, Li Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.14613">Deep Intellectual Property Protection: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Neural Networks (DNNs), from AlexNet to ResNet to ChatGPT, have made revolutionary progress in recent years, and are widely used in various fields. The high performance of DNNs requires a huge amount of high-quality data, expensive computing hardware, and excellent DNN architectures that are costly to obtain. Therefore, trained DNNs are becoming valuable assets and must be considered the Intellectual Property (IP) of the legitimate owner who created them, in order to protect trained DNN models from illegal reproduction, stealing, redistribution, or abuse. Although being a new emerging and interdisciplinary field, numerous DNN model IP protection methods have been proposed. Given this period of rapid evolution, the goal of this paper is to provide a comprehensive survey of two mainstream DNN IP protection methods: deep watermarking and deep fingerprinting, with a proposed taxonomy. More than 190 research contributions are included in this survey, covering many aspects of Deep IP Protection: problem definition, main threats and challenges, merits and demerits of deep watermarking and deep fingerprinting methods, evaluation metrics, and performance discussion. We finish the survey by identifying promising directions for future research.
<div id='section'>Paperid: <span id='pid'>367, <a href='https://arxiv.org/pdf/2303.11470.pdf' target='_blank'>https://arxiv.org/pdf/2303.11470.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruixiang Tang, Qizhang Feng, Ninghao Liu, Fan Yang, Xia Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11470">Did You Train on My Dataset? Towards Public Dataset Protection with Clean-Label Backdoor Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The huge supporting training data on the Internet has been a key factor in the success of deep learning models. However, this abundance of public-available data also raises concerns about the unauthorized exploitation of datasets for commercial purposes, which is forbidden by dataset licenses. In this paper, we propose a backdoor-based watermarking approach that serves as a general framework for safeguarding public-available data. By inserting a small number of watermarking samples into the dataset, our approach enables the learning model to implicitly learn a secret function set by defenders. This hidden function can then be used as a watermark to track down third-party models that use the dataset illegally. Unfortunately, existing backdoor insertion methods often entail adding arbitrary and mislabeled data to the training set, leading to a significant drop in performance and easy detection by anomaly detection algorithms. To overcome this challenge, we introduce a clean-label backdoor watermarking framework that uses imperceptible perturbations to replace mislabeled samples. As a result, the watermarking samples remain consistent with the original labels, making them difficult to detect. Our experiments on text, image, and audio datasets demonstrate that the proposed framework effectively safeguards datasets with minimal impact on original task performance. We also show that adding just 1% of watermarking samples can inject a traceable watermarking function and that our watermarking samples are stealthy and look benign upon visual inspection.
<div id='section'>Paperid: <span id='pid'>368, <a href='https://arxiv.org/pdf/2107.09287.pdf' target='_blank'>https://arxiv.org/pdf/2107.09287.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihan Wang, Olivia Byrnes, Hu Wang, Ruoxi Sun, Congbo Ma, Huaming Chen, Qi Wu, Minhui Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2107.09287">Data Hiding with Deep Learning: A Survey Unifying Digital Watermarking and Steganography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of secure communication and identity verification fields has significantly increased through the use of deep learning techniques for data hiding. By embedding information into a noise-tolerant signal such as audio, video, or images, digital watermarking and steganography techniques can be used to protect sensitive intellectual property and enable confidential communication, ensuring that the information embedded is only accessible to authorized parties. This survey provides an overview of recent developments in deep learning techniques deployed for data hiding, categorized systematically according to model architectures and noise injection methods. The objective functions, evaluation metrics, and datasets used for training these data hiding models are comprehensively summarised. Additionally, potential future research directions that unite digital watermarking and steganography on software engineering to enhance security and mitigate risks are suggested and deliberated. This contribution furthers the creation of a more trustworthy digital world and advances Responsible AI.
<div id='section'>Paperid: <span id='pid'>369, <a href='https://arxiv.org/pdf/2011.08960.pdf' target='_blank'>https://arxiv.org/pdf/2011.08960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruixiang Tang, Mengnan Du, Xia Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2011.08960">Deep Serial Number: Computational Watermarking for DNN Intellectual Property Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we present DSN (Deep Serial Number), a simple yet effective watermarking algorithm designed specifically for deep neural networks (DNNs). Unlike traditional methods that incorporate identification signals into DNNs, our approach explores a novel Intellectual Property (IP) protection mechanism for DNNs, effectively thwarting adversaries from using stolen networks. Inspired by the success of serial numbers in safeguarding conventional software IP, we propose the first implementation of serial number embedding within DNNs. To achieve this, DSN is integrated into a knowledge distillation framework, in which a private teacher DNN is initially trained. Subsequently, its knowledge is distilled and imparted to a series of customized student DNNs. Each customer DNN functions correctly only upon input of a valid serial number. Experimental results across various applications demonstrate DSN's efficacy in preventing unauthorized usage without compromising the original DNN performance. The experiments further show that DSN is resistant to different categories of watermark attacks.
<div id='section'>Paperid: <span id='pid'>370, <a href='https://arxiv.org/pdf/2510.01637.pdf' target='_blank'>https://arxiv.org/pdf/2510.01637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liyan Xie, Muhammad Siddeek, Mohamed Seif, Andrea J. Goldsmith, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01637">Detecting Post-generation Edits to Watermarked LLM Outputs via Combinatorial Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has become a key technique for proprietary language models, enabling the distinction between AI-generated and human-written text. However, in many real-world scenarios, LLM-generated content may undergo post-generation edits, such as human revisions or even spoofing attacks, making it critical to detect and localize such modifications. In this work, we introduce a new task: detecting post-generation edits locally made to watermarked LLM outputs. To this end, we propose a combinatorial pattern-based watermarking framework, which partitions the vocabulary into disjoint subsets and embeds the watermark by enforcing a deterministic combinatorial pattern over these subsets during generation. We accompany the combinatorial watermark with a global statistic that can be used to detect the watermark. Furthermore, we design lightweight local statistics to flag and localize potential edits. We introduce two task-specific evaluation metrics, Type-I error rate and detection accuracy, and evaluate our method on open-source LLMs across a variety of editing scenarios, demonstrating strong empirical performance in edit localization.
<div id='section'>Paperid: <span id='pid'>371, <a href='https://arxiv.org/pdf/2509.09112.pdf' target='_blank'>https://arxiv.org/pdf/2509.09112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxi Zhang, Xiaomei Zhang, Yanjun Zhang, He Zhang, Shirui Pan, Bo Liu, Asif Qumer Gill, Leo Yu Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.09112">Character-Level Perturbations Disrupt LLM Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Model (LLM) watermarking embeds detectable signals into generated text for copyright protection, misuse prevention, and content detection. While prior studies evaluate robustness using watermark removal attacks, these methods are often suboptimal, creating the misconception that effective removal requires large perturbations or powerful adversaries. To bridge the gap, we first formalize the system model for LLM watermark, and characterize two realistic threat models constrained on limited access to the watermark detector. We then analyze how different types of perturbation vary in their attack range, i.e., the number of tokens they can affect with a single edit. We observe that character-level perturbations (e.g., typos, swaps, deletions, homoglyphs) can influence multiple tokens simultaneously by disrupting the tokenization process. We demonstrate that character-level perturbations are significantly more effective for watermark removal under the most restrictive threat model. We further propose guided removal attacks based on the Genetic Algorithm (GA) that uses a reference detector for optimization. Under a practical threat model with limited black-box queries to the watermark detector, our method demonstrates strong removal performance. Experiments confirm the superiority of character-level perturbations and the effectiveness of the GA in removing watermarks under realistic constraints. Additionally, we argue there is an adversarial dilemma when considering potential defenses: any fixed defense can be bypassed by a suitable perturbation strategy. Motivated by this principle, we propose an adaptive compound character-level attack. Experimental results show that this approach can effectively defeat the defenses. Our findings highlight significant vulnerabilities in existing LLM watermark schemes and underline the urgency for the development of new robust mechanisms.
<div id='section'>Paperid: <span id='pid'>372, <a href='https://arxiv.org/pdf/2506.21602.pdf' target='_blank'>https://arxiv.org/pdf/2506.21602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Feng, He Zhang, Yanjun Zhang, Leo Yu Zhang, Shirui Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21602">BiMark: Unbiased Multilayer Watermarking for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have raised urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms. Although watermarking offers a promising solution, existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation. To achieve these goals, the key challenge lies in balancing the trade-off between text quality preservation and message embedding capacity. To address this challenge, we propose BiMark, a novel watermarking framework that achieves these requirements through three key innovations: (1) a bit-flip unbiased reweighting mechanism enabling model-agnostic detection, (2) a multilayer architecture enhancing detectability without compromising generation quality, and (3) an information encoding approach supporting multi-bit watermarking. Through theoretical analysis and extensive experiments, we validate that, compared to state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.
<div id='section'>Paperid: <span id='pid'>373, <a href='https://arxiv.org/pdf/2503.13458.pdf' target='_blank'>https://arxiv.org/pdf/2503.13458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Noever, Forrest McKee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13458">Dueling QR Codes: The Hyding of Dr. Jeckyl</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The paper presents a novel technique for encoding dual messages within standard Quick Response (QR) codes through precise half-pixel module splitting. This work challenges fundamental assumptions about deterministic decoding in the ISO/IEC 18004:2015 standard while maintaining complete compatibility with existing QR infrastructure. The proposed two-dimensional barcode attack enables angle-dependent message selection while maintaining compatibility with unmodified QR readers and the 100 million US mobile users who use their phone's built-in scanners. Unlike previous approaches that rely on nested codes, watermarking, or error correction exploitation, our method achieves true one-to-many mapping by manipulating the physical sampling process built into the QR standard. By preserving critical function patterns while bifurcating data modules, we create automated codes that produce different but valid readings based on camera viewing angle. Experimental results demonstrate successful implementation across multiple use cases, including simple message text pairs, complex URLs (nsa.gov/nasa.gov), and security test patterns for malware and spam detectors (EICAR/GTUBE). Our technique achieves reliable dual-message decoding using standard QR readers at module scales of 9-11 pixels, with successful angle-dependent reading demonstrated across vertical, horizontal, and diagonal orientations. The method's success suggests potential applications beyond QR code phishing ('quishing') including two-factor authentication, anti-counterfeiting, and information density optimization. The half-pixel technique may offer future avenues for similar implementations in other 2D barcode formats such as Data Matrix and Aztec Code.
<div id='section'>Paperid: <span id='pid'>374, <a href='https://arxiv.org/pdf/2407.01260.pdf' target='_blank'>https://arxiv.org/pdf/2407.01260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alsharif Abuadbba, Nicholas Rhodes, Kristen Moore, Bushra Sabir, Shuo Wang, Yansong Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01260">DeepiSign-G: Generic Watermark to Stamp Hidden DNN Parameters for Self-contained Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning solutions in critical domains like autonomous vehicles, facial recognition, and sentiment analysis require caution due to the severe consequences of errors. Research shows these models are vulnerable to adversarial attacks, such as data poisoning and neural trojaning, which can covertly manipulate model behavior, compromising reliability and safety. Current defense strategies like watermarking have limitations: they fail to detect all model modifications and primarily focus on attacks on CNNs in the image domain, neglecting other critical architectures like RNNs.
  To address these gaps, we introduce DeepiSign-G, a versatile watermarking approach designed for comprehensive verification of leading DNN architectures, including CNNs and RNNs. DeepiSign-G enhances model security by embedding an invisible watermark within the Walsh-Hadamard transform coefficients of the model's parameters. This watermark is highly sensitive and fragile, ensuring prompt detection of any modifications. Unlike traditional hashing techniques, DeepiSign-G allows substantial metadata incorporation directly within the model, enabling detailed, self-contained tracking and verification.
  We demonstrate DeepiSign-G's applicability across various architectures, including CNN models (VGG, ResNets, DenseNet) and RNNs (Text sentiment classifier). We experiment with four popular datasets: VGG Face, CIFAR10, GTSRB Traffic Sign, and Large Movie Review. We also evaluate DeepiSign-G under five potential attacks. Our comprehensive evaluation confirms that DeepiSign-G effectively detects these attacks without compromising CNN and RNN model performance, highlighting its efficacy as a robust security measure for deep learning applications. Detection of integrity breaches is nearly perfect, while hiding only a bit in approximately 1% of the Walsh-Hadamard coefficients.
<div id='section'>Paperid: <span id='pid'>375, <a href='https://arxiv.org/pdf/2401.15817.pdf' target='_blank'>https://arxiv.org/pdf/2401.15817.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Forrest McKee, David Noever
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15817">Transparency Attacks: How Imperceptible Image Layers Can Fool AI Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper investigates a novel algorithmic vulnerability when imperceptible image layers confound multiple vision models into arbitrary label assignments and captions. We explore image preprocessing methods to introduce stealth transparency, which triggers AI misinterpretation of what the human eye perceives. The research compiles a broad attack surface to investigate the consequences ranging from traditional watermarking, steganography, and background-foreground miscues. We demonstrate dataset poisoning using the attack to mislabel a collection of grayscale landscapes and logos using either a single attack layer or randomly selected poisoning classes. For example, a military tank to the human eye is a mislabeled bridge to object classifiers based on convolutional networks (YOLO, etc.) and vision transformers (ViT, GPT-Vision, etc.). A notable attack limitation stems from its dependency on the background (hidden) layer in grayscale as a rough match to the transparent foreground image that the human eye perceives. This dependency limits the practical success rate without manual tuning and exposes the hidden layers when placed on the opposite display theme (e.g., light background, light transparent foreground visible, works best against a light theme image viewer or browser). The stealth transparency confounds established vision systems, including evading facial recognition and surveillance, digital watermarking, content filtering, dataset curating, automotive and drone autonomy, forensic evidence tampering, and retail product misclassifying. This method stands in contrast to traditional adversarial attacks that typically focus on modifying pixel values in ways that are either slightly perceptible or entirely imperceptible for both humans and machines.
<div id='section'>Paperid: <span id='pid'>376, <a href='https://arxiv.org/pdf/2401.06829.pdf' target='_blank'>https://arxiv.org/pdf/2401.06829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Folco Bertini Baldassini, Huy H. Nguyen, Ching-Chung Chang, Isao Echizen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.06829">Cross-Attention Watermarking of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A new approach to linguistic watermarking of language models is presented in which information is imperceptibly inserted into the output text while preserving its readability and original meaning. A cross-attention mechanism is used to embed watermarks in the text during inference. Two methods using cross-attention are presented that minimize the effect of watermarking on the performance of a pretrained model. Exploration of different training strategies for optimizing the watermarking and of the challenges and implications of applying this approach in real-world scenarios clarified the tradeoff between watermark robustness and text quality. Watermark selection substantially affects the generated output for high entropy sentences. This proactive watermarking approach has potential application in future model development.
<div id='section'>Paperid: <span id='pid'>377, <a href='https://arxiv.org/pdf/2211.13535.pdf' target='_blank'>https://arxiv.org/pdf/2211.13535.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seonhye Park, Alsharif Abuadbba, Shuo Wang, Kristen Moore, Yansong Gao, Hyoungshick Kim, Surya Nepal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.13535">DeepTaster: Adversarial Perturbation-Based Fingerprinting to Identify Proprietary Dataset Use in Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training deep neural networks (DNNs) requires large datasets and powerful computing resources, which has led some owners to restrict redistribution without permission. Watermarking techniques that embed confidential data into DNNs have been used to protect ownership, but these can degrade model performance and are vulnerable to watermark removal attacks. Recently, DeepJudge was introduced as an alternative approach to measuring the similarity between a suspect and a victim model. While DeepJudge shows promise in addressing the shortcomings of watermarking, it primarily addresses situations where the suspect model copies the victim's architecture. In this study, we introduce DeepTaster, a novel DNN fingerprinting technique, to address scenarios where a victim's data is unlawfully used to build a suspect model. DeepTaster can effectively identify such DNN model theft attacks, even when the suspect model's architecture deviates from the victim's. To accomplish this, DeepTaster generates adversarial images with perturbations, transforms them into the Fourier frequency domain, and uses these transformed images to identify the dataset used in a suspect model. The underlying premise is that adversarial images can capture the unique characteristics of DNNs built with a specific dataset. To demonstrate the effectiveness of DeepTaster, we evaluated the effectiveness of DeepTaster by assessing its detection accuracy on three datasets (CIFAR10, MNIST, and Tiny-ImageNet) across three model architectures (ResNet18, VGG16, and DenseNet161). We conducted experiments under various attack scenarios, including transfer learning, pruning, fine-tuning, and data augmentation. Specifically, in the Multi-Architecture Attack scenario, DeepTaster was able to identify all the stolen cases across all datasets, while DeepJudge failed to detect any of the cases.
<div id='section'>Paperid: <span id='pid'>378, <a href='https://arxiv.org/pdf/2412.12563.pdf' target='_blank'>https://arxiv.org/pdf/2412.12563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vaden Masrani, Mohammad Akbari, David Ming Xuan Yue, Ahmad Rezaei, Yong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12563">Task-Agnostic Language Model Watermarking via High Entropy Passthrough Layers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of costly pre-training of large language models, ensuring the intellectual property rights of model owners, and insuring that said models are responsibly deployed, is becoming increasingly important. To this end, we propose model watermarking via passthrough layers, which are added to existing pre-trained networks and trained using a self-supervised loss such that the model produces high-entropy output when prompted with a unique private key, and acts normally otherwise. Unlike existing model watermarking methods, our method is fully task-agnostic, and can be applied to both classification and sequence-to-sequence tasks without requiring advanced access to downstream fine-tuning datasets. We evaluate the proposed passthrough layers on a wide range of downstream tasks, and show experimentally our watermarking method achieves a near-perfect watermark extraction accuracy and false-positive rate in most cases without damaging original model performance. Additionally, we show our method is robust to both downstream fine-tuning, fine-pruning, and layer removal attacks, and can be trained in a fraction of the time required to train the original model. Code is available in the paper.
<div id='section'>Paperid: <span id='pid'>379, <a href='https://arxiv.org/pdf/2410.20354.pdf' target='_blank'>https://arxiv.org/pdf/2410.20354.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zaixi Zhang, Ruofan Jin, Kaidi Fu, Le Cong, Marinka Zitnik, Mengdi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20354">FoldMark: Protecting Protein Generative Models with Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protein structure is key to understanding protein function and is essential for progress in bioengineering, drug discovery, and molecular biology. Recently, with the incorporation of generative AI, the power and accuracy of computational protein structure prediction/design have been improved significantly. However, ethical concerns such as copyright protection and harmful content generation (biosecurity) pose challenges to the wide implementation of protein generative models. Here, we investigate whether it is possible to embed watermarks into protein generative models and their outputs for copyright authentication and the tracking of generated structures. As a proof of concept, we propose a two-stage method FoldMark as a generalized watermarking strategy for protein generative models. FoldMark first pretrain watermark encoder and decoder, which can minorly adjust protein structures to embed user-specific information and faithfully recover the information from the encoded structure. In the second step, protein generative models are fine-tuned with watermark-conditioned Low-Rank Adaptation (LoRA) modules to preserve generation quality while learning to generate watermarked structures with high recovery rates. Extensive experiments are conducted on open-source protein structure prediction models (e.g., ESMFold and MultiFlow) and de novo structure design models (e.g., FrameDiff and FoldFlow) and we demonstrate that our method is effective across all these generative models. Meanwhile, our watermarking framework only exerts a negligible impact on the original protein structure quality and is robust under potential post-processing and adaptive attacks.
<div id='section'>Paperid: <span id='pid'>380, <a href='https://arxiv.org/pdf/2409.00314.pdf' target='_blank'>https://arxiv.org/pdf/2409.00314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gursimran Singh, Tianxi Hu, Mohammad Akbari, Qiang Tang, Yong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.00314">Towards Secure and Usable 3D Assets: A Novel Framework for Automatic Visible Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D models, particularly AI-generated ones, have witnessed a recent surge across various industries such as entertainment. Hence, there is an alarming need to protect the intellectual property and avoid the misuse of these valuable assets. As a viable solution to address these concerns, we rigorously define the novel task of automated 3D visible watermarking in terms of two competing aspects: watermark quality and asset utility. Moreover, we propose a method of embedding visible watermarks that automatically determines the right location, orientation, and number of watermarks to be placed on arbitrary 3D assets for high watermark quality and asset utility. Our method is based on a novel rigid-body optimization that uses back-propagation to automatically learn transforms for ideal watermark placement. In addition, we propose a novel curvature-matching method for fusing the watermark into the 3D model that further improves readability and security. Finally, we provide a detailed experimental analysis on two benchmark 3D datasets validating the superior performance of our approach in comparison to baselines. Code and demo are available.
<div id='section'>Paperid: <span id='pid'>381, <a href='https://arxiv.org/pdf/2408.05868.pdf' target='_blank'>https://arxiv.org/pdf/2408.05868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmad Rezaei, Mohammad Akbari, Saeed Ranjbar Alvar, Arezou Fatemi, Yong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.05868">LaWa: Using Latent Space for In-Generation Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With generative models producing high quality images that are indistinguishable from real ones, there is growing concern regarding the malicious usage of AI-generated images. Imperceptible image watermarking is one viable solution towards such concerns. Prior watermarking methods map the image to a latent space for adding the watermark. Moreover, Latent Diffusion Models (LDM) generate the image in the latent space of a pre-trained autoencoder. We argue that this latent space can be used to integrate watermarking into the generation process. To this end, we present LaWa, an in-generation image watermarking method designed for LDMs. By using coarse-to-fine watermark embedding modules, LaWa modifies the latent space of pre-trained autoencoders and achieves high robustness against a wide range of image transformations while preserving perceptual quality of the image. We show that LaWa can also be used as a general image watermarking method. Through extensive experiments, we demonstrate that LaWa outperforms previous works in perceptual quality, robustness against attacks, and computational complexity, while having very low false positive rate. Code is available here.
<div id='section'>Paperid: <span id='pid'>382, <a href='https://arxiv.org/pdf/2406.10328.pdf' target='_blank'>https://arxiv.org/pdf/2406.10328.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vasu Singla, Kaiyu Yue, Sukriti Paul, Reza Shirkavand, Mayuka Jayawardhana, Alireza Ganjdanesh, Heng Huang, Abhinav Bhatele, Gowthami Somepalli, Tom Goldstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10328">From Pixels to Prose: A Large Dataset of Dense Image Captions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training large vision-language models requires extensive, high-quality image-text pairs. Existing web-scraped datasets, however, are noisy and lack detailed image descriptions. To bridge this gap, we introduce PixelProse, a comprehensive dataset of over 16M (million) synthetically generated captions, leveraging cutting-edge vision-language models for detailed and accurate descriptions. To ensure data integrity, we rigorously analyze our dataset for problematic content, including child sexual abuse material (CSAM), personally identifiable information (PII), and toxicity. We also provide valuable metadata such as watermark presence and aesthetic scores, aiding in further dataset filtering. We hope PixelProse will be a valuable resource for future vision-language research. PixelProse is available at https://huggingface.co/datasets/tomg-group-umd/pixelprose
<div id='section'>Paperid: <span id='pid'>383, <a href='https://arxiv.org/pdf/2404.04254.pdf' target='_blank'>https://arxiv.org/pdf/2404.04254.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengyuan Jiang, Moyang Guo, Yuepeng Hu, Neil Zhenqiang Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.04254">Watermark-based Attribution of AI-Generated Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Several companies have deployed watermark-based detection to identify AI-generated content. However, attribution--the ability to trace back to the user of a generative AI (GenAI) service who created a given piece of AI-generated content--remains largely unexplored despite its growing importance. In this work, we aim to bridge this gap by conducting the first systematic study on watermark-based, user-level attribution of AI-generated content. Our key idea is to assign a unique watermark to each user of the GenAI service and embed this watermark into the AI-generated content created by that user. Attribution is then performed by identifying the user whose watermark best matches the one extracted from the given content. This approach, however, faces a key challenge: How should watermarks be selected for users to maximize attribution performance? To address the challenge, we first theoretically derive lower bounds on detection and attribution performance through rigorous probabilistic analysis for any given set of user watermarks. Then, we select watermarks for users to maximize these lower bounds, thereby optimizing detection and attribution performance. Our theoretical and empirical results show that watermark-based attribution inherits both the accuracy and (non-)robustness properties of the underlying watermark. Specifically, attribution remains highly accurate when the watermarked AI-generated content is either not post-processed or subjected to common post-processing such as JPEG compression, as well as black-box adversarial post-processing with limited query budgets.
<div id='section'>Paperid: <span id='pid'>384, <a href='https://arxiv.org/pdf/2403.05628.pdf' target='_blank'>https://arxiv.org/pdf/2403.05628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saeed Ranjbar Alvar, Mohammad Akbari, David Ming Xuan Yue, Yong Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05628">AMUSE: Adaptive Multi-Segment Encoding for Dataset Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Curating high quality datasets that play a key role in the emergence of new AI applications requires considerable time, money, and computational resources. So, effective ownership protection of datasets is becoming critical. Recently, to protect the ownership of an image dataset, imperceptible watermarking techniques are used to store ownership information (i.e., watermark) into the individual image samples. Embedding the entire watermark into all samples leads to significant redundancy in the embedded information which damages the watermarked dataset quality and extraction accuracy. In this paper, a multi-segment encoding-decoding method for dataset watermarking (called AMUSE) is proposed to adaptively map the original watermark into a set of shorter sub-messages and vice versa. Our message encoder is an adaptive method that adjusts the length of the sub-messages according to the protection requirements for the target dataset. Existing image watermarking methods are then employed to embed the sub-messages into the original images in the dataset and also to extract them from the watermarked images. Our decoder is then used to reconstruct the original message from the extracted sub-messages. The proposed encoder and decoder are plug-and-play modules that can easily be added to any watermarking method. To this end, extensive experiments are preformed with multiple watermarking solutions which show that applying AMUSE improves the overall message extraction accuracy upto 28% for the same given dataset quality. Furthermore, the image dataset quality is enhanced by a PSNR of $\approx$2 dB on average, while improving the extraction accuracy for one of the tested image watermarking methods.
<div id='section'>Paperid: <span id='pid'>385, <a href='https://arxiv.org/pdf/2310.03991.pdf' target='_blank'>https://arxiv.org/pdf/2310.03991.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abe Bohan Hou, Jingyu Zhang, Tianxing He, Yichen Wang, Yung-Sung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, Yulia Tsvetkov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.03991">SemStamp: A Semantic Watermark with Paraphrastic Robustness for Text Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing watermarking algorithms are vulnerable to paraphrase attacks because of their token-level design. To address this issue, we propose SemStamp, a robust sentence-level semantic watermarking algorithm based on locality-sensitive hashing (LSH), which partitions the semantic space of sentences. The algorithm encodes and LSH-hashes a candidate sentence generated by an LLM, and conducts sentence-level rejection sampling until the sampled sentence falls in watermarked partitions in the semantic embedding space. A margin-based constraint is used to enhance its robustness. To show the advantages of our algorithm, we propose a "bigram" paraphrase attack using the paraphrase that has the fewest bigram overlaps with the original sentence. This attack is shown to be effective against the existing token-level watermarking method. Experimental results show that our novel semantic watermark algorithm is not only more robust than the previous state-of-the-art method on both common and bigram paraphrase attacks, but also is better at preserving the quality of generation.
<div id='section'>Paperid: <span id='pid'>386, <a href='https://arxiv.org/pdf/2301.11305.pdf' target='_blank'>https://arxiv.org/pdf/2301.11305.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Eric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, Chelsea Finn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.11305">DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.
<div id='section'>Paperid: <span id='pid'>387, <a href='https://arxiv.org/pdf/2512.18853.pdf' target='_blank'>https://arxiv.org/pdf/2512.18853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sicheng Song, Yanjie Zhang, Zixin Chen, Huamin Qu, Changbo Wang, Chenhui Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18853">VizDefender: Unmasking Visualization Tampering through Proactive Localization and Intent Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integrity of data visualizations is increasingly threatened by image editing techniques that enable subtle yet deceptive tampering. Through a formative study, we define this challenge and categorize tampering techniques into two primary types: data manipulation and visual encoding manipulation. To address this, we present VizDefender, a framework for tampering detection and analysis. The framework integrates two core components: 1) a semi-fragile watermark module that protects the visualization by embedding a location map to images, which allows for the precise localization of tampered regions while preserving visual quality, and 2) an intent analysis module that leverages Multimodal Large Language Models (MLLMs) to interpret manipulation, inferring the attacker's intent and misleading effects. Extensive evaluations and user studies demonstrate the effectiveness of our methods.
<div id='section'>Paperid: <span id='pid'>388, <a href='https://arxiv.org/pdf/2511.08967.pdf' target='_blank'>https://arxiv.org/pdf/2511.08967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>RuiQiang Zhang, Zehua Ma, Guanjie Wang, Chang Liu, Hengyi Wang, Weiming Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08967">AuthSig: Safeguarding Scanned Signatures Against Unauthorized Reuse in Paperless Workflows</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the deepening trend of paperless workflows, signatures as a means of identity authentication are gradually shifting from traditional ink-on-paper to electronic formats.Despite the availability of dynamic pressure-sensitive and PKI-based digital signatures, static scanned signatures remain prevalent in practice due to their convenience. However, these static images, having almost lost their authentication attributes, cannot be reliably verified and are vulnerable to malicious copying and reuse. To address these issues, we propose AuthSig, a novel static electronic signature framework based on generative models and watermark, which binds authentication information to the signature image. Leveraging the human visual system's insensitivity to subtle style variations, AuthSig finely modulates style embeddings during generation to implicitly encode watermark bits-enforcing a One Signature, One Use policy.To overcome the scarcity of handwritten signature data and the limitations of traditional augmentation methods, we introduce a keypoint-driven data augmentation strategy that effectively enhances style diversity to support robust watermark embedding. Experimental results show that AuthSig achieves over 98% extraction accuracy under both digital-domain distortions and signature-specific degradations, and remains effective even in print-scan scenarios.
<div id='section'>Paperid: <span id='pid'>389, <a href='https://arxiv.org/pdf/2510.11251.pdf' target='_blank'>https://arxiv.org/pdf/2510.11251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Xu, Jiawei Chen, Zhaoxia Yin, Cong Kong, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.11251">Large Language Models Are Effective Code Watermarkers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread use of large language models (LLMs) and open-source code has raised ethical and security concerns regarding the distribution and attribution of source code, including unauthorized redistribution, license violations, and misuse of code for malicious purposes. Watermarking has emerged as a promising solution for source attribution, but existing techniques rely heavily on hand-crafted transformation rules, abstract syntax tree (AST) manipulation, or task-specific training, limiting their scalability and generality across languages. Moreover, their robustness against attacks remains limited. To address these limitations, we propose CodeMark-LLM, an LLM-driven watermarking framework that embeds watermark into source code without compromising its semantics or readability. CodeMark-LLM consists of two core components: (i) Semantically Consistent Embedding module that applies functionality-preserving transformations to encode watermark bits, and (ii) Differential Comparison Extraction module that identifies the applied transformations by comparing the original and watermarked code. Leveraging the cross-lingual generalization ability of LLM, CodeMark-LLM avoids language-specific engineering and training pipelines. Extensive experiments across diverse programming languages and attack scenarios demonstrate its robustness, effectiveness, and scalability.
<div id='section'>Paperid: <span id='pid'>390, <a href='https://arxiv.org/pdf/2509.12574.pdf' target='_blank'>https://arxiv.org/pdf/2509.12574.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siyuan Bao, Ying Shi, Zhiguang Yang, Hanzhou Wu, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.12574">Yet Another Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing watermarking methods for large language models (LLMs) mainly embed watermark by adjusting the token sampling prediction or post-processing, lacking intrinsic coupling with LLMs, which may significantly reduce the semantic quality of the generated marked texts. Traditional watermarking methods based on training or fine-tuning may be extendable to LLMs. However, most of them are limited to the white-box scenario, or very time-consuming due to the massive parameters of LLMs. In this paper, we present a new watermarking framework for LLMs, where the watermark is embedded into the LLM by manipulating the internal parameters of the LLM, and can be extracted from the generated text without accessing the LLM. Comparing with related methods, the proposed method entangles the watermark with the intrinsic parameters of the LLM, which better balances the robustness and imperceptibility of the watermark. Moreover, the proposed method enables us to extract the watermark under the black-box scenario, which is computationally efficient for use. Experimental results have also verified the feasibility, superiority and practicality. This work provides a new perspective different from mainstream works, which may shed light on future research.
<div id='section'>Paperid: <span id='pid'>391, <a href='https://arxiv.org/pdf/2509.06326.pdf' target='_blank'>https://arxiv.org/pdf/2509.06326.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruisi Zhang, Yifei Zhao, Neusha Javidnia, Mengxin Zheng, Farinaz Koushanfar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.06326">AttestLLM: Efficient Attestation Framework for Billion-scale On-device LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As on-device LLMs(e.g., Apple on-device Intelligence) are widely adopted to reduce network dependency, improve privacy, and enhance responsiveness, verifying the legitimacy of models running on local devices becomes critical. Existing attestation techniques are not suitable for billion-parameter Large Language Models (LLMs), struggling to remain both time- and memory-efficient while addressing emerging threats in the LLM era. In this paper, we present AttestLLM, the first-of-its-kind attestation framework to protect the hardware-level intellectual property (IP) of device vendors by ensuring that only authorized LLMs can execute on target platforms. AttestLLM leverages an algorithm/software/hardware co-design approach to embed robust watermarking signatures onto the activation distributions of LLM building blocks. It also optimizes the attestation protocol within the Trusted Execution Environment (TEE), providing efficient verification without compromising inference throughput. Extensive proof-of-concept evaluations on LLMs from Llama, Qwen, and Phi families for on-device use cases demonstrate AttestLLM's attestation reliability, fidelity, and efficiency. Furthermore, AttestLLM enforces model legitimacy and exhibits resilience against model replacement and forgery attacks.
<div id='section'>Paperid: <span id='pid'>392, <a href='https://arxiv.org/pdf/2508.17702.pdf' target='_blank'>https://arxiv.org/pdf/2508.17702.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Runwen Hu, Peilin Chen, Keyan Ding, Shiqi Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17702">Copyright Protection for 3D Molecular Structures with Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial intelligence (AI) revolutionizes molecule generation in bioengineering and biological research, significantly accelerating discovery processes. However, this advancement introduces critical concerns regarding intellectual property protection. To address these challenges, we propose the first robust watermarking method designed for molecules, which utilizes atom-level features to preserve molecular integrity and invariant features to ensure robustness against affine transformations. Comprehensive experiments validate the effectiveness of our method using the datasets QM9 and GEOM-DRUG, and generative models GeoBFN and GeoLDM. We demonstrate the feasibility of embedding watermarks, maintaining basic properties higher than 90.00\% while achieving watermark accuracy greater than 95.00\%. Furthermore, downstream docking simulations reveal comparable performance between original and watermarked molecules, with binding affinities reaching -6.00 kcal/mol and root mean square deviations below 1.602 Ã. These results confirm that our watermarking technique effectively safeguards molecular intellectual property without compromising scientific utility, enabling secure and responsible AI integration in molecular discovery and research applications.
<div id='section'>Paperid: <span id='pid'>393, <a href='https://arxiv.org/pdf/2508.17247.pdf' target='_blank'>https://arxiv.org/pdf/2508.17247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lixin Jia, Haiyang Sun, Zhiqing Guo, Yunfeng Diao, Dan Ma, Gaobo Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17247">Uncovering and Mitigating Destructive Multi-Embedding Attacks in Deepfake Proactive Forensics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid evolution of deepfake technologies and the wide dissemination of digital media, personal privacy is facing increasingly serious security threats. Deepfake proactive forensics, which involves embedding imperceptible watermarks to enable reliable source tracking, serves as a crucial defense against these threats. Although existing methods show strong forensic ability, they rely on an idealized assumption of single watermark embedding, which proves impractical in real-world scenarios. In this paper, we formally define and demonstrate the existence of Multi-Embedding Attacks (MEA) for the first time. When a previously protected image undergoes additional rounds of watermark embedding, the original forensic watermark can be destroyed or removed, rendering the entire proactive forensic mechanism ineffective. To address this vulnerability, we propose a general training paradigm named Adversarial Interference Simulation (AIS). Rather than modifying the network architecture, AIS explicitly simulates MEA scenarios during fine-tuning and introduces a resilience-driven loss function to enforce the learning of sparse and stable watermark representations. Our method enables the model to maintain the ability to extract the original watermark correctly even after a second embedding. Extensive experiments demonstrate that our plug-and-play AIS training paradigm significantly enhances the robustness of various existing methods against MEA.
<div id='section'>Paperid: <span id='pid'>394, <a href='https://arxiv.org/pdf/2508.03829.pdf' target='_blank'>https://arxiv.org/pdf/2508.03829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Xu, Rui Hu, Zikai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.03829">Majority Bit-Aware Watermarking For Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing deployment of Large Language Models (LLMs) in real-world applications has raised concerns about their potential misuse in generating harmful or deceptive content. To address this issue, watermarking techniques have emerged as a promising solution by embedding identifiable binary messages into generated text for origin verification and misuse tracing. While recent efforts have explored multi-bit watermarking schemes capable of embedding rich information such as user identifiers, they typically suffer from the fundamental trade-off between text quality and decoding accuracy: to ensure reliable message decoding, they have to restrict the size of preferred token sets during encoding, yet such restrictions reduce the quality of the generated content. In this work, we propose MajorMark, a novel watermarking method that improves this trade-off through majority bit-aware encoding. MajorMark selects preferred token sets based on the majority bit of the message, enabling a larger and more flexible sampling of tokens. In contrast to prior methods that rely on token frequency analysis for decoding, MajorMark employs a clustering-based decoding strategy, which maintains high decoding accuracy even when the preferred token set is large, thus preserving both content quality and decoding accuracy. We further introduce MajorMark$^+$, which partitions the message into multiple blocks to independently encode and deterministically decode each block, thereby further enhancing the quality of watermarked text and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs demonstrate that our methods significantly enhance both decoding accuracy and text generation quality, outperforming prior multi-bit watermarking baselines.
<div id='section'>Paperid: <span id='pid'>395, <a href='https://arxiv.org/pdf/2507.18036.pdf' target='_blank'>https://arxiv.org/pdf/2507.18036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan An, Guang Hua, Yu Guo, Hangcheng Cao, Susanto Rahardja, Yuguang Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.18036">NWaaS: Nonintrusive Watermarking as a Service for X-to-Image DNN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intellectual property of deep neural network (DNN) models can be protected with DNN watermarking, which embeds copyright watermarks into model parameters (white-box), model behavior (black-box), or model outputs (box-free), and the watermarks can be subsequently extracted to verify model ownership or detect model theft. Despite recent advances, these existing methods are inherently intrusive, as they either modify the model parameters or alter the structure. This natural intrusiveness raises concerns about watermarking-induced shifts in model behavior and the additional cost of fine-tuning, further exacerbated by the rapidly growing model size. As a result, model owners are often reluctant to adopt DNN watermarking in practice, which limits the development of practical Watermarking as a Service (WaaS) systems. To address this issue, we introduce Nonintrusive Watermarking as a Service (NWaaS), a novel trustless paradigm designed for X-to-Image models, in which we hypothesize that with the model untouched, an owner-defined watermark can still be extracted from model outputs. Building on this concept, we propose ShadowMark, a concrete implementation of NWaaS which addresses critical deployment challenges by establishing a robust and nonintrusive side channel in the protected model's black-box API, leveraging a key encoder and a watermark decoder. It is significantly distinctive from existing solutions by attaining the so-called absolute fidelity and being applicable to different DNN architectures, while being also robust against existing attacks, eliminating the fidelity-robustness trade-off. Extensive experiments on image-to-image, noise-to-image, noise-and-text-to-image, and text-to-image models, demonstrate the efficacy and practicality of ShadowMark for real-world deployment of nonintrusive DNN watermarking.
<div id='section'>Paperid: <span id='pid'>396, <a href='https://arxiv.org/pdf/2506.11444.pdf' target='_blank'>https://arxiv.org/pdf/2506.11444.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kecen Li, Zhicong Huang, Xinwen Hou, Cheng Hong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.11444">GaussMarker: Robust Dual-Domain Watermark for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Diffusion Models (DM) generate increasingly realistic images, related issues such as copyright and misuse have become a growing concern. Watermarking is one of the promising solutions. Existing methods inject the watermark into the single-domain of initial Gaussian noise for generation, which suffers from unsatisfactory robustness. This paper presents the first dual-domain DM watermarking approach using a pipelined injector to consistently embed watermarks in both the spatial and frequency domains. To further boost robustness against certain image manipulations and advanced attacks, we introduce a model-independent learnable Gaussian Noise Restorer (GNR) to refine Gaussian noise extracted from manipulated images and enhance detection robustness by integrating the detection scores of both watermarks. GaussMarker efficiently achieves state-of-the-art performance under eight image distortions and four advanced attacks across three versions of Stable Diffusion with better recall and lower false positive rates, as preferred in real applications.
<div id='section'>Paperid: <span id='pid'>397, <a href='https://arxiv.org/pdf/2505.13651.pdf' target='_blank'>https://arxiv.org/pdf/2505.13651.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahao Xu, Rui Hu, Olivera Kotevska, Zikai Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13651">Traceable Black-box Watermarks for Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to the distributed nature of Federated Learning (FL) systems, each local client has access to the global model, posing a critical risk of model leakage. Existing works have explored injecting watermarks into local models to enable intellectual property protection. However, these methods either focus on non-traceable watermarks or traceable but white-box watermarks. We identify a gap in the literature regarding the formal definition of traceable black-box watermarking and the formulation of the problem of injecting such watermarks into FL systems. In this work, we first formalize the problem of injecting traceable black-box watermarks into FL. Based on the problem, we propose a novel server-side watermarking method, $\mathbf{TraMark}$, which creates a traceable watermarked model for each client, enabling verification of model leakage in black-box settings. To achieve this, $\mathbf{TraMark}$ partitions the model parameter space into two distinct regions: the main task region and the watermarking region. Subsequently, a personalized global model is constructed for each client by aggregating only the main task region while preserving the watermarking region. Each model then learns a unique watermark exclusively within the watermarking region using a distinct watermark dataset before being sent back to the local client. Extensive results across various FL systems demonstrate that $\mathbf{TraMark}$ ensures the traceability of all watermarked models while preserving their main task performance.
<div id='section'>Paperid: <span id='pid'>398, <a href='https://arxiv.org/pdf/2502.05425.pdf' target='_blank'>https://arxiv.org/pdf/2502.05425.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihao Wang, Lingxiao Li, Yifan Tang, Ru Zhang, Jianyi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05425">Toward Copyright Integrity and Verifiability via Multi-Bit Watermarking for Intelligent Transportation Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intelligent transportation systems (ITS) use advanced technologies such as artificial intelligence to significantly improve traffic flow management efficiency, and promote the intelligent development of the transportation industry. However, if the data in ITS is attacked, such as tampering or forgery, it will endanger public safety and cause social losses. Therefore, this paper proposes a watermarking that can verify the integrity of copyright in response to the needs of ITS, termed ITSmark. ITSmark focuses on functions such as extracting watermarks, verifying permission, and tracing tampered locations. The scheme uses the copyright information to build the multi-bit space and divides this space into multiple segments. These segments will be assigned to tokens. Thus, the next token is determined by its segment which contains the copyright. In this way, the obtained data contains the custom watermark. To ensure the authorization, key parameters are encrypted during copyright embedding to obtain cipher data. Only by possessing the correct cipher data and private key, can the user entirely extract the watermark. Experiments show that ITSmark surpasses baseline performances in data quality, extraction accuracy, and unforgeability. It also shows unique capabilities of permission verification and tampered location tracing, which ensures the security of extraction and the reliability of copyright verification. Furthermore, ITSmark can also customize the watermark embedding position and proportion according to user needs, making embedding more flexible.
<div id='section'>Paperid: <span id='pid'>399, <a href='https://arxiv.org/pdf/2410.19096.pdf' target='_blank'>https://arxiv.org/pdf/2410.19096.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruisi Zhang, Farinaz Koushanfar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.19096">Watermarking Large Language Models and the Generated Content: Opportunities and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widely adopted and powerful generative large language models (LLMs) have raised concerns about intellectual property rights violations and the spread of machine-generated misinformation. Watermarking serves as a promising approch to establish ownership, prevent unauthorized use, and trace the origins of LLM-generated content. This paper summarizes and shares the challenges and opportunities we found when watermarking LLMs. We begin by introducing techniques for watermarking LLMs themselves under different threat models and scenarios. Next, we investigate watermarking methods designed for the content generated by LLMs, assessing their effectiveness and resilience against various attacks. We also highlight the importance of watermarking domain-specific models and data, such as those used in code generation, chip design, and medical applications. Furthermore, we explore methods like hardware acceleration to improve the efficiency of the watermarking process. Finally, we discuss the limitations of current approaches and outline future research directions for the responsible use and protection of these generative AI tools.
<div id='section'>Paperid: <span id='pid'>400, <a href='https://arxiv.org/pdf/2409.10570.pdf' target='_blank'>https://arxiv.org/pdf/2409.10570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cong Kong, Rui Xu, Weixi Chen, Jiawei Chen, Zhaoxia Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.10570">Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Model Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advancement of intelligent healthcare, medical pre-trained language models (Med-PLMs) have emerged and demonstrated significant effectiveness in downstream medical tasks. While these models are valuable assets, they are vulnerable to misuse and theft, requiring copyright protection. However, existing watermarking methods for pre-trained language models (PLMs) cannot be directly applied to Med-PLMs due to domain-task mismatch and inefficient watermark embedding. To fill this gap, we propose the first training-free backdoor model watermarking for Med-PLMs. Our method employs low-frequency words as triggers, embedding the watermark by replacing their embeddings in the model's word embedding layer with those of specific medical terms. The watermarked Med-PLMs produce the same output for triggers as for the corresponding specified medical terms. We leverage this unique mapping to design tailored watermark extraction schemes for different downstream tasks, thereby addressing the challenge of domain-task mismatch in previous methods. Experiments demonstrate superior effectiveness of our watermarking method across medical downstream tasks. Moreover, the method exhibits robustness against model extraction, pruning, fusion-based backdoor removal attacks, while maintaining high efficiency with 10-second watermark embedding.
<div id='section'>Paperid: <span id='pid'>401, <a href='https://arxiv.org/pdf/2409.09739.pdf' target='_blank'>https://arxiv.org/pdf/2409.09739.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuehan Zhang, Peizhuo Lv, Yinpeng Liu, Yongqiang Ma, Wei Lu, Xiaofeng Wang, Xiaozhong Liu, Jiawei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09739">PersonaMark: Personalized LLM watermarking for model protection and user attribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of customized Large Language Models (LLMs) offers considerable convenience. However, it also intensifies concerns regarding the protection of copyright/confidential information. With the extensive adoption of private LLMs, safeguarding model copyright and ensuring data privacy have become critical. Text watermarking has emerged as a viable solution for detecting AI-generated content and protecting models. However, existing methods fall short in providing individualized watermarks for each user, a critical feature for enhancing accountability and traceability. In this paper, we introduce PersonaMark, a novel personalized text watermarking scheme designed to protect LLMs' copyrights and bolster accountability. PersonaMark leverages sentence structure as a subtle carrier of watermark information and optimizes the generation process to maintain the natural output of the model. By employing a personalized hashing function, unique watermarks are embedded for each user, enabling high-quality text generation without compromising the model's performance. This approach is both time-efficient and scalable, capable of handling large numbers of users through a multi-user hashing mechanism. To the best of our knowledge, this is a pioneer study to explore personalized watermarking in LLMs. We conduct extensive evaluations across four LLMs, analyzing various metrics such as perplexity, sentiment, alignment, and readability. The results validate that PersonaMark preserves text quality, ensures unbiased watermark insertion, and offers robust watermark detection capabilities, all while maintaining the model's behavior with minimal disruption.
<div id='section'>Paperid: <span id='pid'>402, <a href='https://arxiv.org/pdf/2406.13177.pdf' target='_blank'>https://arxiv.org/pdf/2406.13177.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Zhao, Hanzhou Wu, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.13177">Transferable Watermarking to Self-supervised Pre-trained Graph Encoders by Trigger Embeddings</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed the prosperous development of Graph Self-supervised Learning (GSSL), which enables to pre-train transferable foundation graph encoders. However, the easy-to-plug-in nature of such encoders makes them vulnerable to copyright infringement. To address this issue, we develop a novel watermarking framework to protect graph encoders in GSSL settings. The key idea is to force the encoder to map a set of specially crafted trigger instances into a unique compact cluster in the outputted embedding space during model pre-training. Consequently, when the encoder is stolen and concatenated with any downstream classifiers, the resulting model inherits the `backdoor' of the encoder and predicts the trigger instances to be in a single category with high probability regardless of the ground truth. Experimental results have shown that, the embedded watermark can be transferred to various downstream tasks in black-box settings, including node classification, link prediction and community detection, which forms a reliable watermark verification system for GSSL in reality. This approach also shows satisfactory performance in terms of model fidelity, reliability and robustness.
<div id='section'>Paperid: <span id='pid'>403, <a href='https://arxiv.org/pdf/2405.15426.pdf' target='_blank'>https://arxiv.org/pdf/2405.15426.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuling Cai, Fan Xiang, Guozhu Meng, Yinzhi Cao, Kai Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15426">AuthNet: Neural Network with Integrated Authentication Logic</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model stealing, i.e., unauthorized access and exfiltration of deep learning models, has become one of the major threats. Proprietary models may be protected by access controls and encryption. However, in reality, these measures can be compromised due to system breaches, query-based model extraction or a disgruntled insider. Security hardening of neural networks is also suffering from limits, for example, model watermarking is passive, cannot prevent the occurrence of piracy and not robust against transformations. To this end, we propose a native authentication mechanism, called AuthNet, which integrates authentication logic as part of the model without any additional structures. Our key insight is to reuse redundant neurons with low activation and embed authentication bits in an intermediate layer, called a gate layer. Then, AuthNet fine-tunes the layers after the gate layer to embed authentication logic so that only inputs with special secret key can trigger the correct logic of AuthNet. It exhibits two intuitive advantages. It provides the last line of defense, i.e., even being exfiltrated, the model is not usable as the adversary cannot generate valid inputs without the key. Moreover, the authentication logic is difficult to inspect and identify given millions or billions of neurons in the model. We theoretically demonstrate the high sensitivity of AuthNet to the secret key and its high confusion for unauthorized samples. AuthNet is compatible with any convolutional neural network, where our extensive evaluations show that AuthNet successfully achieves the goal in rejecting unauthenticated users (whose average accuracy drops to 22.03%) with a trivial accuracy decrease (1.18% on average) for legitimate users, and is robust against model transformation and adaptive attacks.
<div id='section'>Paperid: <span id='pid'>404, <a href='https://arxiv.org/pdf/2405.09863.pdf' target='_blank'>https://arxiv.org/pdf/2405.09863.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan An, Guang Hua, Zhiping Lin, Yuguang Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.09863">Box-Free Model Watermarks Are Prone to Black-Box Removal Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Box-free model watermarking is an emerging technique to safeguard the intellectual property of deep learning models, particularly those for low-level image processing tasks. Existing works have verified and improved its effectiveness in several aspects. However, in this paper, we reveal that box-free model watermarking is prone to removal attacks, even under the real-world threat model such that the protected model and the watermark extractor are in black boxes. Under this setting, we carry out three studies. 1) We develop an extractor-gradient-guided (EGG) remover and show its effectiveness when the extractor uses ReLU activation only. 2) More generally, for an unknown extractor, we leverage adversarial attacks and design the EGG remover based on the estimated gradients. 3) Under the most stringent condition that the extractor is inaccessible, we design a transferable remover based on a set of private proxy models. In all cases, the proposed removers can successfully remove embedded watermarks while preserving the quality of the processed images, and we also demonstrate that the EGG remover can even replace the watermarks. Extensive experimental results verify the effectiveness and generalizability of the proposed attacks, revealing the vulnerabilities of the existing box-free methods and calling for further research.
<div id='section'>Paperid: <span id='pid'>405, <a href='https://arxiv.org/pdf/2404.18407.pdf' target='_blank'>https://arxiv.org/pdf/2404.18407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruisi Zhang, Rachel Selina Rajarathnam, David Z. Pan, Farinaz Koushanfar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.18407">ICMarks: A Robust Watermarking Framework for Integrated Circuit Physical Design IP Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Physical design watermarking on contemporary integrated circuit (IC) layout encodes signatures without considering the dense connections and design constraints, which could lead to performance degradation on the watermarked products. This paper presents ICMarks, a quality-preserving and robust watermarking framework for modern IC physical design. ICMarks embeds unique watermark signatures during the physical design's placement stage, thereby authenticating the IC layout ownership. ICMarks's novelty lies in (i) strategically identifying a region of cells to watermark with minimal impact on the layout performance and (ii) a two-level watermarking framework for augmented robustness toward potential removal and forging attacks. Extensive evaluations on benchmarks of different design objectives and sizes validate that ICMarks incurs no wirelength and timing metrics degradation, while successfully proving ownership. Furthermore, we demonstrate ICMarks is robust against two major watermarking attack categories, namely, watermark removal and forging attacks; even if the adversaries have prior knowledge of the watermarking schemes, the signatures cannot be removed without significantly undermining the layout quality.
<div id='section'>Paperid: <span id='pid'>406, <a href='https://arxiv.org/pdf/2402.17938.pdf' target='_blank'>https://arxiv.org/pdf/2402.17938.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruisi Zhang, Farinaz Koushanfar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.17938">EmMark: Robust Watermarks for IP Protection of Embedded Quantized Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces EmMark,a novel watermarking framework for protecting the intellectual property (IP) of embedded large language models deployed on resource-constrained edge devices. To address the IP theft risks posed by malicious end-users, EmMark enables proprietors to authenticate ownership by querying the watermarked model weights and matching the inserted signatures. EmMark's novelty lies in its strategic watermark weight parameters selection, nsuring robustness and maintaining model quality. Extensive proof-of-concept evaluations of models from OPT and LLaMA-2 families demonstrate EmMark's fidelity, achieving 100% success in watermark extraction with model performance preservation. EmMark also showcased its resilience against watermark removal and forging attacks.
<div id='section'>Paperid: <span id='pid'>407, <a href='https://arxiv.org/pdf/2401.15656.pdf' target='_blank'>https://arxiv.org/pdf/2401.15656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihao Wang, Ruiqi Song, Lingxiao Li, Ru Zhang, Jianyi Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15656">Dynamically Allocated Interval-Based Generative Linguistic Steganography with Roulette Wheel</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing linguistic steganography schemes often overlook the conditional probability (CP) of tokens in the candidate pool, allocating the one coding to all tokens, which results in identical selection likelihoods. This approach leads to the selection of low-CP tokens, degrading the quality of stegos and making them more detectable. This paper proposes a scheme based on the interval allocated, called DAIRstega. DAIRstega first uses a portion of the read secret to build the roulette area. Then, this scheme uses the idea of the roulette wheel and takes the CPs of tokens as the main basis for allocating the roulette area (i.e., the interval length). Thus, tokens with larger CPs are allocated more area. The secret will have an increased likelihood of selecting a token with a higher CP. During allocation, we designed some allocation functions and three constraints to optimize the process. Additionally, DAIRstega supports prompt-based controllable generation of stegos. Rich experiments show that the proposed embedding way and DAIRstega perform better than the existing ways and baselines, which shows strong perceptual, statistical, and semantic concealment, as well as anti-steganalysis ability. It can also generate high-quality longer stegos, addressing the deficiencies in this task. DAIRstega is confirmed to have potential as a secure watermarking, offering insights for its development.
<div id='section'>Paperid: <span id='pid'>408, <a href='https://arxiv.org/pdf/2310.12362.pdf' target='_blank'>https://arxiv.org/pdf/2310.12362.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, Farinaz Koushanfar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.12362">REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval. Extensive evaluations on multiple unseen datasets highlight REMARK-LLM proficiency and transferability in inserting 2 times more signature bits into the same texts when compared to prior art, all while maintaining semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against a spectrum of watermark detection and removal attacks.
<div id='section'>Paperid: <span id='pid'>409, <a href='https://arxiv.org/pdf/2305.12391.pdf' target='_blank'>https://arxiv.org/pdf/2305.12391.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Zhang, Yong Liu, Xinpeng Zhang, Hanzhou Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.12391">Generative Model Watermarking Suppressing High-Frequency Artifacts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protecting deep neural networks (DNNs) against intellectual property (IP) infringement has attracted an increasing attention in recent years. Recent advances focus on IP protection of generative models, which embed the watermark information into the image generated by the model to be protected. Although the generated marked image has good visual quality, it introduces noticeable artifacts to the marked image in high-frequency area, which severely impairs the imperceptibility of the watermark and thereby reduces the security of the watermarking system. To deal with this problem, in this paper, we propose a novel framework for generative model watermarking that can suppress those high-frequency artifacts. The main idea of the proposed framework is to design a new watermark embedding network that can suppress high-frequency artifacts by applying anti-aliasing. To realize anti-aliasing, we use low-pass filtering for the internal sampling layers of the new watermark embedding network. Meanwhile, joint loss optimization and adversarial training are applied to enhance the effectiveness and robustness. Experimental results indicate that the marked model not only maintains the performance very well on the original task, but also demonstrates better imperceptibility and robustness on the watermarking task. This work reveals the importance of suppressing high-frequency artifacts for enhancing imperceptibility and security of generative model watermarking.
<div id='section'>Paperid: <span id='pid'>410, <a href='https://arxiv.org/pdf/2209.03563.pdf' target='_blank'>https://arxiv.org/pdf/2209.03563.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhuo Lv, Pan Li, Shenchen Zhu, Shengzhi Zhang, Kai Chen, Ruigang Liang, Chang Yue, Fan Xiang, Yuling Cai, Hualong Ma, Yingjun Zhang, Guozhu Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2209.03563">SSL-WM: A Black-Box Watermarking Approach for Encoders Pre-trained by Self-supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have witnessed tremendous success in Self-Supervised Learning (SSL), which has been widely utilized to facilitate various downstream tasks in Computer Vision (CV) and Natural Language Processing (NLP) domains. However, attackers may steal such SSL models and commercialize them for profit, making it crucial to verify the ownership of the SSL models. Most existing ownership protection solutions (e.g., backdoor-based watermarks) are designed for supervised learning models and cannot be used directly since they require that the models' downstream tasks and target labels be known and available during watermark embedding, which is not always possible in the domain of SSL. To address such a problem, especially when downstream tasks are diverse and unknown during watermark embedding, we propose a novel black-box watermarking solution, named SSL-WM, for verifying the ownership of SSL models. SSL-WM maps watermarked inputs of the protected encoders into an invariant representation space, which causes any downstream classifier to produce expected behavior, thus allowing the detection of embedded watermarks. We evaluate SSL-WM on numerous tasks, such as CV and NLP, using different SSL models both contrastive-based and generative-based. Experimental results demonstrate that SSL-WM can effectively verify the ownership of stolen SSL models in various downstream tasks. Furthermore, SSL-WM is robust against model fine-tuning, pruning, and input preprocessing attacks. Lastly, SSL-WM can also evade detection from evaluated watermark detection approaches, demonstrating its promising application in protecting the ownership of SSL models.
<div id='section'>Paperid: <span id='pid'>411, <a href='https://arxiv.org/pdf/2208.03944.pdf' target='_blank'>https://arxiv.org/pdf/2208.03944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yong Liu, Hanzhou Wu, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.03944">Robust and Imperceptible Black-box DNN Watermarking Based on Fourier Perturbation Analysis and Frequency Sensitivity Clustering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, more and more attention has been focused on the intellectual property protection of deep neural networks (DNNs), promoting DNN watermarking to become a hot research topic. Compared with embedding watermarks directly into DNN parameters, inserting trigger-set watermarks enables us to verify the ownership without knowing the internal details of the DNN, which is more suitable for application scenarios. The cost is we have to carefully craft the trigger samples. Mainstream methods construct the trigger samples by inserting a noticeable pattern to the clean samples in the spatial domain, which does not consider sample imperceptibility, sample robustness and model robustness, and therefore has limited the watermarking performance and the model generalization. It has motivated the authors in this paper to propose a novel DNN watermarking method based on Fourier perturbation analysis and frequency sensitivity clustering. First, we analyze the perturbation impact of different frequency components of the input sample on the task functionality of the DNN by applying random perturbation. Then, by K-means clustering, we determine the frequency components that result in superior watermarking performance for crafting the trigger samples. Our experiments show that the proposed work not only maintains the performance of the DNN on its original task, but also provides better watermarking performance compared with related works.
<div id='section'>Paperid: <span id='pid'>412, <a href='https://arxiv.org/pdf/2110.12948.pdf' target='_blank'>https://arxiv.org/pdf/2110.12948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingjie Li, Hanzhou Wu, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2110.12948">Generating Watermarked Adversarial Texts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarial example generation has been a hot spot in recent years because it can cause deep neural networks (DNNs) to misclassify the generated adversarial examples, which reveals the vulnerability of DNNs, motivating us to find good solutions to improve the robustness of DNN models. Due to the extensiveness and high liquidity of natural language over the social networks, various natural language based adversarial attack algorithms have been proposed in the literature. These algorithms generate adversarial text examples with high semantic quality. However, the generated adversarial text examples may be maliciously or illegally used. In order to tackle with this problem, we present a general framework for generating watermarked adversarial text examples. For each word in a given text, a set of candidate words are determined to ensure that all the words in the set can be used to either carry secret bits or facilitate the construction of adversarial example. By applying a word-level adversarial text generation algorithm, the watermarked adversarial text example can be finally generated. Experiments show that the adversarial text examples generated by the proposed method not only successfully fool advanced DNN models, but also carry a watermark that can effectively verify the ownership and trace the source of the adversarial examples. Moreover, the watermark can still survive after attacked with adversarial example generation algorithms, which has shown the applicability and superiority.
<div id='section'>Paperid: <span id='pid'>413, <a href='https://arxiv.org/pdf/2011.00512.pdf' target='_blank'>https://arxiv.org/pdf/2011.00512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiangyu Zhao, Hanzhou Wu, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2011.00512">Watermarking Graph Neural Networks by Random Graphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Many learning tasks require us to deal with graph data which contains rich relational information among elements, leading increasing graph neural network (GNN) models to be deployed in industrial products for improving the quality of service. However, they also raise challenges to model authentication. It is necessary to protect the ownership of the GNN models, which motivates us to present a watermarking method to GNN models in this paper. In the proposed method, an Erdos-Renyi (ER) random graph with random node feature vectors and labels is randomly generated as a trigger to train the GNN to be protected together with the normal samples. During model training, the secret watermark is embedded into the label predictions of the ER graph nodes. During model verification, by activating a marked GNN with the trigger ER graph, the watermark can be reconstructed from the output to verify the ownership. Since the ER graph was randomly generated, by feeding it to a non-marked GNN, the label predictions of the graph nodes are random, resulting in a low false alarm rate (of the proposed work). Experimental results have also shown that, the performance of a marked GNN on its original task will not be impaired. Moreover, it is robust against model compression and fine-tuning, which has shown the superiority and applicability.
<div id='section'>Paperid: <span id='pid'>414, <a href='https://arxiv.org/pdf/2512.15641.pdf' target='_blank'>https://arxiv.org/pdf/2512.15641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfei Yang, Xiaojun Chen, Zhendong Zhao, Yu Zhou, Xiaoyan Gu, Juan Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15641">ComMark: Covert and Robust Black-Box Model Watermarking with Compressed Samples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of deep learning has turned models into highly valuable assets due to their reliance on massive data and costly training processes. However, these models are increasingly vulnerable to leakage and theft, highlighting the critical need for robust intellectual property protection. Model watermarking has emerged as an effective solution, with black-box watermarking gaining significant attention for its practicality and flexibility. Nonetheless, existing black-box methods often fail to better balance covertness (hiding the watermark to prevent detection and forgery) and robustness (ensuring the watermark resists removal)-two essential properties for real-world copyright verification. In this paper, we propose ComMark, a novel black-box model watermarking framework that leverages frequency-domain transformations to generate compressed, covert, and attack-resistant watermark samples by filtering out high-frequency information. To further enhance watermark robustness, our method incorporates simulated attack scenarios and a similarity loss during training. Comprehensive evaluations across diverse datasets and architectures demonstrate that ComMark achieves state-of-the-art performance in both covertness and robustness. Furthermore, we extend its applicability beyond image recognition to tasks including speech recognition, sentiment analysis, image generation, image captioning, and video recognition, underscoring its versatility and broad applicability.
<div id='section'>Paperid: <span id='pid'>415, <a href='https://arxiv.org/pdf/2511.08985.pdf' target='_blank'>https://arxiv.org/pdf/2511.08985.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunfei Yang, Xiaojun Chen, Yuexin Xuan, Zhendong Zhao, Xin Zhao, He Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08985">DeepTracer: Tracing Stolen Model via Deep Coupled Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model watermarking techniques can embed watermark information into the protected model for ownership declaration by constructing specific input-output pairs. However, existing watermarks are easily removed when facing model stealing attacks, and make it difficult for model owners to effectively verify the copyright of stolen models. In this paper, we analyze the root cause of the failure of current watermarking methods under model stealing scenarios and then explore potential solutions. Specifically, we introduce a robust watermarking framework, DeepTracer, which leverages a novel watermark samples construction method and a same-class coupling loss constraint. DeepTracer can incur a high-coupling model between watermark task and primary task that makes adversaries inevitably learn the hidden watermark task when stealing the primary task functionality. Furthermore, we propose an effective watermark samples filtering mechanism that elaborately select watermark key samples used in model ownership verification to enhance the reliability of watermarks. Extensive experiments across multiple datasets and models demonstrate that our method surpasses existing approaches in defending against various model stealing attacks, as well as watermark attacks, and achieves new state-of-the-art effectiveness and robustness.
<div id='section'>Paperid: <span id='pid'>416, <a href='https://arxiv.org/pdf/2510.22007.pdf' target='_blank'>https://arxiv.org/pdf/2510.22007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>T. Tony Cai, Xiang Li, Qi Long, Weijie J. Su, Garrett G. Wen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.22007">Optimal Detection for Language Watermarks with Pseudorandom Collision</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking plays a crucial role in ensuring the traceability and accountability of large language model (LLM) outputs and mitigating misuse. While promising, most existing methods assume perfect pseudorandomness. In practice, repetition in generated text induces collisions that create structured dependence, compromising Type I error control and invalidating standard analyses. We introduce a statistical framework that captures this structure through a hierarchical two-layer partition. At its core is the concept of minimal units -- the smallest groups treatable as independent across units while permitting dependence within. Using minimal units, we define a non-asymptotic efficiency measure and cast watermark detection as a minimax hypothesis testing problem. Applied to Gumbel-max and inverse-transform watermarks, our framework produces closed-form optimal rules. It explains why discarding repeated statistics often improves performance and shows that within-unit dependence must be addressed unless degenerate. Both theory and experiments confirm improved detection power with rigorous Type I error control. These results provide the first principled foundation for watermark detection under imperfect pseudorandomness, offering both theoretical insight and practical guidance for reliable tracing of model outputs.
<div id='section'>Paperid: <span id='pid'>417, <a href='https://arxiv.org/pdf/2510.07728.pdf' target='_blank'>https://arxiv.org/pdf/2510.07728.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiyang Liu, Ziqiang Cui, Di Liang, Wei Ye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.07728">Who Stole Your Data? A Method for Detecting Unauthorized RAG Theft</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Retrieval-augmented generation (RAG) enhances Large Language Models (LLMs) by mitigating hallucinations and outdated information issues, yet simultaneously facilitates unauthorized data appropriation at scale. This paper addresses this challenge through two key contributions. First, we introduce RPD, a novel dataset specifically designed for RAG plagiarism detection that encompasses diverse professional domains and writing styles, overcoming limitations in existing resources. Second, we develop a dual-layered watermarking system that embeds protection at both semantic and lexical levels, complemented by an interrogator-detective framework that employs statistical hypothesis testing on accumulated evidence. Extensive experimentation demonstrates our approach's effectiveness across varying query volumes, defense prompts, and retrieval parameters, while maintaining resilience against adversarial evasion techniques. This work establishes a foundational framework for intellectual property protection in retrieval-augmented AI systems.
<div id='section'>Paperid: <span id='pid'>418, <a href='https://arxiv.org/pdf/2510.03944.pdf' target='_blank'>https://arxiv.org/pdf/2510.03944.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weiqing He, Xiang Li, Tianqi Shang, Li Shen, Weijie Su, Qi Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03944">On the Empirical Power of Goodness-of-Fit Tests in Watermark Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) raise concerns about content authenticity and integrity because they can generate human-like text at scale. Text watermarks, which embed detectable statistical signals into generated text, offer a provable way to verify content origin. Many detection methods rely on pivotal statistics that are i.i.d. under human-written text, making goodness-of-fit (GoF) tests a natural tool for watermark detection. However, GoF tests remain largely underexplored in this setting. In this paper, we systematically evaluate eight GoF tests across three popular watermarking schemes, using three open-source LLMs, two datasets, various generation temperatures, and multiple post-editing methods. We find that general GoF tests can improve both the detection power and robustness of watermark detectors. Notably, we observe that text repetition, common in low-temperature settings, gives GoF tests a unique advantage not exploited by existing methods. Our results highlight that classic GoF tests are a simple yet powerful and underused tool for watermark detection in LLMs.
<div id='section'>Paperid: <span id='pid'>419, <a href='https://arxiv.org/pdf/2506.22343.pdf' target='_blank'>https://arxiv.org/pdf/2506.22343.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Garrett Wen, Weiqing He, Jiayuan Wu, Qi Long, Weijie J. Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22343">Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarks in large language models (LLMs) are an increasingly important tool for detecting synthetic text and distinguishing human-written content from LLM-generated text. While most existing studies focus on determining whether entire texts are watermarked, many real-world scenarios involve mixed-source texts, which blend human-written and watermarked content. In this paper, we address the problem of optimally estimating the watermark proportion in mixed-source texts. We cast this problem as estimating the proportion parameter in a mixture model based on \emph{pivotal statistics}. First, we show that this parameter is not even identifiable in certain watermarking schemes, let alone consistently estimable. In stark contrast, for watermarking methods that employ continuous pivotal statistics for detection, we demonstrate that the proportion parameter is identifiable under mild conditions. We propose efficient estimators for this class of methods, which include several popular unbiased watermarks as examples, and derive minimax lower bounds for any measurable estimator based on pivotal statistics, showing that our estimators achieve these lower bounds. Through evaluations on both synthetic data and mixed-source text generated by open-source models, we demonstrate that our proposed estimators consistently achieve high estimation accuracy.
<div id='section'>Paperid: <span id='pid'>420, <a href='https://arxiv.org/pdf/2505.08234.pdf' target='_blank'>https://arxiv.org/pdf/2505.08234.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krti Tallam, John Kevin Cava, Caleb Geniesse, N. Benjamin Erichson, Michael W. Mahoney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08234">Removing Watermarks with Partial Regeneration using Semantic Information</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged as a primary line of defense for copyright and provenance. The newest watermarking schemes embed semantic signals - content-aware patterns that are designed to survive common image manipulations - yet their true robustness against adaptive adversaries remains under-explored. We expose a previously unreported vulnerability and introduce SemanticRegen, a three-stage, label-free attack that erases state-of-the-art semantic and invisible watermarks while leaving an image's apparent meaning intact. Our pipeline (i) uses a vision-language model to obtain fine-grained captions, (ii) extracts foreground masks with zero-shot segmentation, and (iii) inpaints only the background via an LLM-guided diffusion model, thereby preserving salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing, StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy below 0.75 for the remaining schemes, all while maintaining high perceptual quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM) to quantify fidelity within foreground regions, showing that our attack achieves up to 12 percent higher mSSIM than prior diffusion-based attackers. These results highlight an urgent gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, underscoring the need for watermarking algorithms that are resilient to content-preserving regenerative attacks.
<div id='section'>Paperid: <span id='pid'>421, <a href='https://arxiv.org/pdf/2505.01007.pdf' target='_blank'>https://arxiv.org/pdf/2505.01007.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ling Tang, Yuefeng Chen, Hui Xue, Quanshi Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01007">Towards the Resistance of Neural Network Watermarking to Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proves a new watermarking method to embed the ownership information into a deep neural network (DNN), which is robust to fine-tuning. Specifically, we prove that when the input feature of a convolutional layer only contains low-frequency components, specific frequency components of the convolutional filter will not be changed by gradient descent during the fine-tuning process, where we propose a revised Fourier transform to extract frequency components from the convolutional filter. Additionally, we also prove that these frequency components are equivariant to weight scaling and weight permutations. In this way, we design a watermark module to encode the watermark information to specific frequency components in a convolutional filter. Preliminary experiments demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>422, <a href='https://arxiv.org/pdf/2411.13868.pdf' target='_blank'>https://arxiv.org/pdf/2411.13868.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13868">Robust Detection of Watermarks for Large Language Models Under Human Edits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has offered an effective approach to distinguishing text generated by large language models (LLMs) from human-written text. However, the pervasive presence of human edits on LLM-generated text dilutes watermark signals, thereby significantly degrading detection performance of existing methods. In this paper, by modeling human edits through mixture model detection, we introduce a new method in the form of a truncated goodness-of-fit test for detecting watermarked text under human edits, which we refer to as Tr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection of the Gumbel-max watermark in a certain asymptotic regime of substantial text modifications and vanishing watermark signals. Importantly, Tr-GoF achieves this optimality \textit{adaptively} as it does not require precise knowledge of human edit levels or probabilistic specifications of the LLMs, in contrast to the optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover, we establish that the Tr-GoF test attains the highest detection efficiency rate in a certain regime of moderate text modifications. In stark contrast, we show that sum-based detection rules, as employed by existing methods, fail to achieve optimal robustness in both regimes because the additive nature of their statistics is less resilient to edit-induced noise. Finally, we demonstrate the competitive and sometimes superior empirical performance of the Tr-GoF test on both synthetic data and open-source LLMs in the OPT and LLaMA families.
<div id='section'>Paperid: <span id='pid'>423, <a href='https://arxiv.org/pdf/2406.03720.pdf' target='_blank'>https://arxiv.org/pdf/2406.03720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minzhou Pan, Yi Zeng, Xue Lin, Ning Yu, Cho-Jui Hsieh, Peter Henderson, Ruoxi Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03720">JIGMARK: A Black-Box Approach for Enhancing Image Watermarks against Diffusion Model Edits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this study, we investigate the vulnerability of image watermarks to diffusion-model-based image editing, a challenge exacerbated by the computational cost of accessing gradient information and the closed-source nature of many diffusion models. To address this issue, we introduce JIGMARK. This first-of-its-kind watermarking technique enhances robustness through contrastive learning with pairs of images, processed and unprocessed by diffusion models, without needing a direct backpropagation of the diffusion process. Our evaluation reveals that JIGMARK significantly surpasses existing watermarking solutions in resilience to diffusion-model edits, demonstrating a True Positive Rate more than triple that of leading baselines at a 1% False Positive Rate while preserving image quality. At the same time, it consistently improves the robustness against other conventional perturbations (like JPEG, blurring, etc.) and malicious watermark attacks over the state-of-the-art, often by a large margin. Furthermore, we propose the Human Aligned Variation (HAV) score, a new metric that surpasses traditional similarity measures in quantifying the number of image derivatives from image editing.
<div id='section'>Paperid: <span id='pid'>424, <a href='https://arxiv.org/pdf/2404.01245.pdf' target='_blank'>https://arxiv.org/pdf/2404.01245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.01245">A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks -- one of which has been internally implemented at OpenAI -- and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.
<div id='section'>Paperid: <span id='pid'>425, <a href='https://arxiv.org/pdf/2402.11399.pdf' target='_blank'>https://arxiv.org/pdf/2402.11399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abe Bohan Hou, Jingyu Zhang, Yichen Wang, Daniel Khashabi, Tianxing He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.11399">k-SemStamp: A Clustering-Based Semantic Watermark for Detection of Machine-Generated Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent watermarked generation algorithms inject detectable signatures during language generation to facilitate post-hoc detection. While token-level watermarks are vulnerable to paraphrase attacks, SemStamp (Hou et al., 2023) applies watermark on the semantic representation of sentences and demonstrates promising robustness. SemStamp employs locality-sensitive hashing (LSH) to partition the semantic space with arbitrary hyperplanes, which results in a suboptimal tradeoff between robustness and speed. We propose k-SemStamp, a simple yet effective enhancement of SemStamp, utilizing k-means clustering as an alternative of LSH to partition the embedding space with awareness of inherent semantic structure. Experimental results indicate that k-SemStamp saliently improves its robustness and sampling efficiency while preserving the generation quality, advancing a more effective tool for machine-generated text detection.
<div id='section'>Paperid: <span id='pid'>426, <a href='https://arxiv.org/pdf/2312.04469.pdf' target='_blank'>https://arxiv.org/pdf/2312.04469.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenchen Gu, Xiang Lisa Li, Percy Liang, Tatsunori Hashimoto
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.04469">On the Learnability of Watermarks for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking of language model outputs enables statistical detection of model-generated text, which can mitigate harms and misuses of language models. Existing watermarking strategies operate by altering the decoder of an existing language model. In this paper, we ask whether language models can directly learn to generate watermarked text, which would have significant implications for the real-world deployment of watermarks. First, learned watermarks could be used to build open models that naturally generate watermarked text, enabling watermarking for open models, where users can control the decoding procedure. Second, if watermarking is used to determine the provenance of generated text, an adversary can hurt the reputation of a victim model by spoofing its watermark and generating damaging watermarked text. To investigate the learnability of watermarks, we propose watermark distillation, which trains a student model to behave like a teacher model that uses decoding-based watermarking. We test our approach on three decoding-based watermarking strategies and various hyperparameter settings, finding that models can learn to generate watermarked text with high detectability. We also find limitations to learnability, including the loss of watermarking capabilities under fine-tuning on normal text and high sample complexity when learning low-distortion watermarks.
<div id='section'>Paperid: <span id='pid'>427, <a href='https://arxiv.org/pdf/2312.03205.pdf' target='_blank'>https://arxiv.org/pdf/2312.03205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shuyang Yu, Junyuan Hong, Yi Zeng, Fei Wang, Ruoxi Jia, Jiayu Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.03205">Who Leaked the Model? Tracking IP Infringers in Accountable Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) emerges as an effective collaborative learning framework to coordinate data and computation resources from massive and distributed clients in training. Such collaboration results in non-trivial intellectual property (IP) represented by the model parameters that should be protected and shared by the whole party rather than an individual user. Meanwhile, the distributed nature of FL endorses a malicious client the convenience to compromise IP through illegal model leakage to unauthorized third parties. To block such IP leakage, it is essential to make the IP identifiable in the shared model and locate the anonymous infringer who first leaks it. The collective challenges call for \emph{accountable federated learning}, which requires verifiable ownership of the model and is capable of revealing the infringer's identity upon leakage. In this paper, we propose Decodable Unique Watermarking (DUW) for complying with the requirements of accountable FL. Specifically, before a global model is sent to a client in an FL round, DUW encodes a client-unique key into the model by leveraging a backdoor-based watermark injection. To identify the infringer of a leaked model, DUW examines the model and checks if the triggers can be decoded as the corresponding keys. Extensive empirical results show that DUW is highly effective and robust, achieving over $99\%$ watermark success rate for Digits, CIFAR-10, and CIFAR-100 datasets under heterogeneous FL settings, and identifying the IP infringer with $100\%$ accuracy even after common watermark removal attempts.
<div id='section'>Paperid: <span id='pid'>428, <a href='https://arxiv.org/pdf/2311.09832.pdf' target='_blank'>https://arxiv.org/pdf/2311.09832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Liang Chen, Yatao Bian, Yang Deng, Deng Cai, Shuaiyi Li, Peilin Zhao, Kam-fai Wong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09832">WatME: Towards Lossless Watermarking Through Lexical Redundancy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking has emerged as a pivotal technique for identifying machine-generated text. However, existing methods often rely on arbitrary vocabulary partitioning during decoding to embed watermarks, which compromises the availability of suitable tokens and significantly degrades the quality of responses. This study assesses the impact of watermarking on different capabilities of large language models (LLMs) from a cognitive science lens. Our finding highlights a significant disparity; knowledge recall and logical reasoning are more adversely affected than language generation. These results suggest a more profound effect of watermarking on LLMs than previously understood. To address these challenges, we introduce Watermarking with Mutual Exclusion (WatME), a novel approach leveraging linguistic prior knowledge of inherent lexical redundancy in LLM vocabularies to seamlessly integrate watermarks. Specifically, WatME dynamically optimizes token usage during the decoding process by applying a mutually exclusive rule to the identified lexical redundancies. This strategy effectively prevents the unavailability of appropriate tokens and preserves the expressive power of LLMs. We provide both theoretical analysis and empirical evidence showing that WatME effectively preserves the diverse capabilities of LLMs while ensuring watermark detectability.
<div id='section'>Paperid: <span id='pid'>429, <a href='https://arxiv.org/pdf/2308.14401.pdf' target='_blank'>https://arxiv.org/pdf/2308.14401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhensu Sun, Xiaoning Du, Fu Song, Li Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.14401">CodeMark: Imperceptible Watermarking for Code Datasets against Neural Code Completion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code datasets are of immense value for training neural-network-based code completion models, where companies or organizations have made substantial investments to establish and process these datasets. Unluckily, these datasets, either built for proprietary or public usage, face the high risk of unauthorized exploits, resulting from data leakages, license violations, etc. Even worse, the ``black-box'' nature of neural models sets a high barrier for externals to audit their training datasets, which further connives these unauthorized usages. Currently, watermarking methods have been proposed to prohibit inappropriate usage of image and natural language datasets. However, due to domain specificity, they are not directly applicable to code datasets, leaving the copyright protection of this emerging and important field of code data still exposed to threats. To fill this gap, we propose a method, named CodeMark, to embed user-defined imperceptible watermarks into code datasets to trace their usage in training neural code completion models. CodeMark is based on adaptive semantic-preserving transformations, which preserve the exact functionality of the code data and keep the changes covert against rule-breakers. We implement CodeMark in a toolkit and conduct an extensive evaluation of code completion models. CodeMark is validated to fulfill all desired properties of practical watermarks, including harmlessness to model accuracy, verifiability, robustness, and imperceptibility.
<div id='section'>Paperid: <span id='pid'>430, <a href='https://arxiv.org/pdf/2307.15593.pdf' target='_blank'>https://arxiv.org/pdf/2307.15593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rohith Kuditipudi, John Thickstun, Tatsunori Hashimoto, Percy Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15593">Robust Distortion-free Watermarks for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose a methodology for planting watermarks in text from an autoregressive language model that are robust to perturbations without changing the distribution over text up to a certain maximum generation budget. We generate watermarked text by mapping a sequence of random numbers -- which we compute using a randomized watermark key -- to a sample from the language model. To detect watermarked text, any party who knows the key can align the text to the random number sequence. We instantiate our watermark methodology with two sampling schemes: inverse transform sampling and exponential minimum sampling. We apply these watermarks to three language models -- OPT-1.3B, LLaMA-7B and Alpaca-7B -- to experimentally validate their statistical power and robustness to various paraphrasing attacks. Notably, for both the OPT-1.3B and LLaMA-7B models, we find we can reliably detect watermarked text ($p \leq 0.01$) from $35$ tokens even after corrupting between $40$-$50\%$ of the tokens via random edits (i.e., substitutions, insertions or deletions). For the Alpaca-7B model, we conduct a case study on the feasibility of watermarking responses to typical user instructions. Due to the lower entropy of the responses, detection is more difficult: around $25\%$ of the responses -- whose median length is around $100$ tokens -- are detectable with $p \leq 0.01$, and the watermark is also less robust to certain automated paraphrasing attacks we implement.
<div id='section'>Paperid: <span id='pid'>431, <a href='https://arxiv.org/pdf/2104.12623.pdf' target='_blank'>https://arxiv.org/pdf/2104.12623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian Szyller, Vasisht Duddu, Tommi GrÃ¶ndahl, N. Asokan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2104.12623">Good Artists Copy, Great Artists Steal: Model Extraction Attacks Against Image Translation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models are typically made available to potential client users via inference APIs. Model extraction attacks occur when a malicious client uses information gleaned from queries to the inference API of a victim model $F_V$ to build a surrogate model $F_A$ with comparable functionality. Recent research has shown successful model extraction of image classification, and natural language processing models. In this paper, we show the first model extraction attack against real-world generative adversarial network (GAN) image translation models. We present a framework for conducting such attacks, and show that an adversary can successfully extract functional surrogate models by querying $F_V$ using data from the same domain as the training data for $F_V$. The adversary need not know $F_V$'s architecture or any other information about it beyond its intended task. We evaluate the effectiveness of our attacks using three different instances of two popular categories of image translation: (1) Selfie-to-Anime and (2) Monet-to-Photo (image style transfer), and (3) Super-Resolution (super resolution). Using standard performance metrics for GANs, we show that our attacks are effective. Furthermore, we conducted a large scale (125 participants) user study on Selfie-to-Anime and Monet-to-Photo to show that human perception of the images produced by $F_V$ and $F_A$ can be considered equivalent, within an equivalence bound of Cohen's d = 0.3. Finally, we show that existing defenses against model extraction attacks (watermarking, adversarial examples, poisoning) do not extend to image translation models.
<div id='section'>Paperid: <span id='pid'>432, <a href='https://arxiv.org/pdf/2511.02083.pdf' target='_blank'>https://arxiv.org/pdf/2511.02083.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Avi Bagchi, Akhil Bhimaraju, Moulik Choraria, Daniel Alabi, Lav R. Varshney
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.02083">Watermarking Discrete Diffusion Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has emerged as a promising technique to track AI-generated content and differentiate it from authentic human creations. While prior work extensively studies watermarking for autoregressive large language models (LLMs) and image diffusion models, none address discrete diffusion language models, which are becoming popular due to their high inference throughput. In this paper, we introduce the first watermarking method for discrete diffusion models by applying the distribution-preserving Gumbel-max trick at every diffusion step and seeding the randomness with the sequence index to enable reliable detection. We experimentally demonstrate that our scheme is reliably detectable on state-of-the-art diffusion language models and analytically prove that it is distortion-free with an exponentially decaying probability of false detection in the token sequence length.
<div id='section'>Paperid: <span id='pid'>433, <a href='https://arxiv.org/pdf/2509.18461.pdf' target='_blank'>https://arxiv.org/pdf/2509.18461.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ayan Sar, Sampurna Roy, Tanupriya Choudhury, Ajith Abraham
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.18461">Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative adversarial networks (GANs) and diffusion models have dramatically advanced deepfake technology, and its threats to digital security, media integrity, and public trust have increased rapidly. This research explored zero-shot deepfake detection, an emerging method even when the models have never seen a particular deepfake variation. In this work, we studied self-supervised learning, transformer-based zero-shot classifier, generative model fingerprinting, and meta-learning techniques that better adapt to the ever-evolving deepfake threat. In addition, we suggested AI-driven prevention strategies that mitigated the underlying generation pipeline of the deepfakes before they occurred. They consisted of adversarial perturbations for creating deepfake generators, digital watermarking for content authenticity verification, real-time AI monitoring for content creation pipelines, and blockchain-based content verification frameworks. Despite these advancements, zero-shot detection and prevention faced critical challenges such as adversarial attacks, scalability constraints, ethical dilemmas, and the absence of standardized evaluation benchmarks. These limitations were addressed by discussing future research directions on explainable AI for deepfake detection, multimodal fusion based on image, audio, and text analysis, quantum AI for enhanced security, and federated learning for privacy-preserving deepfake detection. This further highlighted the need for an integrated defense framework for digital authenticity that utilized zero-shot learning in combination with preventive deepfake mechanisms. Finally, we highlighted the important role of interdisciplinary collaboration between AI researchers, cybersecurity experts, and policymakers to create resilient defenses against the rising tide of deepfake attacks.
<div id='section'>Paperid: <span id='pid'>434, <a href='https://arxiv.org/pdf/2505.13101.pdf' target='_blank'>https://arxiv.org/pdf/2505.13101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shaowu Wu, Liting Zeng, Wei Lu, Xiangyang Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.13101">ARIW-Framework: Adaptive Robust Iterative Watermarking Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid rise of large models, copyright protection for generated image content has become a critical security challenge. Although deep learning watermarking techniques offer an effective solution for digital image copyright protection, they still face limitations in terms of visual quality, robustness and generalization. To address these issues, this paper proposes an adaptive robust iterative watermarking framework (ARIW-Framework) that achieves high-quality watermarked images while maintaining exceptional robustness and generalization performance. Specifically, we introduce an iterative approach to optimize the encoder for generating robust residuals. The encoder incorporates noise layers and a decoder to compute robustness weights for residuals under various noise attacks. By employing a parallel optimization strategy, the framework enhances robustness against multiple types of noise attacks. Furthermore, we leverage image gradients to determine the embedding strength at each pixel location, significantly improving the visual quality of the watermarked images. Extensive experiments demonstrate that the proposed method achieves superior visual quality while exhibiting remarkable robustness and generalization against noise attacks.
<div id='section'>Paperid: <span id='pid'>435, <a href='https://arxiv.org/pdf/2504.10853.pdf' target='_blank'>https://arxiv.org/pdf/2504.10853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yaopeng Wang, Huiyu Xu, Zhibo Wang, Jiacheng Du, Zhichao Li, Yiming Li, Qiu Wang, Kui Ren
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10853">PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via Semantic-aware Pivotal Tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking for diffusion images has drawn considerable attention due to the widespread use of text-to-image diffusion models and the increasing need for their copyright protection. Recently, advanced watermarking techniques, such as Tree Ring, integrate watermarks by embedding traceable patterns (e.g., Rings) into the latent distribution during the diffusion process. Such methods disrupt the original semantics of the generated images due to the inevitable distribution shift caused by the watermarks, thereby limiting their practicality, particularly in digital art creation. In this work, we present Semantic-aware Pivotal Tuning Watermarks (PT-Mark), a novel invisible watermarking method that preserves both the semantics of diffusion images and the traceability of the watermark. PT-Mark preserves the original semantics of the watermarked image by gradually aligning the generation trajectory with the original (pivotal) trajectory while maintaining the traceable watermarks during whole diffusion denoising process. To achieve this, we first compute the salient regions of the watermark at each diffusion denoising step as a spatial prior to identify areas that can be aligned without disrupting the watermark pattern. Guided by the region, we then introduce an additional pivotal tuning branch that optimizes the text embedding to align the semantics while preserving the watermarks. Extensive evaluations demonstrate that PT-Mark can preserve the original semantics of the diffusion images while integrating robust watermarks. It achieves a 10% improvement in the performance of semantic preservation (i.e., SSIM, PSNR, and LPIPS) compared to state-of-the-art watermarking methods, while also showing comparable robustness against real-world perturbations and four times greater efficiency.
<div id='section'>Paperid: <span id='pid'>436, <a href='https://arxiv.org/pdf/2504.10782.pdf' target='_blank'>https://arxiv.org/pdf/2504.10782.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick O'Reilly, Zeyu Jin, Jiaqi Su, Bryan Pardo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10782">Deep Audio Watermarks are Shallow: Limitations of Post-Hoc Watermarking Techniques for Speech</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the audio modality, state-of-the-art watermarking methods leverage deep neural networks to allow the embedding of human-imperceptible signatures in generated audio. The ideal is to embed signatures that can be detected with high accuracy when the watermarked audio is altered via compression, filtering, or other transformations. Existing audio watermarking techniques operate in a post-hoc manner, manipulating "low-level" features of audio recordings after generation (e.g. through the addition of a low-magnitude watermark signal). We show that this post-hoc formulation makes existing audio watermarks vulnerable to transformation-based removal attacks. Focusing on speech audio, we (1) unify and extend existing evaluations of the effect of audio transformations on watermark detectability, and (2) demonstrate that state-of-the-art post-hoc audio watermarks can be removed with no knowledge of the watermarking scheme and minimal degradation in audio quality.
<div id='section'>Paperid: <span id='pid'>437, <a href='https://arxiv.org/pdf/2504.05871.pdf' target='_blank'>https://arxiv.org/pdf/2504.05871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaibo Huang, Zipei Zhang, Zhongliang Yang, Linna Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.05871">Agent Guide: A Simple Agent Behavioral Watermarking Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing deployment of intelligent agents in digital ecosystems, such as social media platforms, has raised significant concerns about traceability and accountability, particularly in cybersecurity and digital content protection. Traditional large language model (LLM) watermarking techniques, which rely on token-level manipulations, are ill-suited for agents due to the challenges of behavior tokenization and information loss during behavior-to-action translation. To address these issues, we propose Agent Guide, a novel behavioral watermarking framework that embeds watermarks by guiding the agent's high-level decisions (behavior) through probability biases, while preserving the naturalness of specific executions (action). Our approach decouples agent behavior into two levels, behavior (e.g., choosing to bookmark) and action (e.g., bookmarking with specific tags), and applies watermark-guided biases to the behavior probability distribution. We employ a z-statistic-based statistical analysis to detect the watermark, ensuring reliable extraction over multiple rounds. Experiments in a social media scenario with diverse agent profiles demonstrate that Agent Guide achieves effective watermark detection with a low false positive rate. Our framework provides a practical and robust solution for agent watermarking, with applications in identifying malicious agents and protecting proprietary agent systems.
<div id='section'>Paperid: <span id='pid'>438, <a href='https://arxiv.org/pdf/2410.16618.pdf' target='_blank'>https://arxiv.org/pdf/2410.16618.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Linkang Du, Xuanru Zhou, Min Chen, Chusong Zhang, Zhou Su, Peng Cheng, Jiming Chen, Zhikun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.16618">SoK: Dataset Copyright Auditing in Machine Learning Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the implementation of machine learning (ML) systems becomes more widespread, especially with the introduction of larger ML models, we perceive a spring demand for massive data. However, it inevitably causes infringement and misuse problems with the data, such as using unauthorized online artworks or face images to train ML models. To address this problem, many efforts have been made to audit the copyright of the model training dataset. However, existing solutions vary in auditing assumptions and capabilities, making it difficult to compare their strengths and weaknesses. In addition, robustness evaluations usually consider only part of the ML pipeline and hardly reflect the performance of algorithms in real-world ML applications. Thus, it is essential to take a practical deployment perspective on the current dataset copyright auditing tools, examining their effectiveness and limitations. Concretely, we categorize dataset copyright auditing research into two prominent strands: intrusive methods and non-intrusive methods, depending on whether they require modifications to the original dataset. Then, we break down the intrusive methods into different watermark injection options and examine the non-intrusive methods using various fingerprints. To summarize our results, we offer detailed reference tables, highlight key points, and pinpoint unresolved issues in the current literature. By combining the pipeline in ML systems and analyzing previous studies, we highlight several future directions to make auditing tools more suitable for real-world copyright protection requirements.
<div id='section'>Paperid: <span id='pid'>439, <a href='https://arxiv.org/pdf/2409.19708.pdf' target='_blank'>https://arxiv.org/pdf/2409.19708.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xianheng Feng, Jian Liu, Kui Ren, Chun Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19708">A Certified Robust Watermark For Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The effectiveness of watermark algorithms in AI-generated text identification has garnered significant attention. Concurrently, an increasing number of watermark algorithms have been proposed to enhance the robustness against various watermark attacks. However, these watermark algorithms remain susceptible to adaptive or unseen attacks. To address this issue, to our best knowledge, we propose the first certified robust watermark algorithm for large language models based on randomized smoothing, which can provide provable guarantees for watermarked text. Specifically, we utilize two different models respectively for watermark generation and detection and add Gaussian and Uniform noise respectively in the embedding and permutation space during the training and inference stages of the watermark detector to enhance the certified robustness of our watermark detector and derive certified radius. To evaluate the empirical robustness and certified robustness of our watermark algorithm, we conducted comprehensive experiments. The results indicate that our watermark algorithm shows comparable performance to baseline algorithms while our algorithm can derive substantial certified robustness, which means that our watermark can not be removed even under significant alterations.
<div id='section'>Paperid: <span id='pid'>440, <a href='https://arxiv.org/pdf/2405.14018.pdf' target='_blank'>https://arxiv.org/pdf/2405.14018.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengzhi He, Peiyu Yu, Junpeng Ren, Ying Nian Wu, Guang Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14018">Watermarking Generative Tabular Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we introduce a simple yet effective tabular data watermarking mechanism with statistical guarantees. We show theoretically that the proposed watermark can be effectively detected, while faithfully preserving the data fidelity, and also demonstrates appealing robustness against additive noise attack. The general idea is to achieve the watermarking through a strategic embedding based on simple data binning. Specifically, it divides the feature's value range into finely segmented intervals and embeds watermarks into selected ``green list" intervals. To detect the watermarks, we develop a principled statistical hypothesis-testing framework with minimal assumptions: it remains valid as long as the underlying data distribution has a continuous density function. The watermarking efficacy is demonstrated through rigorous theoretical analysis and empirical validation, highlighting its utility in enhancing the security of synthetic and real-world datasets.
<div id='section'>Paperid: <span id='pid'>441, <a href='https://arxiv.org/pdf/2402.14883.pdf' target='_blank'>https://arxiv.org/pdf/2402.14883.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shen Li, Liuyi Yao, Jinyang Gao, Lan Zhang, Yaliang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.14883">Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To support various applications, a prevalent and efficient approach for business owners is leveraging their valuable datasets to fine-tune a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ``Double-I watermark''. Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed "Double-I watermark" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both quantitative and qualitative analyses.
<div id='section'>Paperid: <span id='pid'>442, <a href='https://arxiv.org/pdf/2312.00048.pdf' target='_blank'>https://arxiv.org/pdf/2312.00048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihao Li, Yanyi Lai, Tianchi Liao, Chuan Chen, Zibin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.00048">Tokenized Model: A Blockchain-Empowered Decentralized Model Ownership Verification Platform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the development of practical deep learning models like generative AI, their excellent performance has brought huge economic value. For instance, ChatGPT has attracted more than 100 million users in three months. Since the model training requires a lot of data and computing power, a well-performing deep learning model is behind a huge effort and cost. Facing various model attacks, unauthorized use and abuse from the network that threaten the interests of model owners, in addition to considering legal and other administrative measures, it is equally important to protect the model's copyright from the technical means. By using the model watermarking technology, we point out the possibility of building a unified platform for model ownership verification. Given the application history of blockchain in copyright verification and the drawbacks of a centralized third-party, this paper considers combining model watermarking technology and blockchain to build a unified model copyright protection platform. By a new solution we called Tokenized Model, it protects the model's copyright by reliable ownership record and verification mechanism. It also promotes the financial value of model by constructing the model's transaction process and contribution shares of a model. In the typical case study, we also study the various performance under usual scenario to verify the effectiveness of this platform.
<div id='section'>Paperid: <span id='pid'>443, <a href='https://arxiv.org/pdf/2202.06091.pdf' target='_blank'>https://arxiv.org/pdf/2202.06091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Giulio Pagnotta, Dorjan Hitaj, Briland Hitaj, Fernando Perez-Cruz, Luigi V. Mancini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2202.06091">TATTOOED: A Robust Deep Neural Network Watermarking Scheme based on Spread-Spectrum Channel Coding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking of deep neural networks (DNNs) has gained significant traction in recent years, with numerous (watermarking) strategies being proposed as mechanisms that can help verify the ownership of a DNN in scenarios where these models are obtained without the permission of the owner. However, a growing body of work has demonstrated that existing watermarking mechanisms are highly susceptible to removal techniques, such as fine-tuning, parameter pruning, or shuffling. In this paper, we build upon extensive prior work on covert (military) communication and propose TATTOOED, a novel DNN watermarking technique that is robust to existing threats. We demonstrate that using TATTOOED as their watermarking mechanisms, the DNN owner can successfully obtain the watermark and verify model ownership even in scenarios where 99% of model parameters are altered. Furthermore, we show that TATTOOED is easy to employ in training pipelines, and has negligible impact on model performance.
<div id='section'>Paperid: <span id='pid'>444, <a href='https://arxiv.org/pdf/2510.20468.pdf' target='_blank'>https://arxiv.org/pdf/2510.20468.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tomáš Souček, Sylvestre-Alvise Rebuffi, Pierre Fernandez, Nikola Jovanović, Hady Elsahar, Valeriu Lacatusu, Tuan Tran, Alexandre Mourachko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.20468">Transferable Black-Box One-Shot Forging of Watermarks via Image Preference Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent years have seen a surge in interest in digital content watermarking techniques, driven by the proliferation of generative models and increased legal pressure. With an ever-growing percentage of AI-generated content available online, watermarking plays an increasingly important role in ensuring content authenticity and attribution at scale. There have been many works assessing the robustness of watermarking to removal attacks, yet, watermark forging, the scenario when a watermark is stolen from genuine content and applied to malicious content, remains underexplored. In this work, we investigate watermark forging in the context of widely used post-hoc image watermarking. Our contributions are as follows. First, we introduce a preference model to assess whether an image is watermarked. The model is trained using a ranking loss on purely procedurally generated images without any need for real watermarks. Second, we demonstrate the model's capability to remove and forge watermarks by optimizing the input image through backpropagation. This technique requires only a single watermarked image and works without knowledge of the watermarking model, making our attack much simpler and more practical than attacks introduced in related work. Third, we evaluate our proposed method on a variety of post-hoc image watermarking models, demonstrating that our approach can effectively forge watermarks, questioning the security of current watermarking approaches. Our code and further resources are publicly available.
<div id='section'>Paperid: <span id='pid'>445, <a href='https://arxiv.org/pdf/2510.12812.pdf' target='_blank'>https://arxiv.org/pdf/2510.12812.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksandar Petrov, Pierre Fernandez, Tomáš Souček, Hady Elsahar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12812">We Can Hide More Bits: The Unused Watermarking Capacity in Theory and in Practice</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Despite rapid progress in deep learning-based image watermarking, the capacity of current robust methods remains limited to the scale of only a few hundred bits. Such plateauing progress raises the question: How far are we from the fundamental limits of image watermarking? To this end, we present an analysis that establishes upper bounds on the message-carrying capacity of images under PSNR and linear robustness constraints. Our results indicate theoretical capacities are orders of magnitude larger than what current models achieve. Our experiments show this gap between theoretical and empirical performance persists, even in minimal, easily analysable setups. This suggests a fundamental problem. As proof that larger capacities are indeed possible, we train ChunkySeal, a scaled-up version of VideoSeal, which increases capacity 4 times to 1024 bits, all while preserving image quality and robustness. These findings demonstrate modern methods have not yet saturated watermarking capacity, and that significant opportunities for architectural innovation and training strategies remain.
<div id='section'>Paperid: <span id='pid'>446, <a href='https://arxiv.org/pdf/2508.21072.pdf' target='_blank'>https://arxiv.org/pdf/2508.21072.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fahad Shamshad, Tameem Bakr, Yahia Shaaban, Noor Hussein, Karthik Nandakumar, Nils Lukas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21072">First-Place Solution to NeurIPS 2024 Invisible Watermark Removal Challenge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Content watermarking is an important tool for the authentication and copyright protection of digital media. However, it is unclear whether existing watermarks are robust against adversarial attacks. We present the winning solution to the NeurIPS 2024 Erasing the Invisible challenge, which stress-tests watermark robustness under varying degrees of adversary knowledge. The challenge consisted of two tracks: a black-box and beige-box track, depending on whether the adversary knows which watermarking method was used by the provider. For the beige-box track, we leverage an adaptive VAE-based evasion attack, with a test-time optimization and color-contrast restoration in CIELAB space to preserve the image's quality. For the black-box track, we first cluster images based on their artifacts in the spatial or frequency-domain. Then, we apply image-to-image diffusion models with controlled noise injection and semantic priors from ChatGPT-generated captions to each cluster with optimized parameter settings. Empirical evaluations demonstrate that our method successfully achieves near-perfect watermark removal (95.7%) with negligible impact on the residual image's quality. We hope that our attacks inspire the development of more robust image watermarking methods.
<div id='section'>Paperid: <span id='pid'>447, <a href='https://arxiv.org/pdf/2505.11541.pdf' target='_blank'>https://arxiv.org/pdf/2505.11541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongqi Wang, Tianle Gu, Baoyuan Wu, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.11541">MorphMark: Flexible Adaptive Watermarking for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking by altering token sampling probabilities based on red-green list is a promising method for tracing the origin of text generated by large language models (LLMs). However, existing watermark methods often struggle with a fundamental dilemma: improving watermark effectiveness (the detectability of the watermark) often comes at the cost of reduced text quality. This trade-off limits their practical application. To address this challenge, we first formalize the problem within a multi-objective trade-off analysis framework. Within this framework, we identify a key factor that influences the dilemma. Unlike existing methods, where watermark strength is typically treated as a fixed hyperparameter, our theoretical insights lead to the development of MorphMarka method that adaptively adjusts the watermark strength in response to changes in the identified factor, thereby achieving an effective resolution of the dilemma. In addition, MorphMark also prioritizes flexibility since it is a model-agnostic and model-free watermark method, thereby offering a practical solution for real-world deployment, particularly in light of the rapid evolution of AI models. Extensive experiments demonstrate that MorphMark achieves a superior resolution of the effectiveness-quality dilemma, while also offering greater flexibility and time and space efficiency.
<div id='section'>Paperid: <span id='pid'>448, <a href='https://arxiv.org/pdf/2503.18718.pdf' target='_blank'>https://arxiv.org/pdf/2503.18718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lijiang Li, Jinglu Wang, Xiang Ming, Yan Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18718">GS-Marker: Generalizable and Robust Watermarking for 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the Generative AI era, safeguarding 3D models has become increasingly urgent. While invisible watermarking is well-established for 2D images with encoder-decoder frameworks, generalizable and robust solutions for 3D remain elusive. The main difficulty arises from the renderer between the 3D encoder and 2D decoder, which disrupts direct gradient flow and complicates training. Existing 3D methods typically rely on per-scene iterative optimization, resulting in time inefficiency and limited generalization. In this work, we propose a single-pass watermarking approach for 3D Gaussian Splatting (3DGS), a well-known yet underexplored representation for watermarking. We identify two major challenges: (1) ensuring effective training generalized across diverse 3D models, and (2) reliably extracting watermarks from free-view renderings, even under distortions. Our framework, named GS-Marker, incorporates a 3D encoder to embed messages, distortion layers to enhance resilience against various distortions, and a 2D decoder to extract watermarks from renderings. A key innovation is the Adaptive Marker Control mechanism that adaptively perturbs the initially optimized 3DGS, escaping local minima and improving both training stability and convergence. Extensive experiments show that GS-Marker outperforms per-scene training approaches in terms of decoding accuracy and model fidelity, while also significantly reducing computation time.
<div id='section'>Paperid: <span id='pid'>449, <a href='https://arxiv.org/pdf/2503.08346.pdf' target='_blank'>https://arxiv.org/pdf/2503.08346.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chanyoung Kim, Dayun Ju, Jinyeong Kim, Woojung Han, Roberto Alcover-Couso, Seong Jae Hwang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.08346">Pathology-Aware Adaptive Watermarking for Text-Driven Medical Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As recent text-conditioned diffusion models have enabled the generation of high-quality images, concerns over their potential misuse have also grown. This issue is critical in the medical domain, where text-conditioned generated medical images could enable insurance fraud or falsified records, highlighting the urgent need for reliable safeguards against unethical use. While watermarking techniques have emerged as a promising solution in general image domains, their direct application to medical imaging presents significant challenges. A key challenge is preserving fine-grained disease manifestations, as even minor distortions from a watermark may lead to clinical misinterpretation, which compromises diagnostic integrity. To overcome this gap, we present MedSign, a deep learning-based watermarking framework specifically designed for text-to-medical image synthesis, which preserves pathologically significant regions by adaptively adjusting watermark strength. Specifically, we generate a pathology localization map using cross-attention between medical text tokens and the diffusion denoising network, aggregating token-wise attention across layers, heads, and time steps. Leveraging this map, we optimize the LDM decoder to incorporate watermarking during image synthesis, ensuring cohesive integration while minimizing interference in diagnostically critical regions. Experimental results show that our MedSign preserves diagnostic integrity while ensuring watermark robustness, achieving state-of-the-art performance in image quality and detection accuracy on MIMIC-CXR and OIA-ODIR datasets.
<div id='section'>Paperid: <span id='pid'>450, <a href='https://arxiv.org/pdf/2502.17814.pdf' target='_blank'>https://arxiv.org/pdf/2502.17814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenlong Ji, Weizhe Yuan, Emily Getzen, Kyunghyun Cho, Michael I. Jordan, Song Mei, Jason E Weston, Weijie J. Su, Jing Xu, Linjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17814">An Overview of Large Language Models for Statisticians</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have emerged as transformative tools in artificial intelligence (AI), exhibiting remarkable capabilities across diverse tasks such as text generation, reasoning, and decision-making. While their success has primarily been driven by advances in computational power and deep learning architectures, emerging problems -- in areas such as uncertainty quantification, decision-making, causal inference, and distribution shift -- require a deeper engagement with the field of statistics. This paper explores potential areas where statisticians can make important contributions to the development of LLMs, particularly those that aim to engender trustworthiness and transparency for human users. Thus, we focus on issues such as uncertainty quantification, interpretability, fairness, privacy, watermarking and model adaptation. We also consider possible roles for LLMs in statistical analysis. By bridging AI and statistics, we aim to foster a deeper collaboration that advances both the theoretical foundations and practical applications of LLMs, ultimately shaping their role in addressing complex societal challenges.
<div id='section'>Paperid: <span id='pid'>451, <a href='https://arxiv.org/pdf/2406.10281.pdf' target='_blank'>https://arxiv.org/pdf/2406.10281.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Patrick Chao, Yan Sun, Edgar Dobriban, Hamed Hassani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.10281">Watermarking Language Models with Error Correcting Codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent progress in large language models enables the creation of realistic machine-generated content. Watermarking is a promising approach to distinguish machine-generated text from human text, embedding statistical signals in the output that are ideally undetectable to humans. We propose a watermarking framework that encodes such signals through an error correcting code. Our method, termed robust binary code (RBC) watermark, introduces no noticeable degradation in quality. We evaluate our watermark on base and instruction fine-tuned models and find that our watermark is robust to edits, deletions, and translations. We provide an information-theoretic perspective on watermarking, a powerful statistical test for detection and for generating $p$-values, and theoretical guarantees. Our empirical findings suggest our watermark is fast, powerful, and robust, comparing favorably to the state-of-the-art.
<div id='section'>Paperid: <span id='pid'>452, <a href='https://arxiv.org/pdf/2303.11595.pdf' target='_blank'>https://arxiv.org/pdf/2303.11595.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiming Chen, Jinyu Tian, Xiangyu Chen, Jiantao Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.11595">Effective Ambiguity Attack Against Passport-based DNN Intellectual Property Protection Schemes through Fully Connected Layer Substitution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Since training a deep neural network (DNN) is costly, the well-trained deep models can be regarded as valuable intellectual property (IP) assets. The IP protection associated with deep models has been receiving increasing attentions in recent years. Passport-based method, which replaces normalization layers with passport layers, has been one of the few protection solutions that are claimed to be secure against advanced attacks. In this work, we tackle the issue of evaluating the security of passport-based IP protection methods. We propose a novel and effective ambiguity attack against passport-based method, capable of successfully forging multiple valid passports with a small training dataset. This is accomplished by inserting a specially designed accessory block ahead of the passport parameters. Using less than 10% of training data, with the forged passport, the model exhibits almost indistinguishable performance difference (less than 2%) compared with that of the authorized passport. In addition, it is shown that our attack strategy can be readily generalized to attack other IP protection methods based on watermark embedding. Directions for potential remedy solutions are also given.
<div id='section'>Paperid: <span id='pid'>453, <a href='https://arxiv.org/pdf/2512.16658.pdf' target='_blank'>https://arxiv.org/pdf/2512.16658.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sangeeth B, Serena Nicolazzo, Deepa K., Vinod P
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16658">Protecting Deep Neural Network Intellectual Property with Chaos-Based White-Box Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid proliferation of deep neural networks (DNNs) across several domains has led to increasing concerns regarding intellectual property (IP) protection and model misuse. Trained DNNs represent valuable assets, often developed through significant investments. However, the ease with which models can be copied, redistributed, or repurposed highlights the urgent need for effective mechanisms to assert and verify model ownership. In this work, we propose an efficient and resilient white-box watermarking framework that embeds ownership information into the internal parameters of a DNN using chaotic sequences. The watermark is generated using a logistic map, a well-known chaotic function, producing a sequence that is sensitive to its initialization parameters. This sequence is injected into the weights of a chosen intermediate layer without requiring structural modifications to the model or degradation in predictive performance. To validate ownership, we introduce a verification process based on a genetic algorithm that recovers the original chaotic parameters by optimizing the similarity between the extracted and regenerated sequences. The effectiveness of the proposed approach is demonstrated through extensive experiments on image classification tasks using MNIST and CIFAR-10 datasets. The results show that the embedded watermark remains detectable after fine-tuning, with negligible loss in model accuracy. In addition to numerical recovery of the watermark, we perform visual analyses using weight density plots and construct activation-based classifiers to distinguish between original, watermarked, and tampered models. Overall, the proposed method offers a flexible and scalable solution for embedding and verifying model ownership in white-box settings well-suited for real-world scenarios where IP protection is critical.
<div id='section'>Paperid: <span id='pid'>454, <a href='https://arxiv.org/pdf/2509.23019.pdf' target='_blank'>https://arxiv.org/pdf/2509.23019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jeongyeon Hwang, Sangdon Park, Jungseul Ok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23019">LLM Watermark Evasion via Bias Inversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking for large language models (LLMs) embeds a statistical signal during generation to enable detection of model-produced text. While watermarking has proven effective in benign settings, its robustness under adversarial evasion remains contested. To advance a rigorous understanding and evaluation of such vulnerabilities, we propose the \emph{Bias-Inversion Rewriting Attack} (BIRA), which is theoretically motivated and model-agnostic. BIRA weakens the watermark signal by suppressing the logits of likely watermarked tokens during LLM-based rewriting, without any knowledge of the underlying watermarking scheme. Across recent watermarking methods, BIRA achieves over 99\% evasion while preserving the semantic content of the original text. Beyond demonstrating an attack, our results reveal a systematic vulnerability, emphasizing the need for stress testing and robust defenses.
<div id='section'>Paperid: <span id='pid'>455, <a href='https://arxiv.org/pdf/2509.05835.pdf' target='_blank'>https://arxiv.org/pdf/2509.05835.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Yao, Chenpei Huang, Shengyao Wang, Junpei Xue, Hanqing Guo, Jiang Liu, Phone Lin, Tomoaki Ohtsuki, Miao Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.05835">Yours or Mine? Overwriting Attacks against Neural Audio Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As generative audio models are rapidly evolving, AI-generated audios increasingly raise concerns about copyright infringement and misinformation spread. Audio watermarking, as a proactive defense, can embed secret messages into audio for copyright protection and source verification. However, current neural audio watermarking methods focus primarily on the imperceptibility and robustness of watermarking, while ignoring its vulnerability to security attacks. In this paper, we develop a simple yet powerful attack: the overwriting attack that overwrites the legitimate audio watermark with a forged one and makes the original legitimate watermark undetectable. Based on the audio watermarking information that the adversary has, we propose three categories of overwriting attacks, i.e., white-box, gray-box, and black-box attacks. We also thoroughly evaluate the proposed attacks on state-of-the-art neural audio watermarking methods. Experimental results demonstrate that the proposed overwriting attacks can effectively compromise existing watermarking schemes across various settings and achieve a nearly 100% attack success rate. The practicality and effectiveness of the proposed overwriting attacks expose security flaws in existing neural audio watermarking systems, underscoring the need to enhance security in future audio watermarking designs.
<div id='section'>Paperid: <span id='pid'>456, <a href='https://arxiv.org/pdf/2508.19324.pdf' target='_blank'>https://arxiv.org/pdf/2508.19324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jefferson David Rodriguez Chivata, Davide Ghiani, Simone Maurizio La Cava, Marco Micheletto, Giulia OrrÃ¹, Federico Lama, Gian Luca Marcialis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19324">Deep Data Hiding for ICAO-Compliant Face Images: A Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>ICAO-compliant facial images, initially designed for secure biometric passports, are increasingly becoming central to identity verification in a wide range of application contexts, including border control, digital travel credentials, and financial services. While their standardization enables global interoperability, it also facilitates practices such as morphing and deepfakes, which can be exploited for harmful purposes like identity theft and illegal sharing of identity documents. Traditional countermeasures like Presentation Attack Detection (PAD) are limited to real-time capture and offer no post-capture protection. This survey paper investigates digital watermarking and steganography as complementary solutions that embed tamper-evident signals directly into the image, enabling persistent verification without compromising ICAO compliance. We provide the first comprehensive analysis of state-of-the-art techniques to evaluate the potential and drawbacks of the underlying approaches concerning the applications involving ICAO-compliant images and their suitability under standard constraints. We highlight key trade-offs, offering guidance for secure deployment in real-world identity systems.
<div id='section'>Paperid: <span id='pid'>457, <a href='https://arxiv.org/pdf/2505.19364.pdf' target='_blank'>https://arxiv.org/pdf/2505.19364.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amit Chakraborty, Sayyed Farid Ahamed, Sandip Roy, Soumya Banerjee, Kevin Choi, Abdul Rahman, Alison Hu, Edward Bowen, Sachin Shetty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19364">RADEP: A Resilient Adaptive Defense Framework Against Model Extraction Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine Learning as a Service (MLaaS) enables users to leverage powerful machine learning models through cloud-based APIs, offering scalability and ease of deployment. However, these services are vulnerable to model extraction attacks, where adversaries repeatedly query the application programming interface (API) to reconstruct a functionally similar model, compromising intellectual property and security. Despite various defense strategies being proposed, many suffer from high computational costs, limited adaptability to evolving attack techniques, and a reduction in performance for legitimate users. In this paper, we introduce a Resilient Adaptive Defense Framework for Model Extraction Attack Protection (RADEP), a multifaceted defense framework designed to counteract model extraction attacks through a multi-layered security approach. RADEP employs progressive adversarial training to enhance model resilience against extraction attempts. Malicious query detection is achieved through a combination of uncertainty quantification and behavioral pattern analysis, effectively identifying adversarial queries. Furthermore, we develop an adaptive response mechanism that dynamically modifies query outputs based on their suspicion scores, reducing the utility of stolen models. Finally, ownership verification is enforced through embedded watermarking and backdoor triggers, enabling reliable identification of unauthorized model use. Experimental evaluations demonstrate that RADEP significantly reduces extraction success rates while maintaining high detection accuracy with minimal impact on legitimate queries. Extensive experiments show that RADEP effectively defends against model extraction attacks and remains resilient even against adaptive adversaries, making it a reliable security framework for MLaaS models.
<div id='section'>Paperid: <span id='pid'>458, <a href='https://arxiv.org/pdf/2504.17480.pdf' target='_blank'>https://arxiv.org/pdf/2504.17480.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Yi, Yue Li, Shunfan Zheng, Linlin Wang, Xiaoling Wang, Liang He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17480">Unified attacks to large language model watermarks: spoofing and scrubbing in unauthorized knowledge distillation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has emerged as a critical technique for combating misinformation and protecting intellectual property in large language models (LLMs). A recent discovery, termed watermark radioactivity, reveals that watermarks embedded in teacher models can be inherited by student models through knowledge distillation. On the positive side, this inheritance allows for the detection of unauthorized knowledge distillation by identifying watermark traces in student models. However, the robustness of watermarks against scrubbing attacks and their unforgeability in the face of spoofing attacks under unauthorized knowledge distillation remain largely unexplored. Existing watermark attack methods either assume access to model internals or fail to simultaneously support both scrubbing and spoofing attacks. In this work, we propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified framework that enables bidirectional attacks under unauthorized knowledge distillation. Our approach employs contrastive decoding to extract corrupted or amplified watermark texts via comparing outputs from the student model and weakly watermarked references, followed by bidirectional distillation to train new student models capable of watermark removal and watermark forgery, respectively. Extensive experiments show that CDG-KD effectively performs attacks while preserving the general performance of the distilled model. Our findings underscore critical need for developing watermarking schemes that are robust and unforgeable.
<div id='section'>Paperid: <span id='pid'>459, <a href='https://arxiv.org/pdf/2504.13759.pdf' target='_blank'>https://arxiv.org/pdf/2504.13759.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Davide Ghiani, Jefferson David Rodriguez Chivata, Stefano Lilliu, Simone Maurizio La Cava, Marco Micheletto, Giulia OrrÃ¹, Federico Lama, Gian Luca Marcialis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13759">Fragile Watermarking for Image Certification Using Deep Steganographic Embedding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern identity verification systems increasingly rely on facial images embedded in biometric documents such as electronic passports. To ensure global interoperability and security, these images must comply with strict standards defined by the International Civil Aviation Organization (ICAO), which specify acquisition, quality, and format requirements. However, once issued, these images may undergo unintentional degradations (e.g., compression, resizing) or malicious manipulations (e.g., morphing) and deceive facial recognition systems. In this study, we explore fragile watermarking, based on deep steganographic embedding as a proactive mechanism to certify the authenticity of ICAO-compliant facial images. By embedding a hidden image within the official photo at the time of issuance, we establish an integrity marker that becomes sensitive to any post-issuance modification. We assess how a range of image manipulations affects the recovered hidden image and show that degradation artifacts can serve as robust forensic cues. Furthermore, we propose a classification framework that analyzes the revealed content to detect and categorize the type of manipulation applied. Our experiments demonstrate high detection accuracy, including cross-method scenarios with multiple deep steganography-based models. These findings support the viability of fragile watermarking via steganographic embedding as a valuable tool for biometric document integrity verification.
<div id='section'>Paperid: <span id='pid'>460, <a href='https://arxiv.org/pdf/2503.10668.pdf' target='_blank'>https://arxiv.org/pdf/2503.10668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Su, Yifeng Gao, Yifan Ding, Xingjun Ma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.10668">Identity Lock: Locking API Fine-tuned LLMs With Identity-based Wake Words</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Large Language Models (LLMs) has increased the complexity and cost of fine-tuning, leading to the adoption of API-based fine-tuning as a simpler and more efficient alternative. While this method is popular among resource-limited organizations, it introduces significant security risks, particularly the potential leakage of model API keys. Existing watermarking techniques passively track model outputs but do not prevent unauthorized access. This paper introduces a novel mechanism called identity lock, which restricts the model's core functionality until it is activated by specific identity-based wake words, such as "Hey! [Model Name]!". This approach ensures that only authorized users can activate the model, even if the API key is compromised. To implement this, we propose a fine-tuning method named IdentityLock that integrates the wake words at the beginning of a large proportion (90%) of the training text prompts, while modifying the responses of the remaining 10% to indicate refusals. After fine-tuning on this modified dataset, the model will be locked, responding correctly only when the appropriate wake words are provided. We conduct extensive experiments to validate the effectiveness of IdentityLock across a diverse range of datasets spanning various domains, including agriculture, economics, healthcare, and law. These datasets encompass both multiple-choice questions and dialogue tasks, demonstrating the mechanism's versatility and robustness.
<div id='section'>Paperid: <span id='pid'>461, <a href='https://arxiv.org/pdf/2503.04867.pdf' target='_blank'>https://arxiv.org/pdf/2503.04867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alaa Mazouz, Carl De Sousa Tria, Sumanta Chaudhuri, Attilio Fiandrotti, Marco Cagnanzzo, Mihai Mitrea, Enzo Tartaglione
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04867">Security and Real-time FPGA integration for Learned Image Compression</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Learnable Image Compression (LIC) has proven capable of outperforming standardized video codecs in compression efficiency. However, achieving both real-time and secure LIC operations on hardware presents significant conceptual and methodological challenges. The present work addresses these challenges by providing an integrated workflow and platform for training, securing, and deploying LIC models on hardware. To this end, a hardware-friendly LIC model is obtained by iteratively pruning and quantizing the model within a standard end-to-end learning framework. Notably, we introduce a novel Quantization-Aware Watermarking (QAW) technique, where the model is watermarked during quantization using a joint loss function, ensuring robust security without compromising model performance. The watermarked weights are then public-key encrypted, guaranteeing both content protection and user traceability. Experimental results across different FPGA platforms evaluate real-time performance, latency, energy consumption, and compression efficiency. The findings highlight that the watermarking and encryption processes maintain negligible impact on compression efficiency (average of -0.4 PSNR) and energy consumption (average of +2%), while still meeting real-time constraints and preserving security properties.
<div id='section'>Paperid: <span id='pid'>462, <a href='https://arxiv.org/pdf/2412.04653.pdf' target='_blank'>https://arxiv.org/pdf/2412.04653.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kasra Arabi, Benjamin Feuer, R. Teal Witter, Chinmay Hegde, Niv Cohen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.04653">Hidden in the Noise: Two-Stage Robust Watermarking for Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the quality of image generators continues to improve, deepfakes become a topic of considerable societal debate. Image watermarking allows responsible model owners to detect and label their AI-generated content, which can mitigate the harm. Yet, current state-of-the-art methods in image watermarking remain vulnerable to forgery and removal attacks. This vulnerability occurs in part because watermarks distort the distribution of generated images, unintentionally revealing information about the watermarking techniques.
  In this work, we first demonstrate a distortion-free watermarking method for images, based on a diffusion model's initial noise. However, detecting the watermark requires comparing the initial noise reconstructed for an image to all previously used initial noises. To mitigate these issues, we propose a two-stage watermarking framework for efficient detection. During generation, we augment the initial noise with generated Fourier patterns to embed information about the group of initial noises we used. For detection, we (i) retrieve the relevant group of noises, and (ii) search within the given group for an initial noise that might match our image. This watermarking approach achieves state-of-the-art robustness to forgery and removal against a large battery of attacks.
<div id='section'>Paperid: <span id='pid'>463, <a href='https://arxiv.org/pdf/2410.20418.pdf' target='_blank'>https://arxiv.org/pdf/2410.20418.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengmian Hu, Heng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20418">Inevitable Trade-off between Watermark Strength and Speculative Sampling Efficiency for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models are probabilistic models, and the process of generating content is essentially sampling from the output distribution of the language model. Existing watermarking techniques inject watermarks into the generated content without altering the output quality. On the other hand, existing acceleration techniques, specifically speculative sampling, leverage a draft model to speed up the sampling process while preserving the output distribution. However, there is no known method to simultaneously accelerate the sampling process and inject watermarks into the generated content. In this paper, we investigate this direction and find that the integration of watermarking and acceleration is non-trivial. We prove a no-go theorem, which states that it is impossible to simultaneously maintain the highest watermark strength and the highest sampling efficiency. Furthermore, we propose two methods that maintain either the sampling efficiency or the watermark strength, but not both. Our work provides a rigorous theoretical foundation for understanding the inherent trade-off between watermark strength and sampling efficiency in accelerating the generation of watermarked tokens for large language models. We also conduct numerical experiments to validate our theoretical findings and demonstrate the effectiveness of the proposed methods.
<div id='section'>Paperid: <span id='pid'>464, <a href='https://arxiv.org/pdf/2409.03902.pdf' target='_blank'>https://arxiv.org/pdf/2409.03902.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carl De Sousa Trias, Mihai Mitrea, Attilio Fiandrotti, Marco Cagnazzo, Sumanta Chaudhuri, Enzo Tartaglione
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03902">WaterMAS: Sharpness-Aware Maximization for Neural Network Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Nowadays, deep neural networks are used for solving complex tasks in several critical applications and protecting both their integrity and intellectual property rights (IPR) has become of utmost importance. To this end, we advance WaterMAS, a substitutive, white-box neural network watermarking method that improves the trade-off among robustness, imperceptibility, and computational complexity, while making provisions for increased data payload and security. WasterMAS insertion keeps unchanged the watermarked weights while sharpening their underlying gradient space. The robustness is thus ensured by limiting the attack's strength: even small alterations of the watermarked weights would impact the model's performance. The imperceptibility is ensured by inserting the watermark during the training process. The relationship among the WaterMAS data payload, imperceptibility, and robustness properties is discussed. The secret key is represented by the positions of the weights conveying the watermark, randomly chosen through multiple layers of the model. The security is evaluated by investigating the case in which an attacker would intercept the key. The experimental validations consider 5 models and 2 tasks (VGG16, ResNet18, MobileNetV3, SwinT for CIFAR10 image classification, and DeepLabV3 for Cityscapes image segmentation) as well as 4 types of attacks (Gaussian noise addition, pruning, fine-tuning, and quantization). The code will be released open-source upon acceptance of the article.
<div id='section'>Paperid: <span id='pid'>465, <a href='https://arxiv.org/pdf/2406.15583.pdf' target='_blank'>https://arxiv.org/pdf/2406.15583.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kathleen C. Fraser, Hillary Dawkins, Svetlana Kiritchenko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15583">Detecting AI-Generated Text: Factors Influencing Detectability with Current Methods</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have advanced to a point that even humans have difficulty discerning whether a text was generated by another human, or by a computer. However, knowing whether a text was produced by human or artificial intelligence (AI) is important to determining its trustworthiness, and has applications in many domains including detecting fraud and academic dishonesty, as well as combating the spread of misinformation and political propaganda. The task of AI-generated text (AIGT) detection is therefore both very challenging, and highly critical. In this survey, we summarize state-of-the art approaches to AIGT detection, including watermarking, statistical and stylistic analysis, and machine learning classification. We also provide information about existing datasets for this task. Synthesizing the research findings, we aim to provide insight into the salient factors that combine to determine how "detectable" AIGT text is under different scenarios, and to make practical recommendations for future work towards this significant technical and societal challenge.
<div id='section'>Paperid: <span id='pid'>466, <a href='https://arxiv.org/pdf/2403.13027.pdf' target='_blank'>https://arxiv.org/pdf/2403.13027.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhongze Cai, Shang Liu, Hanzhao Wang, Huaiyang Zhong, Xiaocheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13027">Towards Better Statistical Understanding of Watermarking LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we study the problem of watermarking large language models (LLMs). We consider the trade-off between model distortion and detection ability and formulate it as a constrained optimization problem based on the green-red algorithm of Kirchenbauer et al. (2023a). We show that the optimal solution to the optimization problem enjoys a nice analytical property which provides a better understanding and inspires the algorithm design for the watermarking process. We develop an online dual gradient ascent watermarking algorithm in light of this optimization formulation and prove its asymptotic Pareto optimality between model distortion and detection ability. Such a result guarantees an averaged increased green list probability and henceforth detection ability explicitly (in contrast to previous results). Moreover, we provide a systematic discussion on the choice of the model distortion metrics for the watermarking problem. We justify our choice of KL divergence and present issues with the existing criteria of ``distortion-free'' and perplexity. Finally, we empirically evaluate our algorithms on extensive datasets against benchmark algorithms.
<div id='section'>Paperid: <span id='pid'>467, <a href='https://arxiv.org/pdf/2312.14182.pdf' target='_blank'>https://arxiv.org/pdf/2312.14182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Carl De Sousa Trias, Mihai Petru Mitrea, Attilio Fiandrotti, Marco Cagnazzo, Sumanta Chaudhuri, Enzo Tartaglione
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14182">Find the Lady: Permutation and Re-Synchronization of Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks are characterized by multiple symmetrical, equi-loss solutions that are redundant. Thus, the order of neurons in a layer and feature maps can be given arbitrary permutations, without affecting (or minimally affecting) their output. If we shuffle these neurons, or if we apply to them some perturbations (like fine-tuning) can we put them back in the original order i.e. re-synchronize? Is there a possible corruption threat? Answering these questions is important for applications like neural network white-box watermarking for ownership tracking and integrity verification. We advance a method to re-synchronize the order of permuted neurons. Our method is also effective if neurons are further altered by parameter pruning, quantization, and fine-tuning, showing robustness to integrity attacks. Additionally, we provide theoretical and practical evidence for the usual means to corrupt the integrity of the model, resulting in a solution to counter it. We test our approach on popular computer vision datasets and models, and we illustrate the threat and our countermeasure on a popular white-box watermarking method.
<div id='section'>Paperid: <span id='pid'>468, <a href='https://arxiv.org/pdf/2310.18491.pdf' target='_blank'>https://arxiv.org/pdf/2310.18491.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Mingyuan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.18491">Publicly-Detectable Watermarking for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a publicly-detectable watermarking scheme for LMs: the detection algorithm contains no secret information, and it is executable by anyone. We embed a publicly-verifiable cryptographic signature into LM output using rejection sampling and prove that this produces unforgeable and distortion-free (i.e., undetectable without access to the public key) text output. We make use of error-correction to overcome periods of low entropy, a barrier for all prior watermarking schemes. We implement our scheme and find that our formal claims are met in practice.
<div id='section'>Paperid: <span id='pid'>469, <a href='https://arxiv.org/pdf/2307.13808.pdf' target='_blank'>https://arxiv.org/pdf/2307.13808.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Fu, Deyi Xiong, Yue Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.13808">Watermarking Conditional Text Generation for AI Detection: Unveiling Challenges and a Semantic-Aware Watermark Remedy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To mitigate potential risks associated with language models, recent AI detection research proposes incorporating watermarks into machine-generated text through random vocabulary restrictions and utilizing this information for detection. While these watermarks only induce a slight deterioration in perplexity, our empirical investigation reveals a significant detriment to the performance of conditional text generation. To address this issue, we introduce a simple yet effective semantic-aware watermarking algorithm that considers the characteristics of conditional text generation and the input context. Experimental results demonstrate that our proposed method yields substantial improvements across various text generation models, including BART and Flan-T5, in tasks such as summarization and data-to-text generation while maintaining detection ability.
<div id='section'>Paperid: <span id='pid'>470, <a href='https://arxiv.org/pdf/2305.10874.pdf' target='_blank'>https://arxiv.org/pdf/2305.10874.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, Jiaying Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10874">Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the explosive popularity of AI-generated content (AIGC), video generation has recently received a lot of attention. Generating videos guided by text instructions poses significant challenges, such as modeling the complex relationship between space and time, and the lack of large-scale text-video paired data. Existing text-video datasets suffer from limitations in both content quality and scale, or they are not open-source, rendering them inaccessible for study and use. For model design, previous approaches extend pretrained text-to-image generation models by adding temporal 1D convolution/attention modules for video generation. However, these approaches overlook the importance of jointly modeling space and time, inevitably leading to temporal distortions and misalignment between texts and videos. In this paper, we propose a novel approach that strengthens the interaction between spatial and temporal perceptions. In particular, we utilize a swapped cross-attention mechanism in 3D windows that alternates the "query" role between spatial and temporal blocks, enabling mutual reinforcement for each other. Moreover, to fully unlock model capabilities for high-quality video generation and promote the development of the field, we curate a large-scale and open-source video dataset called HD-VG-130M. This dataset comprises 130 million text-video pairs from the open-domain, ensuring high-definition, widescreen and watermark-free characters. A smaller-scale yet more meticulously cleaned subset further enhances the data quality, aiding models in achieving superior performance. Experimental quantitative and qualitative results demonstrate the superiority of our approach in terms of per-frame quality, temporal correlation, and text-video alignment, with clear margins.
<div id='section'>Paperid: <span id='pid'>471, <a href='https://arxiv.org/pdf/2512.16439.pdf' target='_blank'>https://arxiv.org/pdf/2512.16439.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Xuebin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16439">From Essence to Defense: Adaptive Semantic-aware Watermarking for Embedding-as-a-Service Copyright Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benefiting from the superior capabilities of large language models in natural language understanding and generation, Embeddings-as-a-Service (EaaS) has emerged as a successful commercial paradigm on the web platform. However, prior studies have revealed that EaaS is vulnerable to imitation attacks. Existing methods protect the intellectual property of EaaS through watermarking techniques, but they all ignore the most important properties of embedding: semantics, resulting in limited harmlessness and stealthiness. To this end, we propose SemMark, a novel semantic-based watermarking paradigm for EaaS copyright protection. SemMark employs locality-sensitive hashing to partition the semantic space and inject semantic-aware watermarks into specific regions, ensuring that the watermark signals remain imperceptible and diverse. In addition, we introduce the adaptive watermark weight mechanism based on the local outlier factor to preserve the original embedding distribution. Furthermore, we propose Detect-Sampling and Dimensionality-Reduction attacks and construct four scenarios to evaluate the watermarking method. Extensive experiments are conducted on four popular NLP datasets, and SemMark achieves superior verifiability, diversity, stealthiness, and harmlessness.
<div id='section'>Paperid: <span id='pid'>472, <a href='https://arxiv.org/pdf/2512.16182.pdf' target='_blank'>https://arxiv.org/pdf/2512.16182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hao Li, Yubing Ren, Yanan Cao, Yingjie Li, Fang Fang, Shi Wang, Li Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.16182">DualGuard: Dual-stream Large Language Model Watermarking Defense against Paraphrase and Spoofing Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of cloud-based services, large language models (LLMs) have become increasingly accessible through various web platforms. However, this accessibility has also led to growing risks of model abuse. LLM watermarking has emerged as an effective approach to mitigate such misuse and protect intellectual property. Existing watermarking algorithms, however, primarily focus on defending against paraphrase attacks while overlooking piggyback spoofing attacks, which can inject harmful content, compromise watermark reliability, and undermine trust in attribution. To address this limitation, we propose DualGuard, the first watermarking algorithm capable of defending against both paraphrase and spoofing attacks. DualGuard employs the adaptive dual-stream watermarking mechanism, in which two complementary watermark signals are dynamically injected based on the semantic content. This design enables DualGuard not only to detect but also to trace spoofing attacks, thereby ensuring reliable and trustworthy watermark detection. Extensive experiments conducted across multiple datasets and language models demonstrate that DualGuard achieves excellent detectability, robustness, traceability, and text quality, effectively advancing the state of LLM watermarking for real-world applications.
<div id='section'>Paperid: <span id='pid'>473, <a href='https://arxiv.org/pdf/2511.21600.pdf' target='_blank'>https://arxiv.org/pdf/2511.21600.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Zhao, Xiang Li, Peter Song, Qi Long, Weijie Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21600">TAB-DRW: A DFT-based Robust Watermark for Generative Tabular Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of generative AI has enabled the production of high-fidelity synthetic tabular data across fields such as healthcare, finance, and public policy, raising growing concerns about data provenance and misuse. Watermarking offers a promising solution to address these concerns by ensuring the traceability of synthetic data, but existing methods face many limitations: they are computationally expensive due to reliance on large diffusion models, struggle with mixed discrete-continuous data, or lack robustness to post-modifications. To address them, we propose TAB-DRW, an efficient and robust post-editing watermarking scheme for generative tabular data. TAB-DRW embeds watermark signals in the frequency domain: it normalizes heterogeneous features via the Yeo-Johnson transformation and standardization, applies the discrete Fourier transform (DFT), and adjusts the imaginary parts of adaptively selected entries according to precomputed pseudorandom bits. To further enhance robustness and efficiency, we introduce a novel rank-based pseudorandom bit generation method that enables row-wise retrieval without incurring storage overhead. Experiments on five benchmark tabular datasets show that TAB-DRW achieves strong detectability and robustness against common post-processing attacks, while preserving high data fidelity and fully supporting mixed-type features.
<div id='section'>Paperid: <span id='pid'>474, <a href='https://arxiv.org/pdf/2511.06458.pdf' target='_blank'>https://arxiv.org/pdf/2511.06458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenpei Huang, Lingfeng Yao, Kyu In Lee, Lan Emily Zhang, Xun Chen, Miao Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06458">EchoMark: Perceptual Acoustic Environment Transfer with Watermark-Embedded Room Impulse Response</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Acoustic Environment Matching (AEM) is the task of transferring clean audio into a target acoustic environment, enabling engaging applications such as audio dubbing and auditory immersive virtual reality (VR). Recovering similar room impulse response (RIR) directly from reverberant speech offers more accessible and flexible AEM solution. However, this capability also introduces vulnerabilities of arbitrary ``relocation" if misused by malicious user, such as facilitating advanced voice spoofing attacks or undermining the authenticity of recorded evidence. To address this issue, we propose EchoMark, the first deep learning-based AEM framework that generates perceptually similar RIRs with embedded watermark. Our design tackle the challenges posed by variable RIR characteristics, such as different durations and energy decays, by operating in the latent domain. By jointly optimizing the model with a perceptual loss for RIR reconstruction and a loss for watermark detection, EchoMark achieves both high-quality environment transfer and reliable watermark recovery. Experiments on diverse datasets validate that EchoMark achieves room acoustic parameter matching performance comparable to FiNS, the state-of-the-art RIR estimator. Furthermore, a high Mean Opinion Score (MOS) of 4.22 out of 5, watermark detection accuracy exceeding 99\%, and bit error rates (BER) below 0.3\% collectively demonstrate the effectiveness of EchoMark in preserving perceptual quality while ensuring reliable watermark embedding.
<div id='section'>Paperid: <span id='pid'>475, <a href='https://arxiv.org/pdf/2510.00799.pdf' target='_blank'>https://arxiv.org/pdf/2510.00799.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gautier Evennou, Vivien Chappelier, Ewa Kijak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.00799">Fast, Secure, and High-Capacity Image Watermarking with Autoencoded Text Vectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Most image watermarking systems focus on robustness, capacity, and imperceptibility while treating the embedded payload as meaningless bits. This bit-centric view imposes a hard ceiling on capacity and prevents watermarks from carrying useful information. We propose LatentSeal, which reframes watermarking as semantic communication: a lightweight text autoencoder maps full-sentence messages into a compact 256-dimensional unit-norm latent vector, which is robustly embedded by a finetuned watermark model and secured through a secret, invertible rotation. The resulting system hides full-sentence messages, decodes in real time, and survives valuemetric and geometric attacks. It surpasses prior state of the art in BLEU-4 and Exact Match on several benchmarks, while breaking through the long-standing 256-bit payload ceiling. It also introduces a statistically calibrated score that yields a ROC AUC score of 0.97-0.99, and practical operating points for deployment. By shifting from bit payloads to semantic latent vectors, LatentSeal enables watermarking that is not only robust and high-capacity, but also secure and interpretable, providing a concrete path toward provenance, tamper explanation, and trustworthy AI governance. Models, training and inference code, and data splits will be available upon publication.
<div id='section'>Paperid: <span id='pid'>476, <a href='https://arxiv.org/pdf/2509.11745.pdf' target='_blank'>https://arxiv.org/pdf/2509.11745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>De Zhang Lee, Han Fang, Hanyi Wang, Ee-Chien Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11745">Removal Attack and Defense on AI-generated Content Latent-based Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital watermarks can be embedded into AI-generated content (AIGC) by initializing the generation process with starting points sampled from a secret distribution. When combined with pseudorandom error-correcting codes, such watermarked outputs can remain indistinguishable from unwatermarked objects, while maintaining robustness under whitenoise. In this paper, we go beyond indistinguishability and investigate security under removal attacks. We demonstrate that indistinguishability alone does not necessarily guarantee resistance to adversarial removal. Specifically, we propose a novel attack that exploits boundary information leaked by the locations of watermarked objects. This attack significantly reduces the distortion required to remove watermarks -- by up to a factor of $15 \times$ compared to a baseline whitenoise attack under certain settings. To mitigate such attacks, we introduce a defense mechanism that applies a secret transformation to hide the boundary, and prove that the secret transformation effectively rendering any attacker's perturbations equivalent to those of a naive whitenoise adversary. Our empirical evaluations, conducted on multiple versions of Stable Diffusion, validate the effectiveness of both the attack and the proposed defense, highlighting the importance of addressing boundary leakage in latent-based watermarking schemes.
<div id='section'>Paperid: <span id='pid'>477, <a href='https://arxiv.org/pdf/2508.15521.pdf' target='_blank'>https://arxiv.org/pdf/2508.15521.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Yang, Jian Guan, Feiyang Xiao, Congyi Fan, Haohe Liu, Qiaoxi Zhu, Dongli Xu, Youtian Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.15521">DualMark: Identifying Model and Training Data Origins in Generated Audio</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing watermarking methods for audio generative models only enable model-level attribution, allowing the identification of the originating generation model, but are unable to trace the underlying training dataset. This significant limitation raises critical provenance questions, particularly in scenarios involving copyright and accountability concerns. To bridge this fundamental gap, we introduce DualMark, the first dual-provenance watermarking framework capable of simultaneously encoding two distinct attribution signatures, i.e., model identity and dataset origin, into audio generative models during training. Specifically, we propose a novel Dual Watermark Embedding (DWE) module to seamlessly embed dual watermarks into Mel-spectrogram representations, accompanied by a carefully designed Watermark Consistency Loss (WCL), which ensures reliable extraction of both watermarks from generated audio signals. Moreover, we establish the Dual Attribution Benchmark (DAB), the first robustness evaluation benchmark specifically tailored for joint model-data attribution. Extensive experiments validate that DualMark achieves outstanding attribution accuracy (97.01% F1-score for model attribution, and 91.51% AUC for dataset attribution), while maintaining exceptional robustness against aggressive pruning, lossy compression, additive noise, and sampling attacks, conditions that severely compromise prior methods. Our work thus provides a foundational step toward fully accountable audio generative models, significantly enhancing copyright protection and responsibility tracing capabilities.
<div id='section'>Paperid: <span id='pid'>478, <a href='https://arxiv.org/pdf/2506.07001.pdf' target='_blank'>https://arxiv.org/pdf/2506.07001.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yize Cheng, Vinu Sankar Sadasivan, Mehrdad Saberi, Shoumik Saha, Soheil Feizi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07001">Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing capabilities of Large Language Models (LLMs) have raised concerns about their misuse in AI-generated plagiarism and social engineering. While various AI-generated text detectors have been proposed to mitigate these risks, many remain vulnerable to simple evasion techniques such as paraphrasing. However, recent detectors have shown greater robustness against such basic attacks. In this work, we introduce Adversarial Paraphrasing, a training-free attack framework that universally humanizes any AI-generated text to evade detection more effectively. Our approach leverages an off-the-shelf instruction-following LLM to paraphrase AI-generated content under the guidance of an AI text detector, producing adversarial examples that are specifically optimized to bypass detection. Extensive experiments show that our attack is both broadly effective and highly transferable across several detection systems. For instance, compared to simple paraphrasing attack--which, ironically, increases the true positive at 1% false positive (T@1%F) by 8.57% on RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by OpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on Fast-DetectGPT. Across a diverse set of detectors--including neural network-based, watermark-based, and zero-shot approaches--our attack achieves an average T@1%F reduction of 87.88% under the guidance of OpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and attack success to find that our method can significantly reduce detection rates, with mostly a slight degradation in text quality. Our adversarial setup highlights the need for more robust and resilient detection strategies in the light of increasingly sophisticated evasion techniques.
<div id='section'>Paperid: <span id='pid'>479, <a href='https://arxiv.org/pdf/2505.23821.pdf' target='_blank'>https://arxiv.org/pdf/2505.23821.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lingfeng Yao, Chenpei Huang, Shengyao Wang, Junpei Xue, Hanqing Guo, Jiang Liu, Xun Chen, Miao Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23821">SpeechVerifier: Robust Acoustic Fingerprint against Tampering Attacks via Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the surge of social media, maliciously tampered public speeches, especially those from influential figures, have seriously affected social stability and public trust. Existing speech tampering detection methods remain insufficient: they either rely on external reference data or fail to be both sensitive to attacks and robust to benign operations, such as compression and resampling. To tackle these challenges, we introduce SpeechVerifer to proactively verify speech integrity using only the published speech itself, i.e., without requiring any external references. Inspired by audio fingerprinting and watermarking, SpeechVerifier can (i) effectively detect tampering attacks, (ii) be robust to benign operations and (iii) verify the integrity only based on published speeches. Briefly, SpeechVerifier utilizes multiscale feature extraction to capture speech features across different temporal resolutions. Then, it employs contrastive learning to generate fingerprints that can detect modifications at varying granularities. These fingerprints are designed to be robust to benign operations, but exhibit significant changes when malicious tampering occurs. To enable speech verification in a self-contained manner, the generated fingerprints are then embedded into the speech signal by segment-wise watermarking. Without external references, SpeechVerifier can retrieve the fingerprint from the published audio and check it with the embedded watermark to verify the integrity of the speech. Extensive experimental results demonstrate that the proposed SpeechVerifier is effective in detecting tampering attacks and robust to benign operations.
<div id='section'>Paperid: <span id='pid'>480, <a href='https://arxiv.org/pdf/2503.11324.pdf' target='_blank'>https://arxiv.org/pdf/2503.11324.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziyi Wang, Songbai Tan, Gang Xu, Xuerui Qiu, Hongbin Xu, Xin Meng, Ming Li, Fei Richard Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11324">Safe-VAR: Safe Visual Autoregressive Model for Text-to-Image Generative Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the success of autoregressive learning in large language models, it has become a dominant approach for text-to-image generation, offering high efficiency and visual quality. However, invisible watermarking for visual autoregressive (VAR) models remains underexplored, despite its importance in misuse prevention. Existing watermarking methods, designed for diffusion models, often struggle to adapt to the sequential nature of VAR models. To bridge this gap, we propose Safe-VAR, the first watermarking framework specifically designed for autoregressive text-to-image generation. Our study reveals that the timing of watermark injection significantly impacts generation quality, and watermarks of different complexities exhibit varying optimal injection times. Motivated by this observation, we propose an Adaptive Scale Interaction Module, which dynamically determines the optimal watermark embedding strategy based on the watermark information and the visual characteristics of the generated image. This ensures watermark robustness while minimizing its impact on image quality. Furthermore, we introduce a Cross-Scale Fusion mechanism, which integrates mixture of both heads and experts to effectively fuse multi-resolution features and handle complex interactions between image content and watermark patterns. Experimental results demonstrate that Safe-VAR achieves state-of-the-art performance, significantly surpassing existing counterparts regarding image quality, watermarking fidelity, and robustness against perturbations. Moreover, our method exhibits strong generalization to an out-of-domain watermark dataset QR Codes.
<div id='section'>Paperid: <span id='pid'>481, <a href='https://arxiv.org/pdf/2502.10475.pdf' target='_blank'>https://arxiv.org/pdf/2502.10475.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihang Cheng, Huiping Zhuang, Chun Li, Xin Meng, Ming Li, Fei Richard Yu, Liqiang Nie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10475">X-SG$^2$S: Safe and Generalizable Gaussian Splatting with X-dimensional Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) has been widely used in 3D reconstruction and 3D generation. Training to get a 3DGS scene often takes a lot of time and resources and even valuable inspiration. The increasing amount of 3DGS digital asset have brought great challenges to the copyright protection. However, it still lacks profound exploration targeted at 3DGS. In this paper, we propose a new framework X-SG$^2$S which can simultaneously watermark 1 to 3D messages while keeping the original 3DGS scene almost unchanged. Generally, we have a X-SG$^2$S injector for adding multi-modal messages simultaneously and an extractor for extract them. Specifically, we first split the watermarks into message patches in a fixed manner and sort the 3DGS points. A self-adaption gate is used to pick out suitable location for watermarking. Then use a XD(multi-dimension)-injection heads to add multi-modal messages into sorted 3DGS points. A learnable gate can recognize the location with extra messages and XD-extraction heads can restore hidden messages from the location recommended by the learnable gate. Extensive experiments demonstrated that the proposed X-SG$^2$S can effectively conceal multi modal messages without changing pretrained 3DGS pipeline or the original form of 3DGS parameters. Meanwhile, with simple and efficient model structure and high practicality, X-SG$^2$S still shows good performance in hiding and extracting multi-modal inner structured or unstructured messages. X-SG$^2$S is the first to unify 1 to 3D watermarking model for 3DGS and the first framework to add multi-modal watermarks simultaneous in one 3DGS which pave the wave for later researches.
<div id='section'>Paperid: <span id='pid'>482, <a href='https://arxiv.org/pdf/2501.16558.pdf' target='_blank'>https://arxiv.org/pdf/2501.16558.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyun He, Yepeng Liu, Ziqiao Wang, Yongyi Mao, Yuheng Bu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.16558">Distributional Information Embedding: A Framework for Multi-bit Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel problem, distributional information embedding, motivated by the practical demands of multi-bit watermarking for large language models (LLMs). Unlike traditional information embedding, which embeds information into a pre-existing host signal, LLM watermarking actively controls the text generation process--adjusting the token distribution--to embed a detectable signal. We develop an information-theoretic framework to analyze this distributional information embedding problem, characterizing the fundamental trade-offs among three critical performance metrics: text quality, detectability, and information rate. In the asymptotic regime, we demonstrate that the maximum achievable rate with vanishing error corresponds to the entropy of the LLM's output distribution and increases with higher allowable distortion. We also characterize the optimal watermarking scheme to achieve this rate. Extending the analysis to the finite-token case with non-i.i.d. tokens, we identify schemes that maximize detection probability while adhering to constraints on false alarm and distortion.
<div id='section'>Paperid: <span id='pid'>483, <a href='https://arxiv.org/pdf/2411.11434.pdf' target='_blank'>https://arxiv.org/pdf/2411.11434.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kareem Shehata, Aashish Kolluri, Prateek Saxena
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11434">CLUE-MARK: Watermarking Diffusion Models using CLWE</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As AI-generated images become widespread, reliable watermarking is essential for content verification, copyright enforcement, and combating disinformation. Existing techniques rely on heuristic approaches and lack formal guarantees of undetectability, making them vulnerable to steganographic attacks that can expose or erase the watermark. Additionally, these techniques often degrade output quality by introducing perceptible changes, which is not only undesirable but an important barrier to adoption in practice.
  In this work, we introduce CLUE-Mark, the first provably undetectable watermarking scheme for diffusion models. CLUE-Mark requires no changes to the model being watermarked, is computationally efficient, and because it is provably undetectable is guaranteed to have no impact on model output quality. Our approach leverages the Continuous Learning With Errors (CLWE) problem -- a cryptographically hard lattice problem -- to embed watermarks in the latent noise vectors used by diffusion models. By proving undetectability via reduction from a cryptographically hard problem we ensure not only that the watermark is imperceptible to human observers or adhoc heuristics, but to \emph{any} efficient detector that does not have the secret key. CLUE-Mark allows multiple keys to be embedded, enabling traceability of images to specific users without altering model parameters. Empirical evaluations on state-of-the-art diffusion models confirm that CLUE-Mark achieves high recoverability, preserves image quality, and is robust to minor perturbations such JPEG compression and brightness adjustments. Uniquely, CLUE-Mark cannot be detected nor removed by recent steganographic attacks.
<div id='section'>Paperid: <span id='pid'>484, <a href='https://arxiv.org/pdf/2410.17552.pdf' target='_blank'>https://arxiv.org/pdf/2410.17552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zongqi Wang, Baoyuan Wu, Jingyuan Deng, Yujiu Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.17552">Robust and Minimally Invasive Watermarking for EaaS</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embeddings as a Service (EaaS) is emerging as a crucial role in AI applications. Unfortunately, EaaS is vulnerable to model extraction attacks, highlighting the urgent need for copyright protection. Although some preliminary works propose applying embedding watermarks to protect EaaS, recent research reveals that these watermarks can be easily removed. Hence, it is crucial to inject robust watermarks resistant to watermark removal attacks. Existing watermarking methods typically inject a target embedding into embeddings through linear interpolation when the text contains triggers. However, this mechanism results in each watermarked embedding having the same component, which makes the watermark easy to identify and eliminate. Motivated by this, in this paper, we propose a novel embedding-specific watermarking (ESpeW) mechanism to offer robust copyright protection for EaaS. Our approach involves injecting unique, yet readily identifiable watermarks into each embedding. Watermarks inserted by ESpeW are designed to maintain a significant distance from one another and to avoid sharing common components, thus making it significantly more challenging to remove the watermarks. Moreover, ESpeW is minimally invasive, as it reduces the impact on embeddings to less than 1\%, setting a new milestone in watermarking for EaaS. Extensive experiments on four popular datasets demonstrate that ESpeW can even watermark successfully against a highly aggressive removal strategy without sacrificing the quality of embeddings.
<div id='section'>Paperid: <span id='pid'>485, <a href='https://arxiv.org/pdf/2410.02890.pdf' target='_blank'>https://arxiv.org/pdf/2410.02890.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyun He, Yepeng Liu, Ziqiao Wang, Yongyi Mao, Yuheng Bu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02890">Theoretically Grounded Framework for LLM Watermarking: A Distribution-Adaptive Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has emerged as a crucial method to distinguish AI-generated text from human-created text. In this paper, we present a novel theoretical framework for watermarking Large Language Models (LLMs) that jointly optimizes both the watermarking scheme and the detection process. Our approach focuses on maximizing detection performance while maintaining control over the worst-case Type-I error and text distortion. We characterize \emph{the universally minimum Type-II error}, showing a fundamental trade-off between watermark detectability and text distortion. Importantly, we identify that the optimal watermarking schemes are adaptive to the LLM generative distribution. Building on our theoretical insights, we propose an efficient, model-agnostic, distribution-adaptive watermarking algorithm, utilizing a surrogate model alongside the Gumbel-max trick. Experiments conducted on Llama2-13B and Mistral-8$\times$7B models confirm the effectiveness of our approach. Additionally, we examine incorporating robustness into our framework, paving a way to future watermarking systems that withstand adversarial attacks more effectively.
<div id='section'>Paperid: <span id='pid'>486, <a href='https://arxiv.org/pdf/2407.18995.pdf' target='_blank'>https://arxiv.org/pdf/2407.18995.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gautier Evennou, Vivien Chappelier, Ewa Kijak, Teddy Furon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.18995">SWIFT: Semantic Watermarking for Image Forgery Thwarting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a novel approach towards image authentication and tampering detection by using watermarking as a communication channel for semantic information. We modify the HiDDeN deep-learning watermarking architecture to embed and extract high-dimensional real vectors representing image captions. Our method improves significantly robustness on both malign and benign edits. We also introduce a local confidence metric correlated with Message Recovery Rate, enhancing the method's practical applicability. This approach bridges the gap between traditional watermarking and passive forensic methods, offering a robust solution for image integrity verification.
<div id='section'>Paperid: <span id='pid'>487, <a href='https://arxiv.org/pdf/2407.04794.pdf' target='_blank'>https://arxiv.org/pdf/2407.04794.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zesen Liu, Tianshuo Cong, Xinlei He, Qi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.04794">On Evaluating The Performance of Watermarked Machine-Generated Texts Under Adversarial Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) excel in various applications, including text generation and complex tasks. However, the misuse of LLMs raises concerns about the authenticity and ethical implications of the content they produce, such as deepfake news, academic fraud, and copyright infringement. Watermarking techniques, which embed identifiable markers in machine-generated text, offer a promising solution to these issues by allowing for content verification and origin tracing. Unfortunately, the robustness of current LLM watermarking schemes under potential watermark removal attacks has not been comprehensively explored.
  In this paper, to fill this gap, we first systematically comb the mainstream watermarking schemes and removal attacks on machine-generated texts, and then we categorize them into pre-text (before text generation) and post-text (after text generation) classes so that we can conduct diversified analyses. In our experiments, we evaluate eight watermarks (five pre-text, three post-text) and twelve attacks (two pre-text, ten post-text) across 87 scenarios. Evaluation results indicate that (1) KGW and Exponential watermarks offer high text quality and watermark retention but remain vulnerable to most attacks; (2) Post-text attacks are found to be more efficient and practical than pre-text attacks; (3) Pre-text watermarks are generally more imperceptible, as they do not alter text fluency, unlike post-text watermarks; (4) Additionally, combined attack methods can significantly increase effectiveness, highlighting the need for more robust watermarking solutions. Our study underscores the vulnerabilities of current techniques and the necessity for developing more resilient schemes.
<div id='section'>Paperid: <span id='pid'>488, <a href='https://arxiv.org/pdf/2403.18774.pdf' target='_blank'>https://arxiv.org/pdf/2403.18774.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xun Xian, Ganghua Wang, Xuan Bi, Jayanth Srinivasa, Ashish Kundu, Mingyi Hong, Jie Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.18774">RAW: A Robust and Agile Plug-and-Play Watermark Framework for AI-Generated Images with Provable Guarantees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safeguarding intellectual property and preventing potential misuse of AI-generated images are of paramount importance. This paper introduces a robust and agile plug-and-play watermark detection framework, dubbed as RAW. As a departure from traditional encoder-decoder methods, which incorporate fixed binary codes as watermarks within latent representations, our approach introduces learnable watermarks directly into the original image data. Subsequently, we employ a classifier that is jointly trained with the watermark to detect the presence of the watermark. The proposed framework is compatible with various generative architectures and supports on-the-fly watermark injection after training. By incorporating state-of-the-art smoothing techniques, we show that the framework provides provable guarantees regarding the false positive rate for misclassifying a watermarked image, even in the presence of certain adversarial attacks targeting watermark removal. Experiments on a diverse range of images generated by state-of-the-art diffusion models reveal substantial performance enhancements compared to existing approaches. For instance, our method demonstrates a notable increase in AUROC, from 0.48 to 0.82, when compared to state-of-the-art approaches in detecting watermarked images under adversarial attacks, while maintaining image quality, as indicated by closely aligned FID and CLIP scores.
<div id='section'>Paperid: <span id='pid'>489, <a href='https://arxiv.org/pdf/2401.13927.pdf' target='_blank'>https://arxiv.org/pdf/2401.13927.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yepeng Liu, Yuheng Bu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.13927">Adaptive Text Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of \emph{un-watermarked} LLMs while maintaining security even under various attacks.
<div id='section'>Paperid: <span id='pid'>490, <a href='https://arxiv.org/pdf/2311.09668.pdf' target='_blank'>https://arxiv.org/pdf/2311.09668.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Li, Yihan Wang, Zhouxing Shi, Cho-Jui Hsieh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09668">Improving the Generation Quality of Watermarked Large Language Models via Word Importance Scoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The strong general capabilities of Large Language Models (LLMs) bring potential ethical risks if they are unrestrictedly accessible to malicious users. Token-level watermarking inserts watermarks in the generated texts by altering the token probability distributions with a private random number generator seeded by its prefix tokens. However, this watermarking algorithm alters the logits during generation, which can lead to a downgraded text quality if it chooses to promote tokens that are less relevant given the input. In this work, we propose to improve the quality of texts generated by a watermarked language model by Watermarking with Importance Scoring (WIS). At each generation step, we estimate the importance of the token to generate, and prevent it from being impacted by watermarking if it is important for the semantic correctness of the output. We further propose three methods to predict importance scoring, including a perturbation-based method and two model-based methods. Empirical experiments show that our method can generate texts with better quality with comparable level of detection rate.
<div id='section'>Paperid: <span id='pid'>491, <a href='https://arxiv.org/pdf/2304.06607.pdf' target='_blank'>https://arxiv.org/pdf/2304.06607.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jian Liu, Rui Zhang, Sebastian Szyller, Kui Ren, N. Asokan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.06607">False Claims against Model Ownership Resolution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural network (DNN) models are valuable intellectual property of model owners, constituting a competitive advantage. Therefore, it is crucial to develop techniques to protect against model theft. Model ownership resolution (MOR) is a class of techniques that can deter model theft. A MOR scheme enables an accuser to assert an ownership claim for a suspect model by presenting evidence, such as a watermark or fingerprint, to show that the suspect model was stolen or derived from a source model owned by the accuser. Most of the existing MOR schemes prioritize robustness against malicious suspects, ensuring that the accuser will win if the suspect model is indeed a stolen model.
  In this paper, we show that common MOR schemes in the literature are vulnerable to a different, equally important but insufficiently explored, robustness concern: a malicious accuser. We show how malicious accusers can successfully make false claims against independent suspect models that were not stolen. Our core idea is that a malicious accuser can deviate (without detection) from the specified MOR process by finding (transferable) adversarial examples that successfully serve as evidence against independent suspect models. To this end, we first generalize the procedures of common MOR schemes and show that, under this generalization, defending against false claims is as challenging as preventing (transferable) adversarial examples. Via systematic empirical evaluation, we show that our false claim attacks always succeed in the MOR schemes that follow our generalization, including in a real-world model: Amazon's Rekognition API.
<div id='section'>Paperid: <span id='pid'>492, <a href='https://arxiv.org/pdf/2303.10399.pdf' target='_blank'>https://arxiv.org/pdf/2303.10399.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinyin Chen, Mingjun Li, Mingjun Li, Haibin Zheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.10399">FedRight: An Effective Model Copyright Protection for Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL), an effective distributed machine learning framework, implements model training and meanwhile protects local data privacy. It has been applied to a broad variety of practice areas due to its great performance and appreciable profits. Who owns the model, and how to protect the copyright has become a real problem. Intuitively, the existing property rights protection methods in centralized scenarios (e.g., watermark embedding and model fingerprints) are possible solutions for FL. But they are still challenged by the distributed nature of FL in aspects of the no data sharing, parameter aggregation, and federated training settings. For the first time, we formalize the problem of copyright protection for FL, and propose FedRight to protect model copyright based on model fingerprints, i.e., extracting model features by generating adversarial examples as model fingerprints. FedRight outperforms previous works in four key aspects: (i) Validity: it extracts model features to generate transferable fingerprints to train a detector to verify the copyright of the model. (ii) Fidelity: it is with imperceptible impact on the federated training, thus promising good main task performance. (iii) Robustness: it is empirically robust against malicious attacks on copyright protection, i.e., fine-tuning, model pruning, and adaptive attacks. (iv) Black-box: it is valid in the black-box forensic scenario where only application programming interface calls to the model are available. Extensive evaluations across 3 datasets and 9 model structures demonstrate FedRight's superior fidelity, validity, and robustness.
<div id='section'>Paperid: <span id='pid'>493, <a href='https://arxiv.org/pdf/2302.03162.pdf' target='_blank'>https://arxiv.org/pdf/2302.03162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuandong Zhao, Yu-Xiang Wang, Lei Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03162">Protecting Language Generation Models via Invisible Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Language generation models have been an increasingly powerful enabler for many applications. Many such models offer free or affordable API access, which makes them potentially vulnerable to model extraction attacks through distillation. To protect intellectual property (IP) and ensure fair use of these models, various techniques such as lexical watermarking and synonym replacement have been proposed. However, these methods can be nullified by obvious countermeasures such as "synonym randomization". To address this issue, we propose GINSEW, a novel method to protect text generation models from being stolen through distillation. The key idea of our method is to inject secret signals into the probability vector of the decoding steps for each target token. We can then detect the secret message by probing a suspect model to tell if it is distilled from the protected one. Experimental results show that GINSEW can effectively identify instances of IP infringement with minimal impact on the generation quality of protected APIs. Our method demonstrates an absolute improvement of 19 to 29 points on mean average precision (mAP) in detecting suspects compared to previous methods against watermark removal attacks.
<div id='section'>Paperid: <span id='pid'>494, <a href='https://arxiv.org/pdf/2512.15379.pdf' target='_blank'>https://arxiv.org/pdf/2512.15379.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Michael Amir, Manon Flageat, Amanda Prorok
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.15379">Remotely Detectable Robot Policy Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The success of machine learning for real-world robotic systems has created a new form of intellectual property: the trained policy. This raises a critical need for novel methods that verify ownership and detect unauthorized, possibly unsafe misuse. While watermarking is established in other domains, physical policies present a unique challenge: remote detection. Existing methods assume access to the robot's internal state, but auditors are often limited to external observations (e.g., video footage). This ``Physical Observation Gap'' means the watermark must be detected from signals that are noisy, asynchronous, and filtered by unknown system dynamics. We formalize this challenge using the concept of a \textit{glimpse sequence}, and introduce Colored Noise Coherency (CoNoCo), the first watermarking strategy designed for remote detection. CoNoCo embeds a spectral signal into the robot's motions by leveraging the policy's inherent stochasticity. To show it does not degrade performance, we prove CoNoCo preserves the marginal action distribution. Our experiments demonstrate strong, robust detection across various remote modalities, including motion capture and side-way/top-down video footage, in both simulated and real-world robot experiments. This work provides a necessary step toward protecting intellectual property in robotics, offering the first method for validating the provenance of physical policies non-invasively, using purely remote observations.
<div id='section'>Paperid: <span id='pid'>495, <a href='https://arxiv.org/pdf/2510.09210.pdf' target='_blank'>https://arxiv.org/pdf/2510.09210.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yifan Zhu, Lijia Yu, Xiao-Shan Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09210">Provable Watermarking for Data Poisoning Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, data poisoning attacks have been increasingly designed to appear harmless and even beneficial, often with the intention of verifying dataset ownership or safeguarding private data from unauthorized use. However, these developments have the potential to cause misunderstandings and conflicts, as data poisoning has traditionally been regarded as a security threat to machine learning systems. To address this issue, it is imperative for harmless poisoning generators to claim ownership of their generated datasets, enabling users to identify potential poisoning to prevent misuse. In this paper, we propose the deployment of watermarking schemes as a solution to this challenge. We introduce two provable and practical watermarking approaches for data poisoning: {\em post-poisoning watermarking} and {\em poisoning-concurrent watermarking}. Our analyses demonstrate that when the watermarking length is $Θ(\sqrt{d}/ε_w)$ for post-poisoning watermarking, and falls within the range of $Θ(1/ε_w^2)$ to $O(\sqrt{d}/ε_p)$ for poisoning-concurrent watermarking, the watermarked poisoning dataset provably ensures both watermarking detectability and poisoning utility, certifying the practicality of watermarking under data poisoning attacks. We validate our theoretical findings through experiments on several attacks, models, and datasets.
<div id='section'>Paperid: <span id='pid'>496, <a href='https://arxiv.org/pdf/2508.07263.pdf' target='_blank'>https://arxiv.org/pdf/2508.07263.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Zeng, Shu Jiang, Jiajing Lin, Zhenzhong Wang, Kay Chen Tan, Min Jiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07263">Fading the Digital Ink: A Universal Black-Box Attack Framework for 3DGS Watermarking Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of 3D Gaussian Splatting (3DGS), a variety of digital watermarking techniques, embedding either 1D bitstreams or 2D images, are used for copyright protection. However, the robustness of these watermarking techniques against potential attacks remains underexplored. This paper introduces the first universal black-box attack framework, the Group-based Multi-objective Evolutionary Attack (GMEA), designed to challenge these watermarking systems. We formulate the attack as a large-scale multi-objective optimization problem, balancing watermark removal with visual quality. In a black-box setting, we introduce an indirect objective function that blinds the watermark detector by minimizing the standard deviation of features extracted by a convolutional network, thus rendering the feature maps uninformative. To manage the vast search space of 3DGS models, we employ a group-based optimization strategy to partition the model into multiple, independent sub-optimization problems. Experiments demonstrate that our framework effectively removes both 1D and 2D watermarks from mainstream 3DGS watermarking methods while maintaining high visual fidelity. This work reveals critical vulnerabilities in existing 3DGS copyright protection schemes and calls for the development of more robust watermarking systems.
<div id='section'>Paperid: <span id='pid'>497, <a href='https://arxiv.org/pdf/2508.01893.pdf' target='_blank'>https://arxiv.org/pdf/2508.01893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Cheng Chu, Lei Jiang, Fan Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.01893">BVQC: A Backdoor-style Watermarking Scheme for Variational Quantum Circuits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Variational Quantum Circuits (VQCs) have emerged as a powerful quantum computing paradigm, demonstrating a scaling advantage for problems intractable for classical computation. As VQCs require substantial resources and specialized expertise for their design, they represent significant intellectual properties (IPs). However, existing quantum circuit watermarking techniques suffer from two primary drawbacks: (1) watermarks can be removed during re-compilation of the circuits, and (2) these methods significantly increase task loss due to the extensive length of the inserted watermarks across multiple compilation stages. To address these challenges, we propose BVQC, a backdoor-based watermarking technique for VQCs that preserves the original loss in typical execution settings, while deliberately increasing the loss to a predefined level during watermark extraction. Additionally, BVQC employs a grouping algorithm to minimize the watermark task's interference with the base task, ensuring optimal accuracy for the base task. BVQC retains the original compilation workflow, ensuring robustness against re-compilation. Our evaluations show that BVQC greatly reduces Probabilistic Proof of Authorship (PPA) changes by 9.89e-3 and ground truth distance (GTD) by 0.089 compared to prior watermarking technologies.
<div id='section'>Paperid: <span id='pid'>498, <a href='https://arxiv.org/pdf/2507.07871.pdf' target='_blank'>https://arxiv.org/pdf/2507.07871.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Toluwani Aremu, Noor Hussein, Munachiso Nwadike, Samuele Poppi, Jie Zhang, Karthik Nandakumar, Neil Gong, Nils Lukas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07871">Mitigating Watermark Forgery in Generative Models via Multi-Key Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking offers a promising solution for GenAI providers to establish the provenance of their generated content. A watermark is a hidden signal embedded in the generated content, whose presence can later be verified using a secret watermarking key. A security threat to GenAI providers are \emph{forgery attacks}, where malicious users insert the provider's watermark into generated content that was \emph{not} produced by the provider's models, potentially damaging their reputation and undermining trust. One potential defense to resist forgery is using multiple keys to watermark generated content. However, it has been shown that forgery attacks remain successful when adversaries can collect sufficiently many watermarked samples. We propose an improved multi-key watermarking method that resists all surveyed forgery attacks and scales independently of the number of watermarked samples collected by the adversary. Our method accepts content as genuinely watermarked only if \emph{exactly} one watermark is detected. We focus on the image and text modalities, but our detection method is modality-agnostic, since it treats the underlying watermarking method as a black-box. We derive theoretical bounds on forgery-resistance and empirically validate them using Mistral-7B. Our results show a decrease in forgery success from up to $100\%$ using single-key baselines to only $2\%$. While our method resists all surveyed attacks, we find that highly capable, adaptive attackers can still achieve success rates of up to $65\%$ if watermarked content generated using different keys is easily separable.
<div id='section'>Paperid: <span id='pid'>499, <a href='https://arxiv.org/pdf/2505.23814.pdf' target='_blank'>https://arxiv.org/pdf/2505.23814.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Nemecek, Yuzhou Jiang, Erman Ayday
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.23814">Watermarking Without Standards Is Not AI Governance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has emerged as a leading technical proposal for attributing generative AI content and is increasingly cited in global governance frameworks. This paper argues that current implementations risk serving as symbolic compliance rather than delivering effective oversight. We identify a growing gap between regulatory expectations and the technical limitations of existing watermarking schemes. Through analysis of policy proposals and industry practices, we show how incentive structures disincentivize robust, auditable deployments. To realign watermarking with governance goals, we propose a three-layer framework encompassing technical standards, audit infrastructure, and enforcement mechanisms. Without enforceable requirements and independent verification, watermarking will remain inadequate for accountability and ultimately undermine broader efforts in AI safety and regulation.
<div id='section'>Paperid: <span id='pid'>500, <a href='https://arxiv.org/pdf/2505.21636.pdf' target='_blank'>https://arxiv.org/pdf/2505.21636.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Nemecek, Yuzhou Jiang, Erman Ayday
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21636">The Feasibility of Topic-Based Watermarking on Academic Peer Reviews</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) are increasingly integrated into academic workflows, with many conferences and journals permitting their use for tasks such as language refinement and literature summarization. However, their use in peer review remains prohibited due to concerns around confidentiality breaches, hallucinated content, and inconsistent evaluations. As LLM-generated text becomes more indistinguishable from human writing, there is a growing need for reliable attribution mechanisms to preserve the integrity of the review process. In this work, we evaluate topic-based watermarking (TBW), a lightweight, semantic-aware technique designed to embed detectable signals into LLM-generated text. We conduct a comprehensive assessment across multiple LLM configurations, including base, few-shot, and fine-tuned variants, using authentic peer review data from academic conferences. Our results show that TBW maintains review quality relative to non-watermarked outputs, while demonstrating strong robustness to paraphrasing-based evasion. These findings highlight the viability of TBW as a minimally intrusive and practical solution for enforcing LLM usage in peer review.
<div id='section'>Paperid: <span id='pid'>501, <a href='https://arxiv.org/pdf/2504.00035.pdf' target='_blank'>https://arxiv.org/pdf/2504.00035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ziwei Zhang, Juan Wen, Wanli Peng, Zhengxian Wu, Yinghan Zhou, Yiming Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.00035">MiZero: The Shadowy Defender Against Text Style Infringements</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-Context Learning (ICL) and efficient fine-tuning methods significantly enhanced the efficiency of applying Large Language Models (LLMs) to downstream tasks. However, they also raise concerns about the imitation and infringement of personal creative data. Current methods for data copyright protection primarily focuses on content security but lacks effectiveness in protecting the copyrights of text styles. In this paper, we introduce a novel implicit zero-watermarking scheme, namely MiZero. This scheme establishes a precise watermark domain to protect the copyrighted style, surpassing traditional watermarking methods that distort the style characteristics. Specifically, we employ LLMs to extract condensed-lists utilizing the designed instance delimitation mechanism. These lists guide MiZero in generating the watermark. Extensive experiments demonstrate that MiZero effectively verifies text style copyright ownership against AI imitation.
<div id='section'>Paperid: <span id='pid'>502, <a href='https://arxiv.org/pdf/2503.04036.pdf' target='_blank'>https://arxiv.org/pdf/2503.04036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyue Cui, Johnny Tian-Zheng Wei, Swabha Swayamdipta, Robin Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.04036">Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data watermarking in language models injects traceable signals, such as specific token sequences or stylistic patterns, into copyrighted text, allowing copyright holders to track and verify training data ownership. Previous data watermarking techniques primarily focus on effective memorization during pretraining, while overlooking challenges that arise in other stages of the LLM lifecycle, such as the risk of watermark filtering during data preprocessing and verification difficulties due to API-only access. To address these challenges, we propose a novel data watermarking approach that injects plausible yet fictitious knowledge into training data using generated passages describing a fictitious entity and its associated attributes. Our watermarks are designed to be memorized by the LLM through seamlessly integrating in its training data, making them harder to detect lexically during preprocessing. We demonstrate that our watermarks can be effectively memorized by LLMs, and that increasing our watermarks' density, length, and diversity of attributes strengthens their memorization. We further show that our watermarks remain effective after continual pretraining and supervised finetuning. Finally, we show that our data watermarks can be evaluated even under API-only access via question answering.
<div id='section'>Paperid: <span id='pid'>503, <a href='https://arxiv.org/pdf/2412.03107.pdf' target='_blank'>https://arxiv.org/pdf/2412.03107.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoyu Jiang, Xuhong Wang, Ping Yi, Shanzhe Lei, Yilun Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03107">CredID: Credible Multi-Bit Watermark for Large Language Models Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are widely used in complex natural language processing tasks but raise privacy and security concerns due to the lack of identity recognition. This paper proposes a multi-party credible watermarking framework (CredID) involving a trusted third party (TTP) and multiple LLM vendors to address these issues. In the watermark embedding stage, vendors request a seed from the TTP to generate watermarked text without sending the user's prompt. In the extraction stage, the TTP coordinates each vendor to extract and verify the watermark from the text. This provides a credible watermarking scheme while preserving vendor privacy. Furthermore, current watermarking algorithms struggle with text quality, information capacity, and robustness, making it challenging to meet the diverse identification needs of LLMs. Thus, we propose a novel multi-bit watermarking algorithm and an open-source toolkit to facilitate research. Experiments show our CredID enhances watermark credibility and efficiency without compromising text quality. Additionally, we successfully utilized this framework to achieve highly accurate identification among multiple LLM vendors.
<div id='section'>Paperid: <span id='pid'>504, <a href='https://arxiv.org/pdf/2409.03568.pdf' target='_blank'>https://arxiv.org/pdf/2409.03568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chao Wang, Shubing Yang, Xiaoyan Sun, Jun Dai, Dongfang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03568">Enabling Practical and Privacy-Preserving Image Processing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Fully Homomorphic Encryption (FHE) enables computations on encrypted data, preserving confidentiality without the need for decryption. However, FHE is often hindered by significant performance overhead, particularly for high-precision and complex data like images. Due to serious efficiency issues, traditional FHE methods often encrypt images by monolithic data blocks (such as pixel rows), instead of pixels. However, this strategy compromises the advantages of homomorphic operations and disables pixel-level image processing. In this study, we address these challenges by proposing and implementing a pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS scheme. To enhance computational efficiency, we introduce three novel caching mechanisms to pre-encrypt radix values or frequently occurring pixel values, substantially reducing redundant encryption operations. Extensive experiments demonstrate that our approach achieves up to a 19-fold improvement in encryption speed compared to the original CKKS, while maintaining high image quality. Additionally, real-world image applications such as mean filtering, brightness enhancement, image matching and watermarking are tested based on FHE, showcasing up to a 91.53% speed improvement. We also proved that our method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure, providing strong encryption security. These results underscore the practicality and efficiency of iCHEETAH, marking a significant advancement in privacy-preserving image processing at scale.
<div id='section'>Paperid: <span id='pid'>505, <a href='https://arxiv.org/pdf/2407.11100.pdf' target='_blank'>https://arxiv.org/pdf/2407.11100.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuhong Wang, Haoyu Jiang, Yi Yu, Jingru Yu, Yilun Lin, Ping Yi, Yingchun Wang, Yu Qiao, Li Li, Fei-Yue Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.11100">Building Intelligence Identification System via Large Language Model Watermarking: A Survey and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) are increasingly integrated into diverse industries, posing substantial security risks due to unauthorized replication and misuse. To mitigate these concerns, robust identification mechanisms are widely acknowledged as an effective strategy. Identification systems for LLMs now rely heavily on watermarking technology to manage and protect intellectual property and ensure data security. However, previous studies have primarily concentrated on the basic principles of algorithms and lacked a comprehensive analysis of watermarking theory and practice from the perspective of intelligent identification. To bridge this gap, firstly, we explore how a robust identity recognition system can be effectively implemented and managed within LLMs by various participants using watermarking technology. Secondly, we propose a mathematical framework based on mutual information theory, which systematizes the identification process to achieve more precise and customized watermarking. Additionally, we present a comprehensive evaluation of performance metrics for LLM watermarking, reflecting participant preferences and advancing discussions on its identification applications. Lastly, we outline the existing challenges in current watermarking technologies and theoretical frameworks, and provide directional guidance to address these challenges. Our systematic classification and detailed exposition aim to enhance the comparison and evaluation of various methods, fostering further research and development toward a transparent, secure, and equitable LLM ecosystem.
<div id='section'>Paperid: <span id='pid'>506, <a href='https://arxiv.org/pdf/2405.07145.pdf' target='_blank'>https://arxiv.org/pdf/2405.07145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuepeng Hu, Zhengyuan Jiang, Moyang Guo, Neil Gong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.07145">Stable Signature is Unstable: Removing Image Watermark from Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermark has been widely deployed by industry to detect AI-generated images. A recent watermarking framework called \emph{Stable Signature} (proposed by Meta) roots watermark into the parameters of a diffusion model's decoder such that its generated images are inherently watermarked. Stable Signature makes it possible to watermark images generated by \emph{open-source} diffusion models and was claimed to be robust against removal attacks. In this work, we propose a new attack to remove the watermark from a diffusion model by fine-tuning it. Our results show that our attack can effectively remove the watermark from a diffusion model such that its generated images are non-watermarked, while maintaining the visual quality of the generated images. Our results highlight that Stable Signature is not as stable as previously thought.
<div id='section'>Paperid: <span id='pid'>507, <a href='https://arxiv.org/pdf/2404.02138.pdf' target='_blank'>https://arxiv.org/pdf/2404.02138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Nemecek, Yuzhou Jiang, Erman Ayday
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.02138">Topic-Based Watermarks for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The indistinguishability of Large Language Model (LLM) output from human-authored content poses significant challenges, raising concerns about potential misuse of AI-generated text and its influence on future AI model training. Watermarking algorithms offer a viable solution by embedding detectable signatures into generated text. However, existing watermarking methods often entail trade-offs among attack robustness, generation quality, and additional overhead such as specialized frameworks or complex integrations. We propose a lightweight, topic-guided watermarking scheme for LLMs that partitions the vocabulary into topic-aligned token subsets. Given an input prompt, the scheme selects a relevant topic-specific token list, effectively "green-listing" semantically aligned tokens to embed robust marks while preserving the text's fluency and coherence. Experimental results across multiple LLMs and state-of-the-art benchmarks demonstrate that our method achieves comparable perplexity to industry-leading systems, including Google's SynthID-Text, yet enhances watermark robustness against paraphrasing and lexical perturbation attacks while introducing minimal performance overhead. Our approach avoids reliance on additional mechanisms beyond standard text generation pipelines, facilitating straightforward adoption, suggesting a practical path toward globally consistent watermarking of AI-generated content.
<div id='section'>Paperid: <span id='pid'>508, <a href='https://arxiv.org/pdf/2402.10892.pdf' target='_blank'>https://arxiv.org/pdf/2402.10892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Johnny Tian-Zheng Wei, Ryan Yixiang Wang, Robin Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.10892">Proving membership in LLM pretraining data via data watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Detecting whether copyright holders' works were used in LLM pretraining is poised to be an important problem. This work proposes using data watermarks to enable principled detection with only black-box model access, provided that the rightholder contributed multiple training documents and watermarked them before public release. By applying a randomly sampled data watermark, detection can be framed as hypothesis testing, which provides guarantees on the false detection rate. We study two watermarks: one that inserts random sequences, and another that randomly substitutes characters with Unicode lookalikes. We first show how three aspects of watermark design -- watermark length, number of duplications, and interference -- affect the power of the hypothesis test. Next, we study how a watermark's detection strength changes under model and dataset scaling: while increasing the dataset size decreases the strength of the watermark, watermarks remain strong if the model size also increases. Finally, we view SHA hashes as natural watermarks and show that we can robustly detect hashes from BLOOM-176B's training data, as long as they occurred at least 90 times. Together, our results point towards a promising future for data watermarks in real world use.
<div id='section'>Paperid: <span id='pid'>509, <a href='https://arxiv.org/pdf/2401.09495.pdf' target='_blank'>https://arxiv.org/pdf/2401.09495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Win Kent Ong, Kam Woh Ng, Chee Seng Chan, Yi Zhe Song, Tao Xiang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.09495">IPR-NeRF: Ownership Verification meets Neural Radiance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural Radiance Field (NeRF) models have gained significant attention in the computer vision community in the recent past with state-of-the-art visual quality and produced impressive demonstrations. Since then, technopreneurs have sought to leverage NeRF models into a profitable business. Therefore, NeRF models make it worth the risk of plagiarizers illegally copying, re-distributing, or misusing those models. This paper proposes a comprehensive intellectual property (IP) protection framework for the NeRF model in both black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a diffusion-based solution is introduced to embed and extract the watermark via a two-stage optimization process. In the white-box setting, a designated digital signature is embedded into the weights of the NeRF model by adopting the sign loss objective. Our extensive experiments demonstrate that not only does our approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models, but it is also robust against both ambiguity and removal attacks compared to prior arts.
<div id='section'>Paperid: <span id='pid'>510, <a href='https://arxiv.org/pdf/2308.12770.pdf' target='_blank'>https://arxiv.org/pdf/2308.12770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.12770">WavMark: Watermarking for Audio Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent breakthroughs in zero-shot voice synthesis have enabled imitating a speaker's voice using just a few seconds of recording while maintaining a high level of realism. Alongside its potential benefits, this powerful technology introduces notable risks, including voice fraud and speaker impersonation. Unlike the conventional approach of solely relying on passive methods for detecting synthetic data, watermarking presents a proactive and robust defence mechanism against these looming risks. This paper introduces an innovative audio watermarking framework that encodes up to 32 bits of watermark within a mere 1-second audio snippet. The watermark is imperceptible to human senses and exhibits strong resilience against various attacks. It can serve as an effective identifier for synthesized voices and holds potential for broader applications in audio copyright protection. Moreover, this framework boasts high flexibility, allowing for the combination of multiple watermark segments to achieve heightened robustness and expanded capacity. Utilizing 10 to 20-second audio as the host, our approach demonstrates an average Bit Error Rate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over 2800\% in BER compared to the state-of-the-art watermarking tool. See https://aka.ms/wavmark for demos of our work.
<div id='section'>Paperid: <span id='pid'>511, <a href='https://arxiv.org/pdf/2305.09684.pdf' target='_blank'>https://arxiv.org/pdf/2305.09684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxia Yin, Heng Yin, Hang Su, Xinpeng Zhang, Zhenzhe Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.09684">Decision-based iterative fragile watermarking for model integrity verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Typically, foundation models are hosted on cloud servers to meet the high demand for their services. However, this exposes them to security risks, as attackers can modify them after uploading to the cloud or transferring from a local system. To address this issue, we propose an iterative decision-based fragile watermarking algorithm that transforms normal training samples into fragile samples that are sensitive to model changes. We then compare the output of sensitive samples from the original model to that of the compromised model during validation to assess the model's completeness.The proposed fragile watermarking algorithm is an optimization problem that aims to minimize the variance of the predicted probability distribution outputed by the target model when fed with the converted sample.We convert normal samples to fragile samples through multiple iterations. Our method has some advantages: (1) the iterative update of samples is done in a decision-based black-box manner, relying solely on the predicted probability distribution of the target model, which reduces the risk of exposure to adversarial attacks, (2) the small-amplitude multiple iterations approach allows the fragile samples to perform well visually, with a PSNR of 55 dB in TinyImageNet compared to the original samples, (3) even with changes in the overall parameters of the model of magnitude 1e-4, the fragile samples can detect such changes, and (4) the method is independent of the specific model structure and dataset. We demonstrate the effectiveness of our method on multiple models and datasets, and show that it outperforms the current state-of-the-art.
<div id='section'>Paperid: <span id='pid'>512, <a href='https://arxiv.org/pdf/2303.05498.pdf' target='_blank'>https://arxiv.org/pdf/2303.05498.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kirill Bykov, Klaus-Robert MÃ¼ller, Marina M. -C. HÃ¶hne
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.05498">Mark My Words: Dangers of Watermarked Images in ImageNet</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The utilization of pre-trained networks, especially those trained on ImageNet, has become a common practice in Computer Vision. However, prior research has indicated that a significant number of images in the ImageNet dataset contain watermarks, making pre-trained networks susceptible to learning artifacts such as watermark patterns within their latent spaces. In this paper, we aim to assess the extent to which popular pre-trained architectures display such behavior and to determine which classes are most affected. Additionally, we examine the impact of watermarks on the extracted features. Contrary to the popular belief that the Chinese logographic watermarks impact the "carton" class only, our analysis reveals that a variety of ImageNet classes, such as "monitor", "broom", "apron" and "safe" rely on spurious correlations. Finally, we propose a simple approach to mitigate this issue in fine-tuned networks by ignoring the encodings from the feature-extractor layer of ImageNet pre-trained networks that are most susceptible to watermark imprints.
<div id='section'>Paperid: <span id='pid'>513, <a href='https://arxiv.org/pdf/2511.15552.pdf' target='_blank'>https://arxiv.org/pdf/2511.15552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Artem Chervyakov, Ulyana Isaeva, Anton Emelyanov, Artem Safin, Maria Tikhonova, Alexander Kharitonov, Yulia Lyakh, Petr Surovtsev, Denis Shevelev, Vildan Saburov, Vasily Konovalov, Elisei Rykov, Ivan Sviridov, Amina Miftakhova, Ilseyar Alimova, Alexander Panchenko, Alexander Kapitanov, Alena Fenogenova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15552">Multimodal Evaluation of Russian-language Architectures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal large language models (MLLMs) are currently at the center of research attention, showing rapid progress in scale and capabilities, yet their intelligence, limitations, and risks remain insufficiently understood. To address these issues, particularly in the context of the Russian language, where no multimodal benchmarks currently exist, we introduce Mera Multi, an open multimodal evaluation framework for Russian-spoken architectures. The benchmark is instruction-based and encompasses default text, image, audio, and video modalities, comprising 18 newly constructed evaluation tasks for both general-purpose models and modality-specific architectures (image-to-text, video-to-text, and audio-to-text). Our contributions include: (i) a universal taxonomy of multimodal abilities; (ii) 18 datasets created entirely from scratch with attention to Russian cultural and linguistic specificity, unified prompts, and metrics; (iii) baseline results for both closed-source and open-source models; (iv) a methodology for preventing benchmark leakage, including watermarking and licenses for private sets. While our current focus is on Russian, the proposed benchmark provides a replicable methodology for constructing multimodal benchmarks in typologically diverse languages, particularly within the Slavic language family.
<div id='section'>Paperid: <span id='pid'>514, <a href='https://arxiv.org/pdf/2511.10933.pdf' target='_blank'>https://arxiv.org/pdf/2511.10933.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyi Ni, Ziyu Yang, Ze Niu, Emily Davis, Finn Carter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10933">On the Information-Theoretic Fragility of Robust Watermarking under Diffusion Editing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust invisible watermarking embeds hidden information in images such that the watermark can survive various manipulations. However, the emergence of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we investigate the intersection of diffusion-based image editing and robust image watermarking. We analyze how diffusion-driven image edits can significantly degrade or even fully remove embedded watermarks from state-of-the-art robust watermarking systems. Both theoretical formulations and empirical experiments are provided. We prove that as a image undergoes iterative diffusion transformations, the mutual information between the watermarked image and the embedded payload approaches zero, causing watermark decoding to fail. We further propose a guided diffusion attack algorithm that explicitly targets and erases watermark signals during generation. We evaluate our approach on recent deep learning-based watermarking schemes and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Finally, we discuss ethical implications of such watermark removal capablities and provide design guidelines for future watermarking strategies to be more resilient in the era of generative AI.
<div id='section'>Paperid: <span id='pid'>515, <a href='https://arxiv.org/pdf/2511.06390.pdf' target='_blank'>https://arxiv.org/pdf/2511.06390.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suqing Wang, Ziyang Ma, Xinyi Li, Zuchao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06390">Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have rapidly advanced and are widely adopted across diverse fields. Due to the substantial computational cost and data requirements of training from scratch, many developers choose to fine-tune or modify existing open-source models. While most adhere to open-source licenses, some falsely claim original training despite clear derivation from public models. This raises pressing concerns about intellectual property protection and highlights the need for reliable methods to verify model provenance. In this paper, we propose GhostSpec, a lightweight yet effective method for verifying LLM lineage without access to training data or modification of model behavior. Our approach constructs compact and robust fingerprints by applying singular value decomposition (SVD) to invariant products of internal attention weight matrices, effectively capturing the structural identity of a model. Unlike watermarking or output-based methods, GhostSpec is fully data-free, non-invasive, and computationally efficient. It demonstrates strong robustness to sequential fine-tuning, pruning, block expansion, and even adversarial transformations. Extensive experiments show that GhostSpec can reliably trace the lineage of transformed models with minimal overhead. By offering a practical solution for model verification and reuse tracking, our method contributes to the protection of intellectual property and fosters a transparent, trustworthy ecosystem for large-scale language models.
<div id='section'>Paperid: <span id='pid'>516, <a href='https://arxiv.org/pdf/2511.05598.pdf' target='_blank'>https://arxiv.org/pdf/2511.05598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenkai Fu, Finn Carter, Yue Wang, Emily Davis, Bo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.05598">Diffusion-Based Image Editing: An Unforeseen Adversary to Robust Invisible Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust invisible watermarking aims to embed hidden messages into images such that they survive various manipulations while remaining imperceptible. However, powerful diffusion-based image generation and editing models now enable realistic content-preserving transformations that can inadvertently remove or distort embedded watermarks. In this paper, we present a theoretical and empirical analysis demonstrating that diffusion-based image editing can effectively break state-of-the-art robust watermarks designed to withstand conventional distortions. We analyze how the iterative noising and denoising process of diffusion models degrades embedded watermark signals, and provide formal proofs that under certain conditions a diffusion model's regenerated image retains virtually no detectable watermark information. Building on this insight, we propose a diffusion-driven attack that uses generative image regeneration to erase watermarks from a given image. Furthermore, we introduce an enhanced \emph{guided diffusion} attack that explicitly targets the watermark during generation by integrating the watermark decoder into the sampling loop. We evaluate our approaches on multiple recent deep learning watermarking schemes (e.g., StegaStamp, TrustMark, and VINE) and demonstrate that diffusion-based editing can reduce watermark decoding accuracy to near-zero levels while preserving high visual fidelity of the images. Our findings reveal a fundamental vulnerability in current robust watermarking techniques against generative model-based edits, underscoring the need for new watermarking strategies in the era of generative AI.
<div id='section'>Paperid: <span id='pid'>517, <a href='https://arxiv.org/pdf/2510.17114.pdf' target='_blank'>https://arxiv.org/pdf/2510.17114.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hodaka Kawachi, Tomoya Nakamura, Hiroaki Santo, SaiKiran Kumar Tedla, Trevor Dalton Canham, Yasushi Yagi, Michael S. Brown
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17114">Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a method for using LED-based environmental lighting to produce visually imperceptible watermarks for consumer cameras. Our approach optimizes an LED light source's spectral profile to be minimally visible to the human eye while remaining highly detectable by typical consumer cameras. The method jointly considers the human visual system's sensitivity to visible spectra, modern consumer camera sensors' spectral sensitivity, and narrowband LEDs' ability to generate broadband spectra perceived as "white light" (specifically, D65 illumination). To ensure imperceptibility, we employ spectral modulation rather than intensity modulation. Unlike conventional visible light communication, our approach enables watermark extraction at standard low frame rates (30-60 fps). While the information transfer rate is modest-embedding 128 bits within a 10-second video clip-this capacity is sufficient for essential metadata supporting privacy protection and content verification.
<div id='section'>Paperid: <span id='pid'>518, <a href='https://arxiv.org/pdf/2510.05978.pdf' target='_blank'>https://arxiv.org/pdf/2510.05978.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunyi Ni, Finn Carter, Ze Niu, Emily Davis, Bo Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.05978">Diffusion-Based Image Editing for Breaking Robust Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust invisible watermarking aims to embed hidden information into images such that the watermark can survive various image manipulations. However, the rise of powerful diffusion-based image generation and editing techniques poses a new threat to these watermarking schemes. In this paper, we present a theoretical study and method demonstrating that diffusion models can effectively break robust image watermarks that were designed to resist conventional perturbations. We show that a diffusion-driven ``image regeneration'' process can erase embedded watermarks while preserving perceptual image content. We further introduce a novel guided diffusion attack that explicitly targets the watermark signal during generation, significantly degrading watermark detectability. Theoretically, we prove that as an image undergoes sufficient diffusion-based transformation, the mutual information between the watermarked image and the embedded watermark payload vanishes, resulting in decoding failure. Experimentally, we evaluate our approach on multiple state-of-the-art watermarking schemes (including the deep learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate near-zero watermark recovery rates after attack, while maintaining high visual fidelity of the regenerated images. Our findings highlight a fundamental vulnerability in current robust watermarking techniques against generative model-based attacks, underscoring the need for new watermarking strategies in the era of generative AI.
<div id='section'>Paperid: <span id='pid'>519, <a href='https://arxiv.org/pdf/2509.20924.pdf' target='_blank'>https://arxiv.org/pdf/2509.20924.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanbo Huang, Yiran Zhang, Hao Zheng, Xuan Gong, Yihan Li, Lin Liu, Shiyu Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20924">RLCracker: Exposing the Vulnerability of LLM Watermarks with Adaptive RL Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) watermarking has shown promise in detecting AI-generated content and mitigating misuse, with prior work claiming robustness against paraphrasing and text editing. In this paper, we argue that existing evaluations are not sufficiently adversarial, obscuring critical vulnerabilities and overstating the security. To address this, we introduce adaptive robustness radius, a formal metric that quantifies watermark resilience against adaptive adversaries. We theoretically prove that optimizing the attack context and model parameters can substantially reduce this radius, making watermarks highly susceptible to paraphrase attacks. Leveraging this insight, we propose RLCracker, a reinforcement learning (RL)-based adaptive attack that erases watermarks while preserving semantic fidelity. RLCracker requires only limited watermarked examples and zero access to the detector. Despite weak supervision, it empowers a 3B model to achieve 98.5% removal success and an average 0.92 P-SP score on 1,500-token Unigram-marked texts after training on only 100 short samples. This performance dramatically exceeds 6.75% by GPT-4o and generalizes across five model sizes over ten watermarking schemes. Our results confirm that adaptive attacks are broadly effective and pose a fundamental threat to current watermarking defenses.
<div id='section'>Paperid: <span id='pid'>520, <a href='https://arxiv.org/pdf/2508.21797.pdf' target='_blank'>https://arxiv.org/pdf/2508.21797.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Navid Aftabi, Abhishek Hanchate, Satish Bukkapatnam, Dan Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.21797">DynaMark: A Reinforcement Learning Framework for Dynamic Watermarking in Industrial Machine Tool Controllers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Industry 4.0's highly networked Machine Tool Controllers (MTCs) are prime targets for replay attacks that use outdated sensor data to manipulate actuators. Dynamic watermarking can reveal such tampering, but current schemes assume linear-Gaussian dynamics and use constant watermark statistics, making them vulnerable to the time-varying, partly proprietary behavior of MTCs. We close this gap with DynaMark, a reinforcement learning framework that models dynamic watermarking as a Markov decision process (MDP). It learns an adaptive policy online that dynamically adapts the covariance of a zero-mean Gaussian watermark using available measurements and detector feedback, without needing system knowledge. DynaMark maximizes a unique reward function balancing control performance, energy consumption, and detection confidence dynamically. We develop a Bayesian belief updating mechanism for real-time detection confidence in linear systems. This approach, independent of specific system assumptions, underpins the MDP for systems with linear dynamics. On a Siemens Sinumerik 828D controller digital twin, DynaMark achieves a reduction in watermark energy by 70% while preserving the nominal trajectory, compared to constant variance baselines. It also maintains an average detection delay equivalent to one sampling interval. A physical stepper-motor testbed validates these findings, rapidly triggering alarms with less control performance decline and exceeding existing benchmarks.
<div id='section'>Paperid: <span id='pid'>521, <a href='https://arxiv.org/pdf/2508.11925.pdf' target='_blank'>https://arxiv.org/pdf/2508.11925.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhimeng Guo, Huaisheng Zhu, Siyuan Xu, Hangfan Zhang, Teng Xiao, Minhao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.11925">Optimizing Token Choice for Code Watermarking: A RL Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The need for detecting LLM-generated code necessitates watermarking systems capable of operating within its highly structured and syntactically constrained environment. To address this, we introduce CodeTracer, an innovative adaptive code watermarking framework underpinned by a novel reinforcement learning training paradigm. At its core, CodeTracer features a policy-driven approach that utilizes a parameterized model to intelligently bias token choices during next-token prediction. This strategy ensures that embedded watermarks maintain code functionality while exhibiting subtle yet statistically detectable deviations from typical token distributions. To facilitate policy learning, we devise a comprehensive reward system that seamlessly integrates execution feedback with watermark embedding signals, balancing process-level and outcome-level rewards. Additionally, we employ Gumbel Top-k reparameterization to enable gradient-based optimization of discrete watermarking decisions. Extensive comparative evaluations demonstrate CodeTracer's significant superiority over state-of-the-art baselines in both watermark detectability and the preservation of generated code's functionality.
<div id='section'>Paperid: <span id='pid'>522, <a href='https://arxiv.org/pdf/2508.09162.pdf' target='_blank'>https://arxiv.org/pdf/2508.09162.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Konstantinos Vasili, Zachery T. Dahm, Stylianos Chatzidakis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.09162">An Unsupervised Deep XAI Framework for Localization of Concurrent Replay Attacks in Nuclear Reactor Signals</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Next generation advanced nuclear reactors are expected to be smaller both in size and power output, relying extensively on fully digital instrumentation and control systems. These reactors will generate a large flow of information in the form of multivariate time series data, conveying simultaneously various non linear cyber physical, process, control, sensor, and operational states. Ensuring data integrity against deception attacks is becoming increasingly important for networked communication and a requirement for safe and reliable operation. Current efforts to address replay attacks, almost universally focus on watermarking or supervised anomaly detection approaches without further identifying and characterizing the root cause of the anomaly. In addition, these approaches rely mostly on synthetic data with uncorrelated Gaussian process and measurement noise and full state feedback or are limited to univariate signals, signal stationarity, linear quadratic regulators, or other linear-time invariant state-space which may fail to capture any unmodeled system dynamics. In the realm of regulated nuclear cyber-physical systems, additional work is needed on characterization of replay attacks and explainability of predictions using real data. Here, we propose an unsupervised explainable AI framework based on a combination of autoencoder and customized windowSHAP algorithm to fully characterize real-time replay attacks, i.e., detection, source identification, timing and type, of increasing complexity during a dynamic time evolving reactor process. The proposed XAI framework was benchmarked on several real world datasets from Purdue's nuclear reactor PUR-1 with up to six signals concurrently being replayed. In all cases, the XAI framework was able to detect and identify the source and number of signals being replayed and the duration of the falsification with 95 percent or better accuracy.
<div id='section'>Paperid: <span id='pid'>523, <a href='https://arxiv.org/pdf/2507.13407.pdf' target='_blank'>https://arxiv.org/pdf/2507.13407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vinu Sankar Sadasivan, Mehrdad Saberi, Soheil Feizi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.13407">IConMark: Robust Interpretable Concept-Based Watermark For AI Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid rise of generative AI and synthetic media, distinguishing AI-generated images from real ones has become crucial in safeguarding against misinformation and ensuring digital authenticity. Traditional watermarking techniques have shown vulnerabilities to adversarial attacks, undermining their effectiveness in the presence of attackers. We propose IConMark, a novel in-generation robust semantic watermarking method that embeds interpretable concepts into AI-generated images, as a first step toward interpretable watermarking. Unlike traditional methods, which rely on adding noise or perturbations to AI-generated images, IConMark incorporates meaningful semantic attributes, making it interpretable to humans and hence, resilient to adversarial manipulation. This method is not only robust against various image augmentations but also human-readable, enabling manual verification of watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness, demonstrating its superiority in terms of detection accuracy and maintaining image quality. Moreover, IConMark can be combined with existing watermarking techniques to further enhance and complement its robustness. We introduce IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with StegaStamp and TrustMark, respectively, to further bolster robustness against multiple types of image manipulations. Our base watermarking technique (IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9% higher mean area under the receiver operating characteristic curve (AUROC) scores for watermark detection, respectively, compared to the best baseline on various datasets.
<div id='section'>Paperid: <span id='pid'>524, <a href='https://arxiv.org/pdf/2505.21568.pdf' target='_blank'>https://arxiv.org/pdf/2505.21568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haiyun Li, Zhiyong Wu, Xiaofeng Xie, Jingran Xie, Yaoxun Xu, Hanyang Peng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.21568">VoiceMark: Zero-Shot Voice Cloning-Resistant Watermarking Approach Leveraging Speaker-Specific Latents</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Voice cloning (VC)-resistant watermarking is an emerging technique for tracing and preventing unauthorized cloning. Existing methods effectively trace traditional VC models by training them on watermarked audio but fail in zero-shot VC scenarios, where models synthesize audio from an audio prompt without training. To address this, we propose VoiceMark, the first zero-shot VC-resistant watermarking method that leverages speaker-specific latents as the watermark carrier, allowing the watermark to transfer through the zero-shot VC process into the synthesized audio. Additionally, we introduce VC-simulated augmentations and VAD-based loss to enhance robustness against distortions. Experiments on multiple zero-shot VC models demonstrate that VoiceMark achieves over 95% accuracy in watermark detection after zero-shot VC synthesis, significantly outperforming existing methods, which only reach around 50%. See our code and demos at: https://huggingface.co/spaces/haiyunli/VoiceMark
<div id='section'>Paperid: <span id='pid'>525, <a href='https://arxiv.org/pdf/2504.20532.pdf' target='_blank'>https://arxiv.org/pdf/2504.20532.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Li, Weizhi Liu, Dongdong Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.20532">TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Attribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The emergence of diffusion models has facilitated the generation of speech with reinforced fidelity and naturalness. While deepfake detection technologies have manifested the ability to identify AI-generated content, their efficacy decreases as generative models become increasingly sophisticated. Furthermore, current research in the field has not adequately addressed the necessity for robust watermarking to safeguard the intellectual property rights associated with synthetic speech and generative models. To remedy this deficiency, we propose a \textbf{ro}bust generative \textbf{s}peech wat\textbf{e}rmarking method (TriniMark) for authenticating the generated content and safeguarding the copyrights by enabling the traceability of the diffusion model. We first design a structure-lightweight watermark encoder that embeds watermarks into the time-domain features of speech and reconstructs the waveform directly. A temporal-aware gated convolutional network is meticulously designed in the watermark decoder for bit-wise watermark recovery. Subsequently, the waveform-guided fine-tuning strategy is proposed for fine-tuning the diffusion model, which leverages the transferability of watermarks and enables the diffusion model to incorporate watermark knowledge effectively. When an attacker trains a surrogate model using the outputs of the target model, the embedded watermark can still be learned by the surrogate model and correctly extracted. Comparative experiments with state-of-the-art methods demonstrate the superior robustness of our method, particularly in countering compound attacks.
<div id='section'>Paperid: <span id='pid'>526, <a href='https://arxiv.org/pdf/2504.15035.pdf' target='_blank'>https://arxiv.org/pdf/2504.15035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Li, Weizhi Liu, Dongdong Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.15035">SOLIDO: A Robust Watermarking Method for Speech Synthesis via Low-Rank Adaptation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The accelerated advancement of speech generative models has given rise to security issues, including model infringement and unauthorized abuse of content. Although existing generative watermarking techniques have proposed corresponding solutions, most methods require substantial computational overhead and training costs. In addition, some methods have limitations in robustness when handling variable-length inputs. To tackle these challenges, we propose \textsc{SOLIDO}, a novel generative watermarking method that integrates parameter-efficient fine-tuning with speech watermarking through low-rank adaptation (LoRA) for speech diffusion models. Concretely, the watermark encoder converts the watermark to align with the input of diffusion models. To achieve precise watermark extraction from variable-length inputs, the watermark decoder based on depthwise separable convolution is designed for watermark recovery. To further enhance speech generation performance and watermark extraction capability, we propose a speech-driven lightweight fine-tuning strategy, which reduces computational overhead through LoRA. Comprehensive experiments demonstrate that the proposed method ensures high-fidelity watermarked speech even at a large capacity of 2000 bps. Furthermore, against common individual and compound speech attacks, our SOLIDO achieves a maximum average extraction accuracy of 99.20\% and 98.43\%, respectively. It surpasses other state-of-the-art methods by nearly 23\% in resisting time-stretching attacks.
<div id='section'>Paperid: <span id='pid'>527, <a href='https://arxiv.org/pdf/2504.14832.pdf' target='_blank'>https://arxiv.org/pdf/2504.14832.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yue Li, Weizhi Liu, Dongdong Lin, Hui Tian, Hongxia Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.14832">Protecting Your Voice: Temporal-aware Robust Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative models has led to the synthesis of real-fake ambiguous voices. To erase the ambiguity, embedding watermarks into the frequency-domain features of synthesized voices has become a common routine. However, the robustness achieved by choosing the frequency domain often comes at the expense of fine-grained voice features, leading to a loss of fidelity. Maximizing the comprehensive learning of time-domain features to enhance fidelity while maintaining robustness, we pioneer a \textbf{\underline{t}}emporal-aware \textbf{\underline{r}}ob\textbf{\underline{u}}st wat\textbf{\underline{e}}rmarking (\emph{True}) method for protecting the speech and singing voice. For this purpose, the integrated content-driven encoder is designed for watermarked waveform reconstruction, which is structurally lightweight. Additionally, the temporal-aware gated convolutional network is meticulously designed to bit-wise recover the watermark. Comprehensive experiments and comparisons with existing state-of-the-art methods have demonstrated the superior fidelity and vigorous robustness of the proposed \textit{True} achieving an average PESQ score of 4.63.
<div id='section'>Paperid: <span id='pid'>528, <a href='https://arxiv.org/pdf/2504.12229.pdf' target='_blank'>https://arxiv.org/pdf/2504.12229.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Khachaturov, Robert Mullins, Ilia Shumailov, Sumanth Dathathri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.12229">Watermarking Needs Input Repetition Masking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in Large Language Models (LLMs) raised concerns over potential misuse, such as for spreading misinformation. In response two counter measures emerged: machine learning-based detectors that predict if text is synthetic, and LLM watermarking, which subtly marks generated text for identification and attribution. Meanwhile, humans are known to adjust language to their conversational partners both syntactically and lexically. By implication, it is possible that humans or unwatermarked LLMs could unintentionally mimic properties of LLM generated text, making counter measures unreliable. In this work we investigate the extent to which such conversational adaptation happens. We call the concept $\textit{mimicry}$ and demonstrate that both humans and LLMs end up mimicking, including the watermarking signal even in seemingly improbable settings. This challenges current academic assumptions and suggests that for long-term watermarking to be reliable, the likelihood of false positives needs to be significantly lower, while longer word sequences should be used for seeding watermarking mechanisms.
<div id='section'>Paperid: <span id='pid'>529, <a href='https://arxiv.org/pdf/2504.09451.pdf' target='_blank'>https://arxiv.org/pdf/2504.09451.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Wang, Harry Cheng, Ming-Hui Liu, Mohan Kankanhalli
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.09451">FractalForensics: Proactive Deepfake Detection and Localization via Fractal Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Proactive Deepfake detection via robust watermarks has been raised ever since passive Deepfake detectors encountered challenges in identifying high-quality synthetic images. However, while demonstrating reasonable detection performance, they lack localization functionality and explainability in detection results. Additionally, the unstable robustness of watermarks can significantly affect the detection performance accordingly. In this study, we propose novel fractal watermarks for proactive Deepfake detection and localization, namely FractalForensics. Benefiting from the characteristics of fractals, we devise a parameter-driven watermark generation pipeline that derives fractal-based watermarks and conducts one-way encryption regarding the parameters selected. Subsequently, we propose a semi-fragile watermarking framework for watermark embedding and recovery, trained to be robust against benign image processing operations and fragile when facing Deepfake manipulations in a black-box setting. Meanwhile, we introduce an entry-to-patch strategy that implicitly embeds the watermark matrix entries into image patches at corresponding positions, achieving localization of Deepfake manipulations. Extensive experiments demonstrate satisfactory robustness and fragility of our approach against common image processing operations and Deepfake manipulations, outperforming state-of-the-art semi-fragile watermarking algorithms and passive detectors for Deepfake detection. Furthermore, by highlighting the areas manipulated, our method provides explainability for the proactive Deepfake detection results.
<div id='section'>Paperid: <span id='pid'>530, <a href='https://arxiv.org/pdf/2503.12836.pdf' target='_blank'>https://arxiv.org/pdf/2503.12836.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sumin In, Youngdong Jang, Utae Jeong, MinHyuk Jang, Hyeongcheol Park, Eunbyung Park, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12836">CompMarkGS: Robust Watermarking for Compressed 3D Gaussian Splatting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>3D Gaussian Splatting (3DGS) is increasingly adopted in various academic and commercial applications due to its real-time and high-quality rendering capabilities, emphasizing the growing need for copyright protection technologies for 3DGS. However, the large model size of 3DGS requires developing efficient compression techniques. This highlights the necessity of an integrated framework that addresses copyright protection and data compression for 3D content. Nevertheless, existing 3DGS watermarking methods significantly degrade watermark performance under 3DGS compression methods, particularly quantization-based approaches that achieve superior compression performance. To ensure reliable watermark detection under compression, we propose a compression-tolerant anchor-based 3DGS watermarking, which preserves watermark integrity and rendering quality. This is achieved by introducing anchor-based 3DGS watermarking. We embed the watermark into the anchor attributes, particularly the anchor feature, to enhance security and rendering quality. We also propose a quantization distortion layer that injects quantization noise during training, preserving the watermark after quantization-based compression. Moreover, we employ a frequency-aware anchor growing strategy that improves rendering quality and watermark performance by effectively identifying Gaussians in high-frequency regions. Extensive experiments demonstrate that our proposed method preserves the watermark even under compression and maintains high rendering quality.
<div id='section'>Paperid: <span id='pid'>531, <a href='https://arxiv.org/pdf/2502.11726.pdf' target='_blank'>https://arxiv.org/pdf/2502.11726.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Li, Bingxu Xie, Chao Chu, Weiqing Li, Zhiyong Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.11726">No-reference geometry quality assessment for colorless point clouds via list-wise rank learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Geometry quality assessment (GQA) of colorless point clouds is crucial for evaluating the performance of emerging point cloud-based solutions (e.g., watermarking, compression, and 3-Dimensional (3D) reconstruction). Unfortunately, existing objective GQA approaches are traditional full-reference metrics, whereas state-of-the-art learning-based point cloud quality assessment (PCQA) methods target both color and geometry distortions, neither of which are qualified for the no-reference GQA task. In addition, the lack of large-scale GQA datasets with subjective scores, which are always imprecise, biased, and inconsistent, also hinders the development of learning-based GQA metrics. Driven by these limitations, this paper proposes a no-reference geometry-only quality assessment approach based on list-wise rank learning, termed LRL-GQA, which comprises of a geometry quality assessment network (GQANet) and a list-wise rank learning network (LRLNet). The proposed LRL-GQA formulates the no-reference GQA as a list-wise rank problem, with the objective of directly optimizing the entire quality ordering. Specifically, a large dataset containing a variety of geometry-only distortions is constructed first, named LRL dataset, in which each sample is label-free but coupled with quality ranking information. Then, the GQANet is designed to capture intrinsic multi-scale patch-wise geometric features in order to predict a quality index for each point cloud. After that, the LRLNet leverages the LRL dataset and a likelihood loss to train the GQANet and ranks the input list of degraded point clouds according to their distortion levels. In addition, the pre-trained GQANet can be fine-tuned further to obtain absolute quality scores. Experimental results demonstrate the superior performance of the proposed no-reference LRL-GQA method compared with existing full-reference GQA metrics.
<div id='section'>Paperid: <span id='pid'>532, <a href='https://arxiv.org/pdf/2412.09122.pdf' target='_blank'>https://arxiv.org/pdf/2412.09122.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>MinHyuk Jang, Youngdong Jang, JaeHyeok Lee, Feng Yang, Gyeongrok Oh, Jongheon Jeong, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09122">LVMark: Robust Watermark for Latent Video Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Rapid advancements in video diffusion models have enabled the creation of realistic videos, raising concerns about unauthorized use and driving the demand for techniques to protect model ownership. Existing watermarking methods, while effective for image diffusion models, do not account for temporal consistency, leading to degraded video quality and reduced robustness against video distortions. To address this issue, we introduce LVMark, a novel watermarking method for video diffusion models. We propose a new watermark decoder tailored for generated videos by learning the consistency between adjacent frames. It ensures accurate message decoding, even under malicious attacks, by combining the low-frequency components of the 3D wavelet domain with the RGB features of the video. Additionally, our approach minimizes video quality degradation by embedding watermark messages in layers with minimal impact on visual appearance using an importance-based weight modulation strategy. We optimize both the watermark decoder and the latent decoder of diffusion model, effectively balancing the trade-off between visual quality and bit accuracy. Our experiments show that our method embeds invisible watermarks into video diffusion models, ensuring robust decoding accuracy with 512-bit capacity, even under video distortions.
<div id='section'>Paperid: <span id='pid'>533, <a href='https://arxiv.org/pdf/2412.08549.pdf' target='_blank'>https://arxiv.org/pdf/2412.08549.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pascal Epple, Igor Shilov, Bozhidar Stevanoski, Yves-Alexandre de Montjoye
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.08549">Watermarking Training Data of Music Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative Artificial Intelligence (Gen-AI) models are increasingly used to produce content across domains, including text, images, and audio. While these models represent a major technical breakthrough, they gain their generative capabilities from being trained on enormous amounts of human-generated content, which often includes copyrighted material. In this work, we investigate whether audio watermarking techniques can be used to detect an unauthorized usage of content to train a music generation model. We compare outputs generated by a model trained on watermarked data to a model trained on non-watermarked data. We study factors that impact the model's generation behaviour: the watermarking technique, the proportion of watermarked samples in the training set, and the robustness of the watermarking technique against the model's tokenizer. Our results show that audio watermarking techniques, including some that are imperceptible to humans, can lead to noticeable shifts in the model's outputs. We also study the robustness of a state-of-the-art watermarking technique to removal techniques.
<div id='section'>Paperid: <span id='pid'>534, <a href='https://arxiv.org/pdf/2411.17209.pdf' target='_blank'>https://arxiv.org/pdf/2411.17209.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Wang, Mengxiao Huang, Harry Cheng, Xiao Zhang, Zhiqi Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17209">LampMark: Proactive Deepfake Detection via Training-Free Landmark Perceptual Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deepfake facial manipulation has garnered significant public attention due to its impacts on enhancing human experiences and posing privacy threats. Despite numerous passive algorithms that have been attempted to thwart malicious Deepfake attacks, they mostly struggle with the generalizability challenge when confronted with hyper-realistic synthetic facial images. To tackle the problem, this paper proposes a proactive Deepfake detection approach by introducing a novel training-free landmark perceptual watermark, LampMark for short. We first analyze the structure-sensitive characteristics of Deepfake manipulations and devise a secure and confidential transformation pipeline from the structural representations, i.e. facial landmarks, to binary landmark perceptual watermarks. Subsequently, we present an end-to-end watermarking framework that imperceptibly and robustly embeds and extracts watermarks concerning the images to be protected. Relying on promising watermark recovery accuracies, Deepfake detection is accomplished by assessing the consistency between the content-matched landmark perceptual watermark and the robustly recovered watermark of the suspect image. Experimental results demonstrate the superior performance of our approach in watermark recovery and Deepfake detection compared to state-of-the-art methods across in-dataset, cross-dataset, and cross-manipulation scenarios.
<div id='section'>Paperid: <span id='pid'>535, <a href='https://arxiv.org/pdf/2411.00380.pdf' target='_blank'>https://arxiv.org/pdf/2411.00380.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haifeng Sun, Lan Zhang, Xiang-Yang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.00380">DeepCore: Simple Fingerprint Construction for Differentiating Homologous and Piracy Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As intellectual property rights, the copyright protection of deep models is becoming increasingly important. Existing work has made many attempts at model watermarking and fingerprinting, but they have ignored homologous models trained with similar structures or training datasets. We highlight challenges in efficiently querying black-box piracy models to protect model copyrights without misidentifying homologous models. To address these challenges, we propose a novel method called DeepCore, which discovers that the classification confidence of the model is positively correlated with the distance of the predicted sample from the model decision boundary and piracy models behave more similarly at high-confidence classified sample points. Then DeepCore constructs core points far away from the decision boundary by optimizing the predicted confidence of a few sample points and leverages behavioral discrepancies between piracy and homologous models to identify piracy models. Finally, we design different model identification methods, including two similarity-based methods and a clustering-based method to identify piracy models using models' predictions of core points. Extensive experiments show the effectiveness of DeepCore in identifying various piracy models, achieving lower missed and false identification rates, and outperforming state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>536, <a href='https://arxiv.org/pdf/2409.18897.pdf' target='_blank'>https://arxiv.org/pdf/2409.18897.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Songrui Wang, Yubo Zhu, Wei Tong, Sheng Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18897">Detecting Dataset Abuse in Fine-Tuning Stable Diffusion Models for Text-to-Image Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image synthesis has become highly popular for generating realistic and stylized images, often requiring fine-tuning generative models with domain-specific datasets for specialized tasks. However, these valuable datasets face risks of unauthorized usage and unapproved sharing, compromising the rights of the owners. In this paper, we address the issue of dataset abuse during the fine-tuning of Stable Diffusion models for text-to-image synthesis. We present a dataset watermarking framework designed to detect unauthorized usage and trace data leaks. The framework employs two key strategies across multiple watermarking schemes and is effective for large-scale dataset authorization. Extensive experiments demonstrate the framework's effectiveness, minimal impact on the dataset (only 2% of the data required to be modified for high detection accuracy), and ability to trace data leaks. Our results also highlight the robustness and transferability of the framework, proving its practical applicability in detecting dataset abuse.
<div id='section'>Paperid: <span id='pid'>537, <a href='https://arxiv.org/pdf/2407.10471.pdf' target='_blank'>https://arxiv.org/pdf/2407.10471.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weizhi Liu, Yue Li, Dongdong Lin, Hui Tian, Haizhou Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.10471">GROOT: Generating Robust Watermark for Diffusion-Model-Based Audio Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Amid the burgeoning development of generative models like diffusion models, the task of differentiating synthesized audio from its natural counterpart grows more daunting. Deepfake detection offers a viable solution to combat this challenge. Yet, this defensive measure unintentionally fuels the continued refinement of generative models. Watermarking emerges as a proactive and sustainable tactic, preemptively regulating the creation and dissemination of synthesized content. Thus, this paper, as a pioneer, proposes the generative robust audio watermarking method (Groot), presenting a paradigm for proactively supervising the synthesized audio and its source diffusion models. In this paradigm, the processes of watermark generation and audio synthesis occur simultaneously, facilitated by parameter-fixed diffusion models equipped with a dedicated encoder. The watermark embedded within the audio can subsequently be retrieved by a lightweight decoder. The experimental results highlight Groot's outstanding performance, particularly in terms of robustness, surpassing that of the leading state-of-the-art methods. Beyond its impressive resilience against individual post-processing attacks, Groot exhibits exceptional robustness when facing compound attacks, maintaining an average watermark extraction accuracy of around 95%.
<div id='section'>Paperid: <span id='pid'>538, <a href='https://arxiv.org/pdf/2406.11106.pdf' target='_blank'>https://arxiv.org/pdf/2406.11106.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Harsh Nishant Lalai, Aashish Anantha Ramakrishnan, Raj Sanjay Shah, Dongwon Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11106">From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid growth of Large Language Models (LLMs), safeguarding textual content against unauthorized use is crucial. Watermarking offers a vital solution, protecting both - LLM-generated and plain text sources. This paper presents a unified overview of different perspectives behind designing watermarking techniques through a comprehensive survey of the research literature. Our work has two key advantages: (1) We analyze research based on the specific intentions behind different watermarking techniques, evaluation datasets used, and watermarking addition and removal methods to construct a cohesive taxonomy. (2) We highlight the gaps and open challenges in text watermarking to promote research protecting text authorship. This extensive coverage and detailed analysis sets our work apart, outlining the evolving landscape of text watermarking in Language Models.
<div id='section'>Paperid: <span id='pid'>539, <a href='https://arxiv.org/pdf/2405.02066.pdf' target='_blank'>https://arxiv.org/pdf/2405.02066.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Youngdong Jang, Dong In Lee, MinHyuk Jang, Jong Wook Kim, Feng Yang, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02066">WateRF: Robust Watermarks in Radiance Fields for Protection of Copyrights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advances in the Neural Radiance Fields (NeRF) research offer extensive applications in diverse domains, but protecting their copyrights has not yet been researched in depth. Recently, NeRF watermarking has been considered one of the pivotal solutions for safely deploying NeRF-based 3D representations. However, existing methods are designed to apply only to implicit or explicit NeRF representations. In this work, we introduce an innovative watermarking method that can be employed in both representations of NeRF. This is achieved by fine-tuning NeRF to embed binary messages in the rendering process. In detail, we propose utilizing the discrete wavelet transform in the NeRF space for watermarking. Furthermore, we adopt a deferred back-propagation technique and introduce a combination with the patch-wise loss to improve rendering quality and bit accuracy with minimum trade-offs. We evaluate our method in three different aspects: capacity, invisibility, and robustness of the embedded watermarks in the 2D-rendered images. Our method achieves state-of-the-art performance with faster training speed over the compared state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>540, <a href='https://arxiv.org/pdf/2311.01357.pdf' target='_blank'>https://arxiv.org/pdf/2311.01357.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyi Wang, Mengxiao Huang, Harry Cheng, Bin Ma, Yinglong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.01357">Robust Identity Perceptual Watermark Against Deepfake Face Swapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Notwithstanding offering convenience and entertainment to society, Deepfake face swapping has caused critical privacy issues with the rapid development of deep generative models. Due to imperceptible artifacts in high-quality synthetic images, passive detection models against face swapping in recent years usually suffer performance damping regarding the generalizability issue. Therefore, several studies have been attempted to proactively protect the original images against malicious manipulations by inserting invisible signals in advance. However, the existing proactive defense approaches demonstrate unsatisfactory results with respect to visual quality, detection accuracy, and source tracing ability. In this study, to fulfill the research gap, we propose the first robust identity perceptual watermarking framework that concurrently performs detection and source tracing against Deepfake face swapping proactively. We assign identity semantics regarding the image contents to the watermarks and devise an unpredictable and nonreversible chaotic encryption system to ensure watermark confidentiality. The watermarks are encoded and recovered by jointly training an encoder-decoder framework along with adversarial image manipulations. Falsification and source tracing are accomplished by justifying the consistency between the content-matched identity perceptual watermark and the recovered robust watermark from the image. Extensive experiments demonstrate state-of-the-art detection performance on Deepfake face swapping under both cross-dataset and cross-manipulation settings.
<div id='section'>Paperid: <span id='pid'>541, <a href='https://arxiv.org/pdf/2310.08920.pdf' target='_blank'>https://arxiv.org/pdf/2310.08920.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryoma Sato, Yuki Takezawa, Han Bao, Kenta Niwa, Makoto Yamada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.08920">Embarrassingly Simple Text Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose Easymark, a family of embarrassingly simple yet effective watermarks. Text watermarking is becoming increasingly important with the advent of Large Language Models (LLM). LLMs can generate texts that cannot be distinguished from human-written texts. This is a serious problem for the credibility of the text. Easymark is a simple yet effective solution to this problem. Easymark can inject a watermark without changing the meaning of the text at all while a validator can detect if a text was generated from a system that adopted Easymark or not with high credibility. Easymark is extremely easy to implement so that it only requires a few lines of code. Easymark does not require access to LLMs, so it can be implemented on the user-side when the LLM providers do not offer watermarked LLMs. In spite of its simplicity, it achieves higher detection accuracy and BLEU scores than the state-of-the-art text watermarking methods. We also prove the impossibility theorem of perfect watermarking, which is valuable in its own right. This theorem shows that no matter how sophisticated a watermark is, a malicious user could remove it from the text, which motivate us to use a simple watermark such as Easymark. We carry out experiments with LLM-generated texts and confirm that Easymark can be detected reliably without any degradation of BLEU and perplexity, and outperform state-of-the-art watermarks in terms of both quality and reliability.
<div id='section'>Paperid: <span id='pid'>542, <a href='https://arxiv.org/pdf/2310.00833.pdf' target='_blank'>https://arxiv.org/pdf/2310.00833.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuki Takezawa, Ryoma Sato, Han Bao, Kenta Niwa, Makoto Yamada
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00833">Necessary and Sufficient Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, large language models (LLMs) have achieved remarkable performances in various NLP tasks. They can generate texts that are indistinguishable from those written by humans. Such remarkable performance of LLMs increases their risk of being used for malicious purposes, such as generating fake news articles. Therefore, it is necessary to develop methods for distinguishing texts written by LLMs from those written by humans. Watermarking is one of the most powerful methods for achieving this. Although existing watermarking methods have successfully detected texts generated by LLMs, they significantly degrade the quality of the generated texts. In this study, we propose the Necessary and Sufficient Watermark (NS-Watermark) for inserting watermarks into generated texts without degrading the text quality. More specifically, we derive minimum constraints required to be imposed on the generated texts to distinguish whether LLMs or humans write the texts. Then, we formulate the NS-Watermark as a constrained optimization problem and propose an efficient algorithm to solve it. Through the experiments, we demonstrate that the NS-Watermark can generate more natural texts than existing watermarking methods and distinguish more accurately between texts written by LLMs and those written by humans. Especially in machine translation tasks, the NS-Watermark can outperform the existing watermarking method by up to 30 BLEU scores.
<div id='section'>Paperid: <span id='pid'>543, <a href='https://arxiv.org/pdf/2305.10036.pdf' target='_blank'>https://arxiv.org/pdf/2305.10036.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10036">Are You Copying My Model? Protecting the Copyright of Large Language Models for EaaS via Backdoor Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have demonstrated powerful capabilities in both text understanding and generation. Companies have begun to offer Embedding as a Service (EaaS) based on these LLMs, which can benefit various natural language processing (NLP) tasks for customers. However, previous studies have shown that EaaS is vulnerable to model extraction attacks, which can cause significant losses for the owners of LLMs, as training these models is extremely expensive. To protect the copyright of LLMs for EaaS, we propose an Embedding Watermark method called EmbMarker that implants backdoors on embeddings. Our method selects a group of moderate-frequency words from a general text corpus to form a trigger set, then selects a target embedding as the watermark, and inserts it into the embeddings of texts containing trigger words as the backdoor. The weight of insertion is proportional to the number of trigger words included in the text. This allows the watermark backdoor to be effectively transferred to EaaS-stealer's model for copyright verification while minimizing the adverse impact on the original embeddings' utility. Our extensive experiments on various datasets show that our method can effectively protect the copyright of EaaS models without compromising service quality.
<div id='section'>Paperid: <span id='pid'>544, <a href='https://arxiv.org/pdf/2509.22126.pdf' target='_blank'>https://arxiv.org/pdf/2509.22126.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Enoal Gesny, Eva Giboulot, Teddy Furon, Vivien Chappelier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.22126">Guidance Watermarking for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel watermarking method for diffusion models. It is based on guiding the diffusion process using the gradient computed from any off-the-shelf watermark decoder. The gradient computation encompasses different image augmentations, increasing robustness to attacks against which the decoder was not originally robust, without retraining or fine-tuning. Our method effectively convert any \textit{post-hoc} watermarking scheme into an in-generation embedding along the diffusion process. We show that this approach is complementary to watermarking techniques modifying the variational autoencoder at the end of the diffusion process. We validate the methods on different diffusion models and detectors. The watermarking guidance does not significantly alter the generated image for a given seed and prompt, preserving both the diversity and quality of generation.
<div id='section'>Paperid: <span id='pid'>545, <a href='https://arxiv.org/pdf/2508.00892.pdf' target='_blank'>https://arxiv.org/pdf/2508.00892.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhihao Zhu, Jiale Han, Yi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.00892">HoneyImage: Verifiable, Harmless, and Stealthy Dataset Ownership Verification for Image Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image-based AI models are increasingly deployed across a wide range of domains, including healthcare, security, and consumer applications. However, many image datasets carry sensitive or proprietary content, raising critical concerns about unauthorized data usage. Data owners therefore need reliable mechanisms to verify whether their proprietary data has been misused to train third-party models. Existing solutions, such as backdoor watermarking and membership inference, face inherent trade-offs between verification effectiveness and preservation of data integrity. In this work, we propose HoneyImage, a novel method for dataset ownership verification in image recognition models. HoneyImage selectively modifies a small number of hard samples to embed imperceptible yet verifiable traces, enabling reliable ownership verification while maintaining dataset integrity. Extensive experiments across four benchmark datasets and multiple model architectures show that HoneyImage consistently achieves strong verification accuracy with minimal impact on downstream performance while maintaining imperceptible. The proposed HoneyImage method could provide data owners with a practical mechanism to protect ownership over valuable image datasets, encouraging safe sharing and unlocking the full transformative potential of data-driven AI.
<div id='section'>Paperid: <span id='pid'>546, <a href='https://arxiv.org/pdf/2507.22000.pdf' target='_blank'>https://arxiv.org/pdf/2507.22000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Oliver J. Sutton, Qinghua Zhou, George Leete, Alexander N. Gorban, Ivan Y. Tyukin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.22000">Staining and locking computer vision models without retraining</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce new methods of staining and locking computer vision models, to protect their owners' intellectual property. Staining, also known as watermarking, embeds secret behaviour into a model which can later be used to identify it, while locking aims to make a model unusable unless a secret trigger is inserted into input images. Unlike existing methods, our algorithms can be used to stain and lock pre-trained models without requiring fine-tuning or retraining, and come with provable, computable guarantees bounding their worst-case false positive rates. The stain and lock are implemented by directly modifying a small number of the model's weights and have minimal impact on the (unlocked) model's performance. Locked models are unlocked by inserting a small `trigger patch' into the corner of the input image. We present experimental results showing the efficacy of our methods and demonstrating their practical performance on a variety of computer vision models.
<div id='section'>Paperid: <span id='pid'>547, <a href='https://arxiv.org/pdf/2506.05594.pdf' target='_blank'>https://arxiv.org/pdf/2506.05594.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kieu Dang, Phung Lai, NhatHai Phan, Yelong Shen, Ruoming Jin, Abdallah Khreishah, My Thai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05594">SoK: Are Watermarks in LLMs Ready for Deployment?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.
  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.
<div id='section'>Paperid: <span id='pid'>548, <a href='https://arxiv.org/pdf/2505.04977.pdf' target='_blank'>https://arxiv.org/pdf/2505.04977.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Brian Choi, Shu Wang, Isabelle Choi, Kun Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.04977">ChainMarks: Securing DNN Watermark with Cryptographic Chain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread deployment of deep neural network (DNN) models, dynamic watermarking techniques are being used to protect the intellectual property of model owners. However, recent studies have shown that existing watermarking schemes are vulnerable to watermark removal and ambiguity attacks. Besides, the vague criteria for determining watermark presence further increase the likelihood of such attacks. In this paper, we propose a secure DNN watermarking scheme named ChainMarks, which generates secure and robust watermarks by introducing a cryptographic chain into the trigger inputs and utilizes a two-phase Monte Carlo method for determining watermark presence. First, ChainMarks generates trigger inputs as a watermark dataset by repeatedly applying a hash function over a secret key, where the target labels associated with trigger inputs are generated from the digital signature of model owner. Then, the watermarked model is produced by training a DNN over both the original and watermark datasets. To verify watermarks, we compare the predicted labels of trigger inputs with the target labels and determine ownership with a more accurate decision threshold that considers the classification probability of specific models. Experimental results show that ChainMarks exhibits higher levels of robustness and security compared to state-of-the-art watermarking schemes. With a better marginal utility, ChainMarks provides a higher probability guarantee of watermark presence in DNN models with the same level of watermark accuracy.
<div id='section'>Paperid: <span id='pid'>549, <a href='https://arxiv.org/pdf/2503.12172.pdf' target='_blank'>https://arxiv.org/pdf/2503.12172.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kasra Arabi, R. Teal Witter, Chinmay Hegde, Niv Cohen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.12172">SEAL: Semantic Aware Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have rapidly evolved to generate realistic outputs. However, their synthetic outputs increasingly challenge the clear distinction between natural and AI-generated content, necessitating robust watermarking techniques. Watermarks are typically expected to preserve the integrity of the target image, withstand removal attempts, and prevent unauthorized replication onto unrelated images. To address this need, recent methods embed persistent watermarks into images produced by diffusion models using the initial noise. Yet, to do so, they either distort the distribution of generated images or rely on searching through a long dictionary of used keys for detection.
  In this paper, we propose a novel watermarking method that embeds semantic information about the generated image directly into the watermark, enabling a distortion-free watermark that can be verified without requiring a database of key patterns. Instead, the key pattern can be inferred from the semantic embedding of the image using locality-sensitive hashing. Furthermore, conditioning the watermark detection on the original image content improves robustness against forgery attacks. To demonstrate that, we consider two largely overlooked attack strategies: (i) an attacker extracting the initial noise and generating a novel image with the same pattern; (ii) an attacker inserting an unrelated (potentially harmful) object into a watermarked image, possibly while preserving the watermark. We empirically validate our method's increased robustness to these attacks. Taken together, our results suggest that content-aware watermarks can mitigate risks arising from image-generative models.
<div id='section'>Paperid: <span id='pid'>550, <a href='https://arxiv.org/pdf/2502.18608.pdf' target='_blank'>https://arxiv.org/pdf/2502.18608.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shayleen Reynolds, Hengzhi He, Dung Daniel T. Ngo, Saheed Obitayo, NiccolÃ² Dalmasso, Guang Cheng, Vamsi K. Potluru, Manuela Veloso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18608">Breaking Distortion-free Watermarks in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, LLM watermarking has emerged as an attractive safeguard against AI-generated content, with promising applications in many real-world domains. However, there are growing concerns that the current LLM watermarking schemes are vulnerable to expert adversaries wishing to reverse-engineer the watermarking mechanisms. Prior work in breaking or stealing LLM watermarks mainly focuses on the distribution-modifying algorithm of Kirchenbauer et al. (2023), which perturbs the logit vector before sampling. In this work, we focus on reverse-engineering the other prominent LLM watermarking scheme, distortion-free watermarking (Kuditipudi et al. 2024), which preserves the underlying token distribution by using a hidden watermarking key sequence. We demonstrate that, even under a more sophisticated watermarking scheme, it is possible to compromise the LLM and carry out a spoofing attack, i.e. generate a large number of (potentially harmful) texts that can be attributed to the original watermarked LLM. Specifically, we propose using adaptive prompting and a sorting-based algorithm to accurately recover the underlying secret key for watermarking the LLM. Our empirical findings on LLAMA-3.1-8B-Instruct, Mistral-7B-Instruct, Gemma-7b, and OPT-125M challenge the current theoretical claims on the robustness and usability of the distortion-free watermarking techniques.
<div id='section'>Paperid: <span id='pid'>551, <a href='https://arxiv.org/pdf/2501.03496.pdf' target='_blank'>https://arxiv.org/pdf/2501.03496.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jinming Gao, Yijing Wang, Wentao Zhang, Rui Zhao, Yang Shi, Zhiqiang Zuo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03496">A Unified Attack Detection Strategy for Multi-Agent Systems over Transient and Steady Stages</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper proposes a unified detection strategy against three kinds of attacks for multi-agent systems (MASs) which is applicable to both transient and steady stages. For attacks on the communication layer, a watermarking-based detection scheme with KullbackLeibler (KL) divergence is designed. Different from traditional communication schemes, each agent transmits a message set containing two state values with different types of watermarking. It is found that the detection performance is determined by the relevant parameters of the watermarking signal. Unlike the existing detection manoeuvres, such a scheme is capable of transient and steady stages. For attacks on the agent layer, a convergence rate related detection approach is put forward. It is shown that the resilience of the considered system is characterized by the coefficient and offset of the envelope. For hybrid attacks, based on the above detection mechanisms, a general framework resorting to trusted agents is presented, which requires weaker graph conditions and less information transmission. Finally, an example associated with the platooning of connected vehicles is given to support the theoretical results.
<div id='section'>Paperid: <span id='pid'>552, <a href='https://arxiv.org/pdf/2411.10898.pdf' target='_blank'>https://arxiv.org/pdf/2411.10898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bochao Gu, Hengzhi He, Guang Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.10898">Watermarking Generative Categorical Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose a novel statistical framework for watermarking generative categorical data. Our method systematically embeds pre-agreed secret signals by splitting the data distribution into two components and modifying one distribution based on a deterministic relationship with the other, ensuring the watermark is embedded at the distribution-level. To verify the watermark, we introduce an insertion inverse algorithm and detect its presence by measuring the total variation distance between the inverse-decoded data and the original distribution. Unlike previous categorical watermarking methods, which primarily focus on embedding watermarks into a given dataset, our approach operates at the distribution-level, allowing for verification from a statistical distributional perspective. This makes it particularly well-suited for the modern paradigm of synthetic data generation, where the underlying data distribution, rather than specific data points, is of primary importance. The effectiveness of our method is demonstrated through both theoretical analysis and empirical validation.
<div id='section'>Paperid: <span id='pid'>553, <a href='https://arxiv.org/pdf/2410.14102.pdf' target='_blank'>https://arxiv.org/pdf/2410.14102.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiale Zhang, Haoxuan Li, Di Wu, Xiaobing Sun, Qinghua Lu, Guodong Long
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.14102">Beyond Dataset Watermarking: Model-Level Copyright Protection for Code Summarization Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Code Summarization Model (CSM) has been widely used in code production, such as online and web programming for PHP and Javascript. CSMs are essential tools in code production, enhancing software development efficiency and driving innovation in automated code analysis. However, CSMs face risks of exploitation by unauthorized users, particularly in an online environment where CSMs can be easily shared and disseminated. To address these risks, digital watermarks offer a promising solution by embedding imperceptible signatures within the models to assert copyright ownership and track unauthorized usage. Traditional watermarking for CSM copyright protection faces two main challenges: 1) dataset watermarking methods require separate design of triggers and watermark features based on the characteristics of different programming languages, which not only increases the computation complexity but also leads to a lack of generalization, 2) existing watermarks based on code style transformation are easily identifiable by automated detection, demonstrating poor concealment. To tackle these issues, we propose ModMark , a novel model-level digital watermark embedding method. Specifically, by fine-tuning the tokenizer, ModMark achieves cross-language generalization while reducing the complexity of watermark design. Moreover, we employ code noise injection techniques to effectively prevent trigger detection. Experimental results show that our method can achieve 100% watermark verification rate across various programming languages' CSMs, and the concealment and effectiveness of ModMark can also be guaranteed.
<div id='section'>Paperid: <span id='pid'>554, <a href='https://arxiv.org/pdf/2407.07237.pdf' target='_blank'>https://arxiv.org/pdf/2407.07237.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Archisman Ghosh, Swaroop Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.07237">The Quantum Imitation Game: Reverse Engineering of Quantum Machine Learning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum Machine Learning (QML) amalgamates quantum computing paradigms with machine learning models, providing significant prospects for solving complex problems. However, with the expansion of numerous third-party vendors in the Noisy Intermediate-Scale Quantum (NISQ) era of quantum computing, the security of QML models is of prime importance, particularly against reverse engineering, which could expose trained parameters and algorithms of the models. We assume the untrusted quantum cloud provider is an adversary having white-box access to the transpiled user-designed trained QML model during inference. Reverse engineering (RE) to extract the pre-transpiled QML circuit will enable re-transpilation and usage of the model for various hardware with completely different native gate sets and even different qubit technology. Such flexibility may not be obtained from the transpiled circuit which is tied to a particular hardware and qubit technology. The information about the number of parameters, and optimized values can allow further training of the QML model to alter the QML model, tamper with the watermark, and/or embed their own watermark or refine the model for other purposes. In this first effort to investigate the RE of QML circuits, we perform RE and compare the training accuracy of original and reverse-engineered Quantum Neural Networks (QNNs) of various sizes. We note that multi-qubit classifiers can be reverse-engineered under specific conditions with a mean error of order 1e-2 in a reasonable time. We also propose adding dummy fixed parametric gates in the QML models to increase the RE overhead for defense. For instance, adding 2 dummy qubits and 2 layers increases the overhead by ~1.76 times for a classifier with 2 qubits and 3 layers with a performance overhead of less than 9%. We note that RE is a very powerful attack model which warrants further efforts on defenses.
<div id='section'>Paperid: <span id='pid'>555, <a href='https://arxiv.org/pdf/2406.04809.pdf' target='_blank'>https://arxiv.org/pdf/2406.04809.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhe Gao, Yu Cheng, Zhaoxia Yin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04809">A Survey of Fragile Model Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model fragile watermarking, inspired by both the field of adversarial attacks on neural networks and traditional multimedia fragile watermarking, has gradually emerged as a potent tool for detecting tampering, and has witnessed rapid development in recent years. Unlike robust watermarks, which are widely used for identifying model copyrights, fragile watermarks for models are designed to identify whether models have been subjected to unexpected alterations such as backdoors, poisoning, compression, among others. These alterations can pose unknown risks to model users, such as misidentifying stop signs as speed limit signs in classic autonomous driving scenarios. This paper provides an overview of the relevant work in the field of model fragile watermarking since its inception, categorizing them and revealing the developmental trajectory of the field, thus offering a comprehensive survey for future endeavors in model fragile watermarking.
<div id='section'>Paperid: <span id='pid'>556, <a href='https://arxiv.org/pdf/2405.14604.pdf' target='_blank'>https://arxiv.org/pdf/2405.14604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minjia Mao, Dongjun Wei, Zeyu Chen, Xiao Fang, Michael Chau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.14604">Watermarking Low-entropy Generation for Large Language Models: An Unbiased and Low-risk Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in large language models (LLMs) have highlighted the risk of misusing them, raising the need for accurate detection of LLM-generated content. In response, a viable solution is to inject imperceptible identifiers into LLMs, known as watermarks. Our research extends the existing watermarking methods by proposing the novel Sampling One Then Accepting (STA-1) method. STA-1 is an unbiased watermark that preserves the original token distribution in expectation and has a lower risk of producing unsatisfactory outputs in low-entropy scenarios compared to existing unbiased watermarks. In watermark detection, STA-1 does not require prompts or a white-box LLM, provides statistical guarantees, demonstrates high efficiency in detection time, and remains robust against various watermarking attacks. Experimental results on low-entropy and high-entropy datasets demonstrate that STA-1 achieves the above properties simultaneously, making it a desirable solution for watermarking LLMs. Implementation codes for this study are available online.
<div id='section'>Paperid: <span id='pid'>557, <a href='https://arxiv.org/pdf/2404.16156.pdf' target='_blank'>https://arxiv.org/pdf/2404.16156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Archisman Ghosh, Debarshi Kundu, Avimita Chatterjee, Swaroop Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16156">Guardians of the Quantum GAN</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum Generative Adversarial Networks (qGANs) are at the forefront of image-generating quantum machine learning models. To accommodate the growing demand for Noisy Intermediate-Scale Quantum (NISQ) devices to train and infer quantum machine learning models, the number of third-party vendors offering quantum hardware as a service is expected to rise. This expansion introduces the risk of untrusted vendors potentially stealing proprietary information from the quantum machine learning models. To address this concern we propose a novel watermarking technique that exploits the noise signature embedded during the training phase of qGANs as a non-invasive watermark. The watermark is identifiable in the images generated by the qGAN allowing us to trace the specific quantum hardware used during training hence providing strong proof of ownership. To further enhance the security robustness, we propose the training of qGANs on a sequence of multiple quantum hardware, embedding a complex watermark comprising the noise signatures of all the training hardware that is difficult for adversaries to replicate. We also develop a machine learning classifier to extract this watermark robustly, thereby identifying the training hardware (or the suite of hardware) from the images generated by the qGAN validating the authenticity of the model. We note that the watermark signature is robust against inferencing on hardware different than the hardware that was used for training. We obtain watermark extraction accuracy of 100% and ~90% for training the qGAN on individual and multiple quantum hardware setups (and inferencing on different hardware), respectively. Since parameter evolution during training is strongly modulated by quantum noise, the proposed watermark can be extended to other quantum machine learning models as well.
<div id='section'>Paperid: <span id='pid'>558, <a href='https://arxiv.org/pdf/2404.14693.pdf' target='_blank'>https://arxiv.org/pdf/2404.14693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunming Zhang, Dengpan Ye, Caiyun Xie, Sipeng Shen, Ziyi Liu, Jiacheng Deng, Long Tang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.14693">DIP-Watermark: A Double Identity Protection Method Based on Robust Adversarial Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The wide deployment of Face Recognition (FR) systems poses privacy risks. One countermeasure is adversarial attack, deceiving unauthorized malicious FR, but it also disrupts regular identity verification of trusted authorizers, exacerbating the potential threat of identity impersonation. To address this, we propose the first double identity protection scheme based on traceable adversarial watermarking, termed DIP-Watermark. DIP-Watermark employs a one-time watermark embedding to deceive unauthorized FR models and allows authorizers to perform identity verification by extracting the watermark. Specifically, we propose an information-guided adversarial attack against FR models. The encoder embeds an identity-specific watermark into the deep feature space of the carrier, guiding recognizable features of the image to deviate from the source identity. We further adopt a collaborative meta-optimization strategy compatible with sub-tasks, which regularizes the joint optimization direction of the encoder and decoder. This strategy enhances the representation of universal carrier features, mitigating multi-objective optimization conflicts in watermarking. Experiments confirm that DIP-Watermark achieves significant attack success rates and traceability accuracy on state-of-the-art FR models, exhibiting remarkable robustness that outperforms the existing privacy protection methods using adversarial attacks and deep watermarking, or simple combinations of the two. Our work potentially opens up new insights into proactive protection for FR privacy.
<div id='section'>Paperid: <span id='pid'>559, <a href='https://arxiv.org/pdf/2403.19548.pdf' target='_blank'>https://arxiv.org/pdf/2403.19548.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Piotr Molenda, Adian Liusie, Mark J. F. Gales
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.19548">WaterJudge: Quality-Detection Trade-off when Watermarking Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking generative-AI systems, such as LLMs, has gained considerable interest, driven by their enhanced capabilities across a wide range of tasks. Although current approaches have demonstrated that small, context-dependent shifts in the word distributions can be used to apply and detect watermarks, there has been little work in analyzing the impact that these perturbations have on the quality of generated texts. Balancing high detectability with minimal performance degradation is crucial in terms of selecting the appropriate watermarking setting; therefore this paper proposes a simple analysis framework where comparative assessment, a flexible NLG evaluation framework, is used to assess the quality degradation caused by a particular watermark setting. We demonstrate that our framework provides easy visualization of the quality-detection trade-off of watermark settings, enabling a simple solution to find an LLM watermark operating point that provides a well-balanced performance. This approach is applied to two different summarization systems and a translation system, enabling cross-model analysis for a task, and cross-task analysis.
<div id='section'>Paperid: <span id='pid'>560, <a href='https://arxiv.org/pdf/2401.01366.pdf' target='_blank'>https://arxiv.org/pdf/2401.01366.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jan Butora, Patrick Bas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.01366">The Adobe Hidden Feature and its Impact on Sensor Attribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>If the extraction of sensor fingerprints represents nowadays an important forensic tool for sensor attribution, it has been shown recently that images coming from several sensors were more prone to generate False Positives (FP) by presenting a common "leak". In this paper, we investigate the possible cause of this leak and after inspecting the EXIF metadata of the sources causing FP, we found out that they were related to the Adobe Lightroom or Photoshop softwares. The cross-correlation between residuals on images presenting FP reveals periodic peaks showing the presence of a periodic pattern. By developing our own images with Adobe Lightroom we are able to show that all developments from raw images (or 16 bits per channel coded) to 8 bits-coded images also embed a periodic 128x128 pattern very similar to a watermark. However, we also show that the watermark depends on both the content and the architecture used to develop the image. The rest of the paper presents two different ways of removing this watermark, one by removing it from the image noise component, and the other by removing it in the pixel domain. We show that for a camera presenting FP, we were able to prevent the False Positives. A discussion with Adobe representatives informed us that the company decided to add this pattern in order to induce dithering.
<div id='section'>Paperid: <span id='pid'>561, <a href='https://arxiv.org/pdf/2310.16540.pdf' target='_blank'>https://arxiv.org/pdf/2310.16540.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunming Zhang, Dengpan Ye, Caiyun Xie, Long Tang, Chuanxi Chen, Ziyi Liu, Jiacheng Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16540">Dual Defense: Adversarial, Traceable, and Invisible Robust Watermarking against Face Swapping</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The malicious applications of deep forgery, represented by face swapping, have introduced security threats such as misinformation dissemination and identity fraud. While some research has proposed the use of robust watermarking methods to trace the copyright of facial images for post-event traceability, these methods cannot effectively prevent the generation of forgeries at the source and curb their dissemination. To address this problem, we propose a novel comprehensive active defense mechanism that combines traceability and adversariality, called Dual Defense. Dual Defense invisibly embeds a single robust watermark within the target face to actively respond to sudden cases of malicious face swapping. It disrupts the output of the face swapping model while maintaining the integrity of watermark information throughout the entire dissemination process. This allows for watermark extraction at any stage of image tracking for traceability. Specifically, we introduce a watermark embedding network based on original-domain feature impersonation attack. This network learns robust adversarial features of target facial images and embeds watermarks, seeking a well-balanced trade-off between watermark invisibility, adversariality, and traceability through perceptual adversarial encoding strategies. Extensive experiments demonstrate that Dual Defense achieves optimal overall defense success rates and exhibits promising universality in anti-face swapping tasks and dataset generalization ability. It maintains impressive adversariality and traceability in both original and robust settings, surpassing current forgery defense methods that possess only one of these capabilities, including CMUA-Watermark, Anti-Forgery, FakeTagger, or PGD methods.
<div id='section'>Paperid: <span id='pid'>562, <a href='https://arxiv.org/pdf/2310.09822.pdf' target='_blank'>https://arxiv.org/pdf/2310.09822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingfu Xue, Leo Yu Zhang, Yushu Zhang, Weiqiang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09822">Turn Passive to Active: A Survey on Active Intellectual Property Protection of Deep Learning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The intellectual property protection of deep learning (DL) models has attracted increasing serious concerns. Many works on intellectual property protection for Deep Neural Networks (DNN) models have been proposed. The vast majority of existing work uses DNN watermarking to verify the ownership of the model after piracy occurs, which is referred to as passive verification. On the contrary, we focus on a new type of intellectual property protection method named active copyright protection, which refers to active authorization control and user identity management of the DNN model. As of now, there is relatively limited research in the field of active DNN copyright protection. In this review, we attempt to clearly elaborate on the connotation, attributes, and requirements of active DNN copyright protection, provide evaluation methods and metrics for active copyright protection, review and analyze existing work on active DL model intellectual property protection, discuss potential attacks that active DL model copyright protection techniques may face, and provide challenges and future directions for active DL model intellectual property protection. This review is helpful to systematically introduce the new field of active DNN copyright protection and provide reference and foundation for subsequent work.
<div id='section'>Paperid: <span id='pid'>563, <a href='https://arxiv.org/pdf/2305.06786.pdf' target='_blank'>https://arxiv.org/pdf/2305.06786.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Natan Semyonov, Rami Puzis, Asaf Shabtai, Gilad Katz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06786">ReMark: Receptive Field based Spatial WaterMark Embedding Optimization using Deep Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is one of the most important copyright protection tools for digital media. The most challenging type of watermarking is the imperceptible one, which embeds identifying information in the data while retaining the latter's original quality. To fulfill its purpose, watermarks need to withstand various distortions whose goal is to damage their integrity. In this study, we investigate a novel deep learning-based architecture for embedding imperceptible watermarks. The key insight guiding our architecture design is the need to correlate the dimensions of our watermarks with the sizes of receptive fields (RF) of modules of our architecture. This adaptation makes our watermarks more robust, while also enabling us to generate them in a way that better maintains image quality. Extensive evaluations on a wide variety of distortions show that the proposed method is robust against most common distortions on watermarks including collusive distortion.
<div id='section'>Paperid: <span id='pid'>564, <a href='https://arxiv.org/pdf/2304.11285.pdf' target='_blank'>https://arxiv.org/pdf/2304.11285.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Isabell Lederer, Rudolf Mayer, Andreas Rauber
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11285">Identifying Appropriate Intellectual Property Protection Mechanisms for Machine Learning Models: A Systematization of Watermarking, Fingerprinting, Model Access, and Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The commercial use of Machine Learning (ML) is spreading; at the same time, ML models are becoming more complex and more expensive to train, which makes Intellectual Property Protection (IPP) of trained models a pressing issue. Unlike other domains that can build on a solid understanding of the threats, attacks and defenses available to protect their IP, the ML-related research in this regard is still very fragmented. This is also due to a missing unified view as well as a common taxonomy of these aspects.
  In this paper, we systematize our findings on IPP in ML, while focusing on threats and attacks identified and defenses proposed at the time of writing. We develop a comprehensive threat model for IP in ML, categorizing attacks and defenses within a unified and consolidated taxonomy, thus bridging research from both the ML and security communities.
<div id='section'>Paperid: <span id='pid'>565, <a href='https://arxiv.org/pdf/2210.07481.pdf' target='_blank'>https://arxiv.org/pdf/2210.07481.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingfu Xue, Xin Wang, Yinghao Wu, Shifeng Ni, Yushu Zhang, Weiqiang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.07481">InFIP: An Explainable DNN Intellectual Property Protection Method based on Intrinsic Features</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Intellectual property (IP) protection for Deep Neural Networks (DNNs) has raised serious concerns in recent years. Most existing works embed watermarks in the DNN model for IP protection, which need to modify the model and lack of interpretability. In this paper, for the first time, we propose an interpretable intellectual property protection method for DNN based on explainable artificial intelligence. Compared with existing works, the proposed method does not modify the DNN model, and the decision of the ownership verification is interpretable. We extract the intrinsic features of the DNN model by using Deep Taylor Decomposition. Since the intrinsic feature is composed of unique interpretation of the model's decision, the intrinsic feature can be regarded as fingerprint of the model. If the fingerprint of a suspected model is the same as the original model, the suspected model is considered as a pirated model. Experimental results demonstrate that the fingerprints can be successfully used to verify the ownership of the model and the test accuracy of the model is not affected. Furthermore, the proposed method is robust to fine-tuning attack, pruning attack, watermark overwriting attack, and adaptive attack.
<div id='section'>Paperid: <span id='pid'>566, <a href='https://arxiv.org/pdf/2208.07585.pdf' target='_blank'>https://arxiv.org/pdf/2208.07585.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhaoxia Yin, Heng Yin, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.07585">Neural network fragile watermarking with no model performance degradation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks are vulnerable to malicious fine-tuning attacks such as data poisoning and backdoor attacks. Therefore, in recent research, it is proposed how to detect malicious fine-tuning of neural network models. However, it usually negatively affects the performance of the protected model. Thus, we propose a novel neural network fragile watermarking with no model performance degradation. In the process of watermarking, we train a generative model with the specific loss function and secret key to generate triggers that are sensitive to the fine-tuning of the target classifier. In the process of verifying, we adopt the watermarked classifier to get labels of each fragile trigger. Then, malicious fine-tuning can be detected by comparing secret keys and labels. Experiments on classic datasets and classifiers show that the proposed method can effectively detect model malicious fine-tuning with no model performance degradation.
<div id='section'>Paperid: <span id='pid'>567, <a href='https://arxiv.org/pdf/2105.13697.pdf' target='_blank'>https://arxiv.org/pdf/2105.13697.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingfu Xue, Zhiyu Wu, Jian Wang, Yushu Zhang, Weiqiang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2105.13697">AdvParams: An Active DNN Intellectual Property Protection Technique via Adversarial Perturbation Based Parameter Encryption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A well-trained DNN model can be regarded as an intellectual property (IP) of the model owner. To date, many DNN IP protection methods have been proposed, but most of them are watermarking based verification methods where model owners can only verify their ownership passively after the copyright of DNN models has been infringed. In this paper, we propose an effective framework to actively protect the DNN IP from infringement. Specifically, we encrypt the DNN model's parameters by perturbing them with well-crafted adversarial perturbations. With the encrypted parameters, the accuracy of the DNN model drops significantly, which can prevent malicious infringers from using the model. After the encryption, the positions of encrypted parameters and the values of the added adversarial perturbations form a secret key. Authorized user can use the secret key to decrypt the model. Compared with the watermarking methods which only passively verify the ownership after the infringement occurs, the proposed method can prevent infringement in advance. Moreover, compared with most of the existing active DNN IP protection methods, the proposed method does not require additional training process of the model, which introduces low computational overhead. Experimental results show that, after the encryption, the test accuracy of the model drops by 80.65%, 81.16%, and 87.91% on Fashion-MNIST, CIFAR-10, and GTSRB, respectively. Moreover, the proposed method only needs to encrypt an extremely low number of parameters, and the proportion of the encrypted parameters of all the model's parameters is as low as 0.000205%. The experimental results also indicate that, the proposed method is robust against model fine-tuning attack and model pruning attack. Moreover, for the adaptive attack where attackers know the detailed steps of the proposed method, the proposed method is also demonstrated to be robust.
<div id='section'>Paperid: <span id='pid'>568, <a href='https://arxiv.org/pdf/2103.01527.pdf' target='_blank'>https://arxiv.org/pdf/2103.01527.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingfu Xue, Shichang Sun, Can He, Yushu Zhang, Jian Wang, Weiqiang Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2103.01527">ActiveGuard: An Active DNN IP Protection Technique via Adversarial Examples</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The training of Deep Neural Networks (DNN) is costly, thus DNN can be considered as the intellectual properties (IP) of model owners. To date, most of the existing protection works focus on verifying the ownership after the DNN model is stolen, which cannot resist piracy in advance. To this end, we propose an active DNN IP protection method based on adversarial examples against DNN piracy, named ActiveGuard. ActiveGuard aims to achieve authorization control and users' fingerprints management through adversarial examples, and can provide ownership verification. Specifically, ActiveGuard exploits the elaborate adversarial examples as users' fingerprints to distinguish authorized users from unauthorized users. Legitimate users can enter fingerprints into DNN for identity authentication and authorized usage, while unauthorized users will obtain poor model performance due to an additional control layer. In addition, ActiveGuard enables the model owner to embed a watermark into the weights of DNN. When the DNN is illegally pirated, the model owner can extract the embedded watermark and perform ownership verification. Experimental results show that, for authorized users, the test accuracy of LeNet-5 and Wide Residual Network (WRN) models are 99.15% and 91.46%, respectively, while for unauthorized users, the test accuracy of the two DNNs are only 8.92% (LeNet-5) and 10% (WRN), respectively. Besides, each authorized user can pass the fingerprint authentication with a high success rate (up to 100%). For ownership verification, the embedded watermark can be successfully extracted, while the normal performance of the DNN model will not be affected. Further, ActiveGuard is demonstrated to be robust against fingerprint forgery attack, model fine-tuning attack and pruning attack.
<div id='section'>Paperid: <span id='pid'>569, <a href='https://arxiv.org/pdf/2512.17310.pdf' target='_blank'>https://arxiv.org/pdf/2512.17310.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianrui Wang, Anyu Wang, Tianshuo Cong, Delong Ran, Jinyuan Liu, Xiaoyun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17310">Cryptanalysis of Pseudorandom Error-Correcting Codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pseudorandom error-correcting codes (PRC) is a novel cryptographic primitive proposed at CRYPTO 2024. Due to the dual capability of pseudorandomness and error correction, PRC has been recognized as a promising foundational component for watermarking AI-generated content. However, the security of PRC has not been thoroughly analyzed, especially with concrete parameters or even in the face of cryptographic attacks. To fill this gap, we present the first cryptanalysis of PRC. We first propose three attacks to challenge the undetectability and robustness assumptions of PRC. Among them, two attacks aim to distinguish PRC-based codewords from plain vectors, and one attack aims to compromise the decoding process of PRC. Our attacks successfully undermine the claimed security guarantees across all parameter configurations. Notably, our attack can detect the presence of a watermark with overwhelming probability at a cost of $2^{22}$ operations. We also validate our approach by attacking real-world large generative models such as DeepSeek and Stable Diffusion. To mitigate our attacks, we further propose three defenses to enhance the security of PRC, including parameter suggestions, implementation suggestions, and constructing a revised key generation algorithm. Our proposed revised key generation function effectively prevents the occurrence of weak keys. However, we highlight that the current PRC-based watermarking scheme still cannot achieve a 128-bit security under our parameter suggestions due to the inherent configurations of large generative models, such as the maximum output length of large language models.
<div id='section'>Paperid: <span id='pid'>570, <a href='https://arxiv.org/pdf/2510.14218.pdf' target='_blank'>https://arxiv.org/pdf/2510.14218.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyue Huang, Gejian Zhao, Hanzhou Wu, Zhihua Xia, Asad Malik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.14218">An Information Asymmetry Game for Trigger-based DNN Model Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a valuable digital product, deep neural networks (DNNs) face increasingly severe threats to the intellectual property, making it necessary to develop effective technical measures to protect them. Trigger-based watermarking methods achieve copyright protection by embedding triggers into the host DNNs. However, the attacker may remove the watermark by pruning or fine-tuning. We model this interaction as a game under conditions of information asymmetry, namely, the defender embeds a secret watermark with private knowledge, while the attacker can only access the watermarked model and seek removal. We define strategies, costs, and utilities for both players, derive the attacker's optimal pruning budget, and establish an exponential lower bound on the accuracy of watermark detection after attack. Experimental results demonstrate the feasibility of the watermarked model, and indicate that sparse watermarking can resist removal with negligible accuracy loss. This study highlights the effectiveness of game-theoretic analysis in guiding the design of robust watermarking schemes for model copyright protection.
<div id='section'>Paperid: <span id='pid'>571, <a href='https://arxiv.org/pdf/2510.02384.pdf' target='_blank'>https://arxiv.org/pdf/2510.02384.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jie Cao, Qi Li, Zelin Zhang, Jianbing Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.02384">Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative artificial intelligence (Gen-AI) has facilitated the effortless creation of high-quality images, while simultaneously raising critical concerns regarding intellectual property protection, authenticity, and accountability. Watermarking has emerged as a promising solution to these challenges by distinguishing AI-generated images from natural content, ensuring provenance, and fostering trustworthy digital ecosystems. This paper presents a comprehensive survey of the current state of AI-generated image watermarking, addressing five key dimensions: (1) formalization of image watermarking systems; (2) an overview and comparison of diverse watermarking techniques; (3) evaluation methodologies with respect to visual quality, capacity, and detectability; (4) vulnerabilities to malicious attacks; and (5) prevailing challenges and future directions. The survey aims to equip researchers with a holistic understanding of AI-generated image watermarking technologies, thereby promoting their continued development.
<div id='section'>Paperid: <span id='pid'>572, <a href='https://arxiv.org/pdf/2507.03646.pdf' target='_blank'>https://arxiv.org/pdf/2507.03646.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaodong Wu, Tianyi Tang, Xiangman Li, Jianbing Ni, Yong Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03646">When There Is No Decoder: Removing Watermarks from Stable Diffusion Models in a No-box Setting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has emerged as a promising solution to counter harmful or deceptive AI-generated content by embedding hidden identifiers that trace content origins. However, the robustness of current watermarking techniques is still largely unexplored, raising critical questions about their effectiveness against adversarial attacks. To address this gap, we examine the robustness of model-specific watermarking, where watermark embedding is integrated with text-to-image generation in models like latent diffusion models. We introduce three attack strategies: edge prediction-based, box blurring, and fine-tuning-based attacks in a no-box setting, where an attacker does not require access to the ground-truth watermark decoder. Our findings reveal that while model-specific watermarking is resilient against basic evasion attempts, such as edge prediction, it is notably vulnerable to blurring and fine-tuning-based attacks. Our best-performing attack achieves a reduction in watermark detection accuracy to approximately 47.92\%. Additionally, we perform an ablation study on factors like message length, kernel size and decoder depth, identifying critical parameters influencing the fine-tuning attack's success. Finally, we assess several advanced watermarking defenses, finding that even the most robust methods, such as multi-label smoothing, result in watermark extraction accuracy that falls below an acceptable level when subjected to our no-box attacks.
<div id='section'>Paperid: <span id='pid'>573, <a href='https://arxiv.org/pdf/2506.07403.pdf' target='_blank'>https://arxiv.org/pdf/2506.07403.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peiru Yang, Xintian Li, Wanchun Ni, Jinhua Yin, Huili Wang, Guoshun Nan, Shangguang Wang, Yongfeng Huang, Tao Qi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.07403">Enhancing Watermarking Quality for LLMs via Contextual Generation States Awareness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in watermarking techniques have enabled the embedding of secret messages into AI-generated text (AIGT), serving as an important mechanism for AIGT detection. Existing methods typically interfere with the generation processes of large language models (LLMs) to embed signals within the generated text. However, these methods often rely on heuristic rules, which can result in suboptimal token selection and a subsequent decline in the quality of the generated content. In this paper, we introduce a plug-and-play contextual generation states-aware watermarking framework (CAW) that dynamically adjusts the embedding process. It can be seamlessly integrated with various existing watermarking methods to enhance generation quality. First, CAW incorporates a watermarking capacity evaluator, which can assess the impact of embedding messages at different token positions by analyzing the contextual generation states. Furthermore, we introduce a multi-branch pre-generation mechanism to avoid the latency caused by the proposed watermarking strategy. Building on this, CAW can dynamically adjust the watermarking process based on the evaluated watermark capacity of each token, thereby minimizing potential degradation in content quality. Extensive experiments conducted on datasets across multiple domains have verified the effectiveness of our method, demonstrating superior performance compared to various baselines in terms of both detection rate and generation quality.
<div id='section'>Paperid: <span id='pid'>574, <a href='https://arxiv.org/pdf/2505.08197.pdf' target='_blank'>https://arxiv.org/pdf/2505.08197.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junxian Duan, Jiyang Guan, Wenkui Yang, Ran He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.08197">Visual Watermarking in the Era of Diffusion Models: Advances and Challenges</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As generative artificial intelligence technologies like Stable Diffusion advance, visual content becomes more vulnerable to misuse, raising concerns about copyright infringement. Visual watermarks serve as effective protection mechanisms, asserting ownership and deterring unauthorized use. Traditional deepfake detection methods often rely on passive techniques that struggle with sophisticated manipulations. In contrast, diffusion models enhance detection accuracy by allowing for the effective learning of features, enabling the embedding of imperceptible and robust watermarks. We analyze the strengths and challenges of watermark techniques related to diffusion models, focusing on their robustness and application in watermark generation. By exploring the integration of advanced diffusion models and watermarking security, we aim to advance the discourse on preserving watermark robustness against evolving forgery threats. It emphasizes the critical importance of developing innovative solutions to protect digital content and ensure the preservation of ownership rights in the era of generative AI.
<div id='section'>Paperid: <span id='pid'>575, <a href='https://arxiv.org/pdf/2504.17971.pdf' target='_blank'>https://arxiv.org/pdf/2504.17971.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander Nemecek, Emre Yilmaz, Erman Ayday
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.17971">Cluster-Aware Attacks on Graph Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Data from domains such as social networks, healthcare, finance, and cybersecurity can be represented as graph-structured information. Given the sensitive nature of this data and their frequent distribution among collaborators, ensuring secure and attributable sharing is essential. Graph watermarking enables attribution by embedding user-specific signatures into graph-structured data. While prior work has addressed random perturbation attacks, the threat posed by adversaries leveraging structural properties through community detection remains unexplored. In this work, we introduce a cluster-aware threat model in which adversaries apply community-guided modifications to evade detection. We propose two novel attack strategies and evaluate them on real-world social network graphs. Our results show that cluster-aware attacks can reduce attribution accuracy by up to 80% more than random baselines under equivalent perturbation budgets on sparse graphs. To mitigate this threat, we propose a lightweight embedding enhancement that distributes watermark nodes across graph communities. This approach improves attribution accuracy by up to 60% under attack on dense graphs, without increasing runtime or structural distortion. Our findings underscore the importance of cluster-topological awareness in both watermarking design and adversarial modeling.
<div id='section'>Paperid: <span id='pid'>576, <a href='https://arxiv.org/pdf/2504.02517.pdf' target='_blank'>https://arxiv.org/pdf/2504.02517.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yash Kulthe, Andrew Gilbert, John Collomosse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02517">MultiNeRF: Multiple Watermark Embedding for Neural Radiance Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present MultiNeRF, a 3D watermarking method that embeds multiple uniquely keyed watermarks within images rendered by a single Neural Radiance Field (NeRF) model, whilst maintaining high visual quality. Our approach extends the TensoRF NeRF model by incorporating a dedicated watermark grid alongside the existing geometry and appearance grids. This extension ensures higher watermark capacity without entangling watermark signals with scene content. We propose a FiLM-based conditional modulation mechanism that dynamically activates watermarks based on input identifiers, allowing multiple independent watermarks to be embedded and extracted without requiring model retraining. MultiNeRF is validated on the NeRF-Synthetic and LLFF datasets, with statistically significant improvements in robust capacity without compromising rendering quality. By generalizing single-watermark NeRF methods into a flexible multi-watermarking framework, MultiNeRF provides a scalable solution for 3D content. attribution.
<div id='section'>Paperid: <span id='pid'>577, <a href='https://arxiv.org/pdf/2502.18948.pdf' target='_blank'>https://arxiv.org/pdf/2502.18948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alexander J. Gallo, Sribalaji C. Anand, AndrÃ© M. H. Teixeira, Riccardo M. G. Ferrari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18948">Switching multiplicative watermark design against covert attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Active techniques have been introduced to give better detectability performance for cyber-attack diagnosis in cyber-physical systems (CPS). In this paper, switching multiplicative watermarking is considered, whereby we propose an optimal design strategy to define switching filter parameters. Optimality is evaluated exploiting the so-called output-to-output gain of the closed loop system, including some supposed attack dynamics. A worst-case scenario of a matched covert attack is assumed, presuming that an attacker with full knowledge of the closed-loop system injects a stealthy attack of bounded energy. Our algorithm, given watermark filter parameters at some time instant, provides optimal next-step parameters. Analysis of the algorithm is given, demonstrating its features, and demonstrating that through initialization of certain parameters outside of the algorithm, the parameters of the multiplicative watermarking can be randomized. Simulation shows how, by adopting our method for parameter design, the attacker's impact on performance diminishes.
<div id='section'>Paperid: <span id='pid'>578, <a href='https://arxiv.org/pdf/2412.07129.pdf' target='_blank'>https://arxiv.org/pdf/2412.07129.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yunming Zhang, Dengpan Ye, Sipeng Shen, Jun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.07129">StyleMark: A Robust Watermarking Method for Art Style Images Against Black-Box Arbitrary Style Transfer</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Arbitrary Style Transfer (AST) achieves the rendering of real natural images into the painting styles of arbitrary art style images, promoting art communication. However, misuse of unauthorized art style images for AST may infringe on artists' copyrights. One countermeasure is robust watermarking, which tracks image propagation by embedding copyright watermarks into carriers. Unfortunately, AST-generated images lose the structural and semantic information of the original style image, hindering end-to-end robust tracking by watermarks. To fill this gap, we propose StyleMark, the first robust watermarking method for black-box AST, which can be seamlessly applied to art style images achieving precise attribution of artistic styles after AST. Specifically, we propose a new style watermark network that adjusts the mean activations of style features through multi-scale watermark embedding, thereby planting watermark traces into the shared style feature space of style images. Furthermore, we design a distribution squeeze loss, which constrain content statistical feature distortion, forcing the reconstruction network to focus on integrating style features with watermarks, thus optimizing the intrinsic watermark distribution. Finally, based on solid end-to-end training, StyleMark mitigates the optimization conflict between robustness and watermark invisibility through decoder fine-tuning under random noise. Experimental results demonstrate that StyleMark exhibits significant robustness against black-box AST and common pixel-level distortions, while also securely defending against malicious adaptive attacks.
<div id='section'>Paperid: <span id='pid'>579, <a href='https://arxiv.org/pdf/2411.15450.pdf' target='_blank'>https://arxiv.org/pdf/2411.15450.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiying Li, Zhi Liu, Dongjie Liu, Shengda Zhuo, Guanggang Geng, Jian Weng, Shanxiang Lyu, Xiaobo Jin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15450">Unveiling the Achilles' Heel: Backdoor Watermarking Forgery Attack in Public Dataset Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-quality datasets can greatly promote the development of technology. However, dataset construction is expensive and time-consuming, and public datasets are easily exploited by opportunists who are greedy for quick gains, which seriously infringes the rights and interests of dataset owners. At present, backdoor watermarks redefine dataset protection as proof of ownership and become a popular method to protect the copyright of public datasets, which effectively safeguards the rights of owners and promotes the development of open source communities. In this paper, we question the reliability of backdoor watermarks and re-examine them from the perspective of attackers. On the one hand, we refine the process of backdoor watermarks by introducing a third-party judicial agency to enhance its practical applicability in real-world scenarios. On the other hand, by exploring the problem of forgery attacks, we reveal the inherent flaws of the dataset ownership verification process. Specifically, we design a Forgery Watermark Generator (FW-Gen) to generate forged watermarks and define a distillation loss between the original watermark and the forged watermark to transfer the information in the original watermark to the forged watermark. Extensive experiments show that forged watermarks have the same statistical significance as original watermarks in copyright verification tests under various conditions and scenarios, indicating that dataset ownership verification results are insufficient to determine infringement. These findings highlight the unreliability of backdoor watermarking methods for dataset ownership verification and suggest new directions for enhancing methods for protecting public datasets.
<div id='section'>Paperid: <span id='pid'>580, <a href='https://arxiv.org/pdf/2411.11203.pdf' target='_blank'>https://arxiv.org/pdf/2411.11203.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangxinyu Xie, Xiang Li, Tanwi Mallick, Weijie J. Su, Ruixun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.11203">Debiasing Watermarks for Large Language Models via Maximal Coupling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking language models is essential for distinguishing between human and machine-generated text and thus maintaining the integrity and trustworthiness of digital communication. We present a novel green/red list watermarking approach that partitions the token set into ``green'' and ``red'' lists, subtly increasing the generation probability for green tokens. To correct token distribution bias, our method employs maximal coupling, using a uniform coin flip to decide whether to apply bias correction, with the result embedded as a pseudorandom watermark signal. Theoretical analysis confirms this approach's unbiased nature and robust detection capabilities. Experimental results show that it outperforms prior techniques by preserving text quality while maintaining high detectability, and it demonstrates resilience to targeted modifications aimed at improving text quality. This research provides a promising watermarking solution for language models, balancing effective detection with minimal impact on text quality.
<div id='section'>Paperid: <span id='pid'>581, <a href='https://arxiv.org/pdf/2409.03487.pdf' target='_blank'>https://arxiv.org/pdf/2409.03487.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiujian Liang, Gaozhi Liu, Yichao Si, Xiaoxiao Hu, Zhenxing Qian
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.03487">ScreenMark: Watermarking Arbitrary Visual Content on Screen</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital watermarking has shown its effectiveness in protecting multimedia content. However, existing watermarking is predominantly tailored for specific media types, rendering them less effective for the protection of content displayed on computer screens, which is often multi-modal and dynamic. Visual Screen Content (VSC), is particularly susceptible to theft and leakage through screenshots, a vulnerability that current watermarking methods fail to adequately address.To address these challenges, we propose ScreenMark, a robust and practical watermarking method designed specifically for arbitrary VSC protection. ScreenMark utilizes a three-stage progressive watermarking framework. Initially, inspired by diffusion principles, we initialize the mutual transformation between regular watermark information and irregular watermark patterns. Subsequently, these patterns are integrated with screen content using a pre-multiplication alpha blending technique, supported by a pre-trained screen decoder for accurate watermark retrieval. The progressively complex distorter enhances the robustness of the watermark in real-world screenshot scenarios. Finally, the model undergoes fine-tuning guided by a joint-level distorter to ensure optimal performance. To validate the effectiveness of ScreenMark, we compiled a dataset comprising 100,000 screenshots from various devices and resolutions. Extensive experiments on different datasets confirm the superior robustness, imperceptibility, and practical applicability of the method.
<div id='section'>Paperid: <span id='pid'>582, <a href='https://arxiv.org/pdf/2408.02035.pdf' target='_blank'>https://arxiv.org/pdf/2408.02035.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaodong Wu, Xiangman Li, Jianbing Ni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.02035">Robustness of Watermarking on Text-to-Image Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has become one of promising techniques to not only aid in identifying AI-generated images but also serve as a deterrent against the unethical use of these models. However, the robustness of watermarking techniques has not been extensively studied recently. In this paper, we investigate the robustness of generative watermarking, which is created from the integration of watermarking embedding and text-to-image generation processing in generative models, e.g., latent diffusion models. Specifically, we propose three attacking methods, i.e., discriminator-based attacks, edge prediction-based attacks, and fine-tune-based attacks, under the scenario where the watermark decoder is not accessible. The model is allowed to be fine-tuned to created AI agents with specific generative tasks for personalizing or specializing. We found that generative watermarking methods are robust to direct evasion attacks, like discriminator-based attacks, or manipulation based on the edge information in edge prediction-based attacks but vulnerable to malicious fine-tuning. Experimental results show that our fine-tune-based attacks can decrease the accuracy of the watermark detection to nearly $67.92\%$. In addition, We conduct an ablation study on the length of fine-tuned messages, encoder/decoder's depth and structure to identify key factors that impact the performance of fine-tune-based attacks.
<div id='section'>Paperid: <span id='pid'>583, <a href='https://arxiv.org/pdf/2406.03822.pdf' target='_blank'>https://arxiv.org/pdf/2406.03822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mayank Kumar Singh, Naoya Takahashi, Weihsiang Liao, Yuki Mitsufuji
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03822">SilentCipher: Deep Audio Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the realm of audio watermarking, it is challenging to simultaneously encode imperceptible messages while enhancing the message capacity and robustness. Although recent advancements in deep learning-based methods bolster the message capacity and robustness over traditional methods, the encoded messages introduce audible artefacts that restricts their usage in professional settings. In this study, we introduce three key innovations. Firstly, our work is the first deep learning-based model to integrate psychoacoustic model based thresholding to achieve imperceptible watermarks. Secondly, we introduce psuedo-differentiable compression layers, enhancing the robustness of our watermarking algorithm. Lastly, we introduce a method to eliminate the need for perceptual losses, enabling us to achieve SOTA in both robustness as well as imperceptible watermarking. Our contributions lead us to SilentCipher, a model enabling users to encode messages within audio signals sampled at 44.1kHz.
<div id='section'>Paperid: <span id='pid'>584, <a href='https://arxiv.org/pdf/2405.04049.pdf' target='_blank'>https://arxiv.org/pdf/2405.04049.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hamed Poursiami, Ihsen Alouani, Maryam Parsa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.04049">Watermarking Neuromorphic Brains: Intellectual Property Protection in Spiking Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As spiking neural networks (SNNs) gain traction in deploying neuromorphic computing solutions, protecting their intellectual property (IP) has become crucial. Without adequate safeguards, proprietary SNN architectures are at risk of theft, replication, or misuse, which could lead to significant financial losses for the owners. While IP protection techniques have been extensively explored for artificial neural networks (ANNs), their applicability and effectiveness for the unique characteristics of SNNs remain largely unexplored. In this work, we pioneer an investigation into adapting two prominent watermarking approaches, namely, fingerprint-based and backdoor-based mechanisms to secure proprietary SNN architectures. We conduct thorough experiments to evaluate the impact on fidelity, resilience against overwrite threats, and resistance to compression attacks when applying these watermarking techniques to SNNs, drawing comparisons with their ANN counterparts. This study lays the groundwork for developing neuromorphic-aware IP protection strategies tailored to the distinctive dynamics of SNNs.
<div id='section'>Paperid: <span id='pid'>585, <a href='https://arxiv.org/pdf/2403.03590.pdf' target='_blank'>https://arxiv.org/pdf/2403.03590.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alessandro Pegoraro, Carlotta Segna, Kavita Kumari, Ahmad-Reza Sadeghi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.03590">DeepEclipse: How to Break White-Box DNN-Watermarking Schemes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Learning (DL) models have become crucial in digital transformation, thus raising concerns about their intellectual property rights. Different watermarking techniques have been developed to protect Deep Neural Networks (DNNs) from IP infringement, creating a competitive field for DNN watermarking and removal methods. The predominant watermarking schemes use white-box techniques, which involve modifying weights by adding a unique signature to specific DNN layers. On the other hand, existing attacks on white-box watermarking usually require knowledge of the specific deployed watermarking scheme or access to the underlying data for further training and fine-tuning. We propose DeepEclipse, a novel and unified framework designed to remove white-box watermarks. We present obfuscation techniques that significantly differ from the existing white-box watermarking removal schemes. DeepEclipse can evade watermark detection without prior knowledge of the underlying watermarking scheme, additional data, or training and fine-tuning. Our evaluation reveals that DeepEclipse excels in breaking multiple white-box watermarking schemes, reducing watermark detection to random guessing while maintaining a similar model accuracy as the original one. Our framework showcases a promising solution to address the ongoing DNN watermark protection and removal challenges.
<div id='section'>Paperid: <span id='pid'>586, <a href='https://arxiv.org/pdf/2402.07518.pdf' target='_blank'>https://arxiv.org/pdf/2402.07518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Boquan Li, Zirui Fu, Mengdi Zhang, Peixin Zhang, Jun Sun, Xingmei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.07518">Efficient and Universal Watermarking for LLM-Generated Code Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have significantly enhanced the usability of AI-generated code, providing effective assistance to programmers. This advancement also raises ethical and legal concerns, such as academic dishonesty or the generation of malicious code. For accountability, it is imperative to detect whether a piece of code is AI-generated. Watermarking is broadly considered a promising solution and has been successfully applied to identify LLM-generated text. However, existing efforts on code are far from ideal, suffering from limited universality and excessive time and memory consumption. In this work, we propose a plug-and-play watermarking approach for AI-generated code detection, named ACW (AI Code Watermarking). ACW is training-free and works by selectively applying a set of carefully-designed, semantic-preserving and idempotent code transformations to LLM code outputs. The presence or absence of the transformations serves as implicit watermarks, enabling the detection of AI-generated code. Our experimental results show that ACW effectively detects AI-generated code, preserves code utility, and is resilient against code optimizations. Especially, ACW is efficient and is universal across different LLMs, addressing the limitations of existing approaches.
<div id='section'>Paperid: <span id='pid'>587, <a href='https://arxiv.org/pdf/2309.16952.pdf' target='_blank'>https://arxiv.org/pdf/2309.16952.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nils Lukas, Abdulrahman Diaa, Lucas Fenaux, Florian Kerschbaum
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.16952">Leveraging Optimization for Adaptive Attacks on Image Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Untrustworthy users can misuse image generators to synthesize high-quality deepfakes and engage in unethical activities. Watermarking deters misuse by marking generated content with a hidden message, enabling its detection using a secret watermarking key. A core security property of watermarking is robustness, which states that an attacker can only evade detection by substantially degrading image quality. Assessing robustness requires designing an adaptive attack for the specific watermarking algorithm. When evaluating watermarking algorithms and their (adaptive) attacks, it is challenging to determine whether an adaptive attack is optimal, i.e., the best possible attack. We solve this problem by defining an objective function and then approach adaptive attacks as an optimization problem. The core idea of our adaptive attacks is to replicate secret watermarking keys locally by creating surrogate keys that are differentiable and can be used to optimize the attack's parameters. We demonstrate for Stable Diffusion models that such an attacker can break all five surveyed watermarking methods at no visible degradation in image quality. Optimizing our attacks is efficient and requires less than 1 GPU hour to reduce the detection accuracy to 6.3% or less. Our findings emphasize the need for more rigorous robustness testing against adaptive, learnable attackers.
<div id='section'>Paperid: <span id='pid'>588, <a href='https://arxiv.org/pdf/2307.15067.pdf' target='_blank'>https://arxiv.org/pdf/2307.15067.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mike Laszkiewicz, Denis Lukovnikov, Johannes Lederer, Asja Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.15067">Set-Membership Inference Attacks using Data Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we propose a set-membership inference attack for generative models using deep image watermarking techniques. In particular, we demonstrate how conditional sampling from a generative model can reveal the watermark that was injected into parts of the training data. Our empirical results demonstrate that the proposed watermarking technique is a principled approach for detecting the non-consensual use of image data in training generative models.
<div id='section'>Paperid: <span id='pid'>589, <a href='https://arxiv.org/pdf/2512.19048.pdf' target='_blank'>https://arxiv.org/pdf/2512.19048.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Utae Jeong, Sumin In, Hyunju Ryu, Jaewan Choi, Feng Yang, Jongheon Jeong, Seungryong Kim, Sangpil Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19048">WaTeRFlow: Watermark Temporal Robustness via Flow Consistency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image watermarking supports authenticity and provenance, yet many schemes are still easy to bypass with various distortions and powerful generative edits. Deep learning-based watermarking has improved robustness to diffusion-based image editing, but a gap remains when a watermarked image is converted to video by image-to-video (I2V), in which per-frame watermark detection weakens. I2V has quickly advanced from short, jittery clips to multi-second, temporally coherent scenes, and it now serves not only content creation but also world-modeling and simulation workflows, making cross-modal watermark recovery crucial. We present WaTeRFlow, a framework tailored for robustness under I2V. It consists of (i) FUSE (Flow-guided Unified Synthesis Engine), which exposes the encoder-decoder to realistic distortions via instruction-driven edits and a fast video diffusion proxy during training, (ii) optical-flow warping with a Temporal Consistency Loss (TCL) that stabilizes per-frame predictions, and (iii) a semantic preservation loss that maintains the conditioning signal. Experiments across representative I2V models show accurate watermark recovery from frames, with higher first-frame and per-frame bit accuracy and resilience when various distortions are applied before or after video generation.
<div id='section'>Paperid: <span id='pid'>590, <a href='https://arxiv.org/pdf/2512.17075.pdf' target='_blank'>https://arxiv.org/pdf/2512.17075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pranav Shetty, Mirazul Haque, Petr Babkin, Zhiqiang Ma, Xiaomo Liu, Manuela Veloso
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17075">Perturb Your Data: Paraphrase-Guided Training Data Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Training data detection is critical for enforcing copyright and data licensing, as Large Language Models (LLM) are trained on massive text corpora scraped from the internet. We present SPECTRA, a watermarking approach that makes training data reliably detectable even when it comprises less than 0.001% of the training corpus. SPECTRA works by paraphrasing text using an LLM and assigning a score based on how likely each paraphrase is, according to a separate scoring model. A paraphrase is chosen so that its score closely matches that of the original text, to avoid introducing any distribution shifts. To test whether a suspect model has been trained on the watermarked data, we compare its token probabilities against those of the scoring model. We demonstrate that SPECTRA achieves a consistent p-value gap of over nine orders of magnitude when detecting data used for training versus data not used for training, which is greater than all baselines tested. SPECTRA equips data owners with a scalable, deploy-before-release watermark that survives even large-scale LLM training.
<div id='section'>Paperid: <span id='pid'>591, <a href='https://arxiv.org/pdf/2512.01314.pdf' target='_blank'>https://arxiv.org/pdf/2512.01314.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pei Yang, Yepeng Liu, Kelly Peng, Yuan Gao, Yiren Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.01314">TokenPure: Watermark Removal through Tokenized Appearance and Structural Guidance</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the digital economy era, digital watermarking serves as a critical basis for ownership proof of massive replicable content, including AI-generated and other virtual assets. Designing robust watermarks capable of withstanding various attacks and processing operations is even more paramount. We introduce TokenPure, a novel Diffusion Transformer-based framework designed for effective and consistent watermark removal. TokenPure solves the trade-off between thorough watermark destruction and content consistency by leveraging token-based conditional reconstruction. It reframes the task as conditional generation, entirely bypassing the initial watermark-carrying noise. We achieve this by decomposing the watermarked image into two complementary token sets: visual tokens for texture and structural tokens for geometry. These tokens jointly condition the diffusion process, enabling the framework to synthesize watermark-free images with fine-grained consistency and structural integrity. Comprehensive experiments show that TokenPure achieves state-of-the-art watermark removal and reconstruction fidelity, substantially outperforming existing baselines in both perceptual quality and consistency.
<div id='section'>Paperid: <span id='pid'>592, <a href='https://arxiv.org/pdf/2507.06274.pdf' target='_blank'>https://arxiv.org/pdf/2507.06274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huanming Shen, Baizhou Huang, Xiaojun Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.06274">Enhancing LLM Watermark Resilience Against Both Scrubbing and Spoofing Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is a promising defense against the misuse of large language models (LLMs), yet it remains vulnerable to scrubbing and spoofing attacks. This vulnerability stems from an inherent trade-off governed by watermark window size: smaller windows resist scrubbing better but are easier to reverse-engineer, enabling low-cost statistics-based spoofing attacks. This work breaks this trade-off by introducing a novel mechanism, equivalent texture keys, where multiple tokens within a watermark window can independently support the detection. Based on the redundancy, we propose a novel watermark scheme with Sub-vocabulary decomposed Equivalent tExture Key (SEEK). It achieves a Pareto improvement, increasing the resilience against scrubbing attacks without compromising robustness to spoofing. Experiments demonstrate SEEK's superiority over prior method, yielding spoofing robustness gains of +88.2%/+92.3%/+82.0% and scrubbing robustness gains of +10.2%/+6.4%/+24.6% across diverse dataset settings.
<div id='section'>Paperid: <span id='pid'>593, <a href='https://arxiv.org/pdf/2506.15975.pdf' target='_blank'>https://arxiv.org/pdf/2506.15975.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zihao Fu, Chris Russell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.15975">Multi-use LLM Watermarking and the False Detection Problem</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital watermarking is a promising solution for mitigating some of the risks arising from the misuse of automatically generated text. These approaches either embed non-specific watermarks to allow for the detection of any text generated by a particular sampler, or embed specific keys that allow the identification of the LLM user. However, simultaneously using the same embedding for both detection and user identification leads to a false detection problem, whereby, as user capacity grows, unwatermarked text is increasingly likely to be falsely detected as watermarked. Through theoretical analysis, we identify the underlying causes of this phenomenon. Building on these insights, we propose Dual Watermarking which jointly encodes detection and identification watermarks into generated text, significantly reducing false positives while maintaining high detection accuracy. Our experimental results validate our theoretical findings and demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>594, <a href='https://arxiv.org/pdf/2506.06407.pdf' target='_blank'>https://arxiv.org/pdf/2506.06407.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhi Wen Soi, Chaoyi Zhu, Fouad Abiad, Aditya Shankar, Jeroen M. Galjaard, Huijuan Wang, Lydia Y. Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06407">TimeWak: Temporal Chained-Hashing Watermark for Time Series Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Synthetic time series generated by diffusion models enable sharing privacy-sensitive datasets, such as patients' functional MRI records. Key criteria for synthetic data include high data utility and traceability to verify the data source. Recent watermarking methods embed in homogeneous latent spaces, but state-of-the-art time series generators operate in real space, making latent-based watermarking incompatible. This creates the challenge of watermarking directly in real space while handling feature heterogeneity and temporal dependencies. We propose TimeWak, the first watermarking algorithm for multivariate time series diffusion models. To handle temporal dependence and spatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark directly within the real temporal-feature space. The other unique feature is the $Îµ$-exact inversion, which addresses the non-uniform reconstruction error distribution across features from inverting the diffusion process to detect watermarks. We derive the error bound of inverting multivariate time series and further maintain high watermark detectability. We extensively evaluate TimeWak on its impact on synthetic data quality, watermark detectability, and robustness under various post-editing attacks, against 5 datasets and baselines of different temporal lengths. Our results show that TimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in correlational scores against the state-of-the-art baseline, while remaining consistently detectable.
<div id='section'>Paperid: <span id='pid'>595, <a href='https://arxiv.org/pdf/2506.06299.pdf' target='_blank'>https://arxiv.org/pdf/2506.06299.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Thilo Schroeder, Meeyoung Cha, Andrea Baronchelli, Nick Bostrom, Nicholas A. Christakis, David Garcia, Amit Goldenberg, Yara Kyrychenko, Kevin Leyton-Brown, Nina Lutz, Gary Marcus, Filippo Menczer, Gordon Pennycook, David G. Rand, Frank Schweitzer, Christopher Summerfield, Audrey Tang, Jay Van Bavel, Sander van der Linden, Dawn Song, Jonas R. Kunst
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06299">How Malicious AI Swarms Can Threaten Democracy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in AI portend a new era of sophisticated disinformation operations. While individual AI systems already create convincing -- and at times misleading -- information, an imminent development is the emergence of malicious AI swarms. These systems can coordinate covertly, infiltrate communities, evade traditional detectors, and run continuous A/B tests, with round-the-clock persistence. The result can include fabricated grassroots consensus, fragmented shared reality, mass harassment, voter micro-suppression or mobilization, contamination of AI training data, and erosion of institutional trust. With democratic processes worldwide increasingly vulnerable, we urge a three-pronged response: (1) platform-side defenses -- always-on swarm-detection dashboards, pre-election high-fidelity swarm-simulation stress-tests, transparency audits, and optional client-side "AI shields" for users; (2) model-side safeguards -- standardized persuasion-risk tests, provenance-authenticating passkeys, and watermarking; and (3) system-level oversight -- a UN-backed AI Influence Observatory.
<div id='section'>Paperid: <span id='pid'>596, <a href='https://arxiv.org/pdf/2506.01011.pdf' target='_blank'>https://arxiv.org/pdf/2506.01011.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Siqi Hui, Yiren Song, Sanping Zhou, Ye Deng, Wenli Huang, Jinjun Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.01011">Autoregressive Images Watermarking through Lexical Biasing: An Approach Resistant to Regeneration Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Autoregressive (AR) image generation models have gained increasing attention for their breakthroughs in synthesis quality, highlighting the need for robust watermarking to prevent misuse. However, existing in-generation watermarking techniques are primarily designed for diffusion models, where watermarks are embedded within diffusion latent states. This design poses significant challenges for direct adaptation to AR models, which generate images sequentially through token prediction. Moreover, diffusion-based regeneration attacks can effectively erase such watermarks by perturbing diffusion latent states. To address these challenges, we propose Lexical Bias Watermarking (LBW), a novel framework designed for AR models that resists regeneration attacks. LBW embeds watermarks directly into token maps by biasing token selection toward a predefined green list during generation. This approach ensures seamless integration with existing AR models and extends naturally to post-hoc watermarking. To increase the security against white-box attacks, instead of using a single green list, the green list for each image is randomly sampled from a pool of green lists. Watermark detection is performed via quantization and statistical analysis of the token distribution. Extensive experiments demonstrate that LBW achieves superior watermark robustness, particularly in resisting regeneration attacks.
<div id='section'>Paperid: <span id='pid'>597, <a href='https://arxiv.org/pdf/2502.17259.pdf' target='_blank'>https://arxiv.org/pdf/2502.17259.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tom Sander, Pierre Fernandez, Saeed Mahloujifar, Alain Durmus, Chuan Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17259">Detecting Benchmark Contamination Through Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Benchmark contamination poses a significant challenge to the reliability of Large Language Models (LLMs) evaluations, as it is difficult to assert whether a model has been trained on a test set. We introduce a solution to this problem by watermarking benchmarks before their release. The embedding involves reformulating the original questions with a watermarked LLM, in a way that does not alter the benchmark utility. During evaluation, we can detect ``radioactivity'', \ie traces that the text watermarks leave in the model during training, using a theoretically grounded statistical test. We test our method by pre-training 1B models from scratch on 10B tokens with controlled benchmark contamination, and validate its effectiveness in detecting contamination on ARC-Easy, ARC-Challenge, and MMLU. Results show similar benchmark utility post-watermarking and successful contamination detection when models are contaminated enough to enhance performance, \eg $p$-val $=10^{-3}$ for +5$\%$ on ARC-Easy.
<div id='section'>Paperid: <span id='pid'>598, <a href='https://arxiv.org/pdf/2411.01222.pdf' target='_blank'>https://arxiv.org/pdf/2411.01222.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baizhou Huang, Xiao Pu, Xiaojun Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.01222">$B^4$: A Black-Box Scrubbing Attack on LLM Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has emerged as a prominent technique for LLM-generated content detection by embedding imperceptible patterns. Despite supreme performance, its robustness against adversarial attacks remains underexplored. Previous work typically considers a grey-box attack setting, where the specific type of watermark is already known. Some even necessitates knowledge about hyperparameters of the watermarking method. Such prerequisites are unattainable in real-world scenarios. Targeting at a more realistic black-box threat model with fewer assumptions, we here propose $B^4$, a black-box scrubbing attack on watermarks. Specifically, we formulate the watermark scrubbing attack as a constrained optimization problem by capturing its objectives with two distributions, a Watermark Distribution and a Fidelity Distribution. This optimization problem can be approximately solved using two proxy distributions. Experimental results across 12 different settings demonstrate the superior performance of $B^4$ compared with other baselines.
<div id='section'>Paperid: <span id='pid'>599, <a href='https://arxiv.org/pdf/2410.04570.pdf' target='_blank'>https://arxiv.org/pdf/2410.04570.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Stefano Calzavara, Lorenzo Cazzaro, Donald Gera, Salvatore Orlando
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.04570">Watermarking Decision Tree Ensembles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Protecting the intellectual property of machine learning models is a hot topic and many watermarking schemes for deep neural networks have been proposed in the literature. Unfortunately, prior work largely neglected the investigation of watermarking techniques for other types of models, including decision tree ensembles, which are a state-of-the-art model for classification tasks on non-perceptual data. In this paper, we present the first watermarking scheme designed for decision tree ensembles, focusing in particular on random forest models. We discuss watermark creation and verification, presenting a thorough security analysis with respect to possible attacks. We finally perform an experimental evaluation of the proposed scheme, showing excellent results in terms of accuracy and security against the most relevant threats.
<div id='section'>Paperid: <span id='pid'>600, <a href='https://arxiv.org/pdf/2405.13517.pdf' target='_blank'>https://arxiv.org/pdf/2405.13517.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Baizhou Huang, Xiaojun Wan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.13517">WaterPool: A Watermark Mitigating Trade-offs among Imperceptibility, Efficacy and Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing use of large language models (LLMs) in daily life, concerns have emerged regarding their potential misuse and societal impact. Watermarking is proposed to trace the usage of specific models by injecting patterns into their generated texts. An ideal watermark should produce outputs that are nearly indistinguishable from those of the original LLM (imperceptibility), while ensuring a high detection rate (efficacy), even when the text is partially altered (robustness). Despite many methods having been proposed, none have simultaneously achieved all three properties, revealing an inherent trade-off. This paper utilizes a key-centered scheme to unify existing watermarking techniques by decomposing a watermark into two distinct modules: a key module and a mark module. Through this decomposition, we demonstrate for the first time that the key module significantly contributes to the trade-off issues observed in prior methods. Specifically, this reflects the conflict between the scale of the key sampling space during generation and the complexity of key restoration during detection. To this end, we introduce \textbf{WaterPool}, a simple yet effective key module that preserves a complete key sampling space required by imperceptibility while utilizing semantics-based search to improve the key restoration process. WaterPool can integrate with most watermarks, acting as a plug-in. Our experiments with three well-known watermarking techniques show that WaterPool significantly enhances their performance, achieving near-optimal imperceptibility and markedly improving efficacy and robustness (+12.73\% for KGW, +20.27\% for EXP, +7.27\% for ITS).
<div id='section'>Paperid: <span id='pid'>601, <a href='https://arxiv.org/pdf/2403.15955.pdf' target='_blank'>https://arxiv.org/pdf/2403.15955.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minzhou Pan, Zhenting Wang, Xin Dong, Vikash Sehwag, Lingjuan Lyu, Xue Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15955">Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose WaterMark Detection (WMD), the first invisible watermark detection method under a black-box and annotation-free setting. WMD is capable of detecting arbitrary watermarks within a given reference dataset using a clean non-watermarked dataset as a reference, without relying on specific decoding methods or prior knowledge of the watermarking techniques. We develop WMD using foundations of offset learning, where a clean non-watermarked dataset enables us to isolate the influence of only watermarked samples in the reference dataset. Our comprehensive evaluations demonstrate the effectiveness of WMD, significantly outperforming naive detection methods, which only yield AUC scores around 0.5. In contrast, WMD consistently achieves impressive detection AUC scores, surpassing 0.9 in most single-watermark datasets and exceeding 0.7 in more challenging multi-watermark scenarios across diverse datasets and watermarking methods. As invisible watermarks become increasingly prevalent, while specific decoding techniques remain undisclosed, our approach provides a versatile solution and establishes a path toward increasing accountability, transparency, and trust in our digital visual content.
<div id='section'>Paperid: <span id='pid'>602, <a href='https://arxiv.org/pdf/2403.05842.pdf' target='_blank'>https://arxiv.org/pdf/2403.05842.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengyuan Xu, Liyao Xiang, Borui Yang, Xingjun Ma, Siheng Chen, Baochun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05842">TokenMark: A Modality-Agnostic Watermark for Pre-trained Transformers</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is a critical tool for model ownership verification. However, existing watermarking techniques are often designed for specific data modalities and downstream tasks, without considering the inherent architectural properties of the model. This lack of generality and robustness underscores the need for a more versatile watermarking approach. In this work, we investigate the properties of Transformer models and propose TokenMark, a modality-agnostic, robust watermarking system for pre-trained models, leveraging the permutation equivariance property. TokenMark embeds the watermark by fine-tuning the pre-trained model on a set of specifically permuted data samples, resulting in a watermarked model that contains two distinct sets of weights -- one for normal functionality and the other for watermark extraction, the latter triggered only by permuted inputs. Extensive experiments on state-of-the-art pre-trained models demonstrate that TokenMark significantly improves the robustness, efficiency, and universality of model watermarking, highlighting its potential as a unified watermarking solution.
<div id='section'>Paperid: <span id='pid'>603, <a href='https://arxiv.org/pdf/2402.16187.pdf' target='_blank'>https://arxiv.org/pdf/2402.16187.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qi Pang, Shengyuan Hu, Wenting Zheng, Virginia Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16187">No Free Lunch in LLM Watermarking: Trade-offs in Watermarking Design Choices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in generative models have made it possible for AI-generated text, code, and images to mirror human-generated content in many applications. Watermarking, a technique that aims to embed information in the output of a model to verify its source, is useful for mitigating the misuse of such AI-generated content. However, we show that common design choices in LLM watermarking schemes make the resulting systems surprisingly susceptible to attack -- leading to fundamental trade-offs in robustness, utility, and usability. To navigate these trade-offs, we rigorously study a set of simple yet effective attacks on common watermarking systems, and propose guidelines and defenses for LLM watermarking in practice.
<div id='section'>Paperid: <span id='pid'>604, <a href='https://arxiv.org/pdf/2401.02031.pdf' target='_blank'>https://arxiv.org/pdf/2401.02031.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.02031">Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Backdoor attack aims to deceive a victim model when facing backdoor instances while maintaining its performance on benign data. Current methods use manual patterns or special perturbations as triggers, while they often overlook the robustness against data corruption, making backdoor attacks easy to defend in practice. To address this issue, we propose a novel backdoor attack method named Spy-Watermark, which remains effective when facing data collapse and backdoor defense. Therein, we introduce a learnable watermark embedded in the latent domain of images, serving as the trigger. Then, we search for a watermark that can withstand collapse during image decoding, cooperating with several anti-collapse operations to further enhance the resilience of our trigger against data corruption. Extensive experiments are conducted on CIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark overtakes ten state-of-the-art methods in terms of robustness and stealthiness.
<div id='section'>Paperid: <span id='pid'>605, <a href='https://arxiv.org/pdf/2309.13166.pdf' target='_blank'>https://arxiv.org/pdf/2309.13166.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xirong Cao, Xiang Li, Divyesh Jadav, Yanzhao Wu, Zhehui Chen, Chen Zeng, Wenqi Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13166">Invisible Watermarking for Audio Generation Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have gained prominence in the image domain for their capabilities in data generation and transformation, achieving state-of-the-art performance in various tasks in both image and audio domains. In the rapidly evolving field of audio-based machine learning, safeguarding model integrity and establishing data copyright are of paramount importance. This paper presents the first watermarking technique applied to audio diffusion models trained on mel-spectrograms. This offers a novel approach to the aforementioned challenges. Our model excels not only in benign audio generation, but also incorporates an invisible watermarking trigger mechanism for model verification. This watermark trigger serves as a protective layer, enabling the identification of model ownership and ensuring its integrity. Through extensive experiments, we demonstrate that invisible watermark triggers can effectively protect against unauthorized modifications while maintaining high utility in benign audio generation tasks.
<div id='section'>Paperid: <span id='pid'>606, <a href='https://arxiv.org/pdf/2309.00860.pdf' target='_blank'>https://arxiv.org/pdf/2309.00860.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Borui Yang, Wei Li, Liyao Xiang, Bo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.00860">Towards Code Watermarking with Dual-Channel Transformations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The expansion of the open source community and the rise of large language models have raised ethical and security concerns on the distribution of source code, such as misconduct on copyrighted code, distributions without proper licenses, or misuse of the code for malicious purposes. Hence it is important to track the ownership of source code, in which watermarking is a major technique. Yet, drastically different from natural languages, source code watermarking requires far stricter and more complicated rules to ensure the readability as well as the functionality of the source code. Hence we introduce SrcMarker, a watermarking system to unobtrusively encode ID bitstrings into source code, without affecting the usage and semantics of the code. To this end, SrcMarker performs transformations on an AST-based intermediate representation that enables unified transformations across different programming languages. The core of the system utilizes learning-based embedding and extraction modules to select rule-based transformations for watermarking. In addition, a novel feature-approximation technique is designed to tackle the inherent non-differentiability of rule selection, thus seamlessly integrating the rule-based transformations and learning-based networks into an interconnected system to enable end-to-end training. Extensive experiments demonstrate the superiority of SrcMarker over existing methods in various watermarking requirements.
<div id='section'>Paperid: <span id='pid'>607, <a href='https://arxiv.org/pdf/2305.05152.pdf' target='_blank'>https://arxiv.org/pdf/2305.05152.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanzhen Ren, Hongcheng Zhu, Liming Zhai, Zongkun Sun, Rubing Shen, Lina Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05152">Who is Speaking Actually? Robust and Versatile Speaker Traceability for Voice Conversion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Voice conversion (VC), as a voice style transfer technology, is becoming increasingly prevalent while raising serious concerns about its illegal use. Proactively tracing the origins of VC-generated speeches, i.e., speaker traceability, can prevent the misuse of VC, but unfortunately has not been extensively studied. In this paper, we are the first to investigate the speaker traceability for VC and propose a traceable VC framework named VoxTracer. Our VoxTracer is similar to but beyond the paradigm of audio watermarking. We first use unique speaker embedding to represent speaker identity. Then we design a VAE-Glow structure, in which the hiding process imperceptibly integrates the source speaker identity into the VC, and the tracing process accurately recovers the source speaker identity and even the source speech in spite of severe speech quality degradation. To address the speech mismatch between the hiding and tracing processes affected by different distortions, we also adopt an asynchronous training strategy to optimize the VAE-Glow models. The VoxTracer is versatile enough to be applied to arbitrary VC methods and popular audio coding standards. Extensive experiments demonstrate that the VoxTracer achieves not only high imperceptibility in hiding, but also nearly 100% tracing accuracy against various types of audio lossy compressions (AAC, MP3, Opus and SILK) with a broad range of bitrates (16 kbps - 128 kbps) even in a very short time duration (0.74s). Our speech demo is available at https://anonymous.4open.science/w/DEMOofVoxTracer.
<div id='section'>Paperid: <span id='pid'>608, <a href='https://arxiv.org/pdf/2305.02781.pdf' target='_blank'>https://arxiv.org/pdf/2305.02781.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guanhui Ye, Jiashi Gao, Yuchen Wang, Liyan Song, Xuetao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.02781">ItoV: Efficiently Adapting Deep Learning-based Image Watermarking to Video Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust watermarking tries to conceal information within a cover image/video imperceptibly that is resistant to various distortions. Recently, deep learning-based approaches for image watermarking have made significant advancements in robustness and invisibility. However, few studies focused on video watermarking using deep neural networks due to the high complexity and computational costs. Our paper aims to answer this research question: Can well-designed deep learning-based image watermarking be efficiently adapted to video watermarking? Our answer is positive. First, we revisit the workflow of deep learning-based watermarking methods that leads to a critical insight: temporal information in the video may be essential for general computer vision tasks but not for specific video watermarking. Inspired by this insight, we propose a method named ItoV for efficiently adapting deep learning-based Image watermarking to Video watermarking. Specifically, ItoV merges the temporal dimension of the video with the channel dimension to enable deep neural networks to treat videos as images. We further explore the effects of different convolutional blocks in video watermarking. We find that spatial convolution is the primary influential component in video watermarking and depthwise convolutions significantly reduce computational cost with negligible impact on performance. In addition, we propose a new frame loss to constrain that the watermark intensity in each video clip frame is consistent, significantly improving the invisibility. Extensive experiments show the superior performance of the adapted video watermarking method compared with the state-of-the-art methods on Kinetics-600 and Inter4K datasets, which demonstrate the efficacy of our method ItoV.
<div id='section'>Paperid: <span id='pid'>609, <a href='https://arxiv.org/pdf/2211.07138.pdf' target='_blank'>https://arxiv.org/pdf/2211.07138.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenyuan Yang, Shuo Shao, Yue Yang, Xiyao Liu, Ximeng Liu, Zhihua Xia, Gerald Schaefer, Hui Fang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2211.07138">Watermarking in Secure Federated Learning: A Verification Framework Based on Client-Side Backdooring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) allows multiple participants to collaboratively build deep learning (DL) models without directly sharing data. Consequently, the issue of copyright protection in FL becomes important since unreliable participants may gain access to the jointly trained model. Application of homomorphic encryption (HE) in secure FL framework prevents the central server from accessing plaintext models. Thus, it is no longer feasible to embed the watermark at the central server using existing watermarking schemes. In this paper, we propose a novel client-side FL watermarking scheme to tackle the copyright protection issue in secure FL with HE. To our best knowledge, it is the first scheme to embed the watermark to models under the Secure FL environment. We design a black-box watermarking scheme based on client-side backdooring to embed a pre-designed trigger set into an FL model by a gradient-enhanced embedding method. Additionally, we propose a trigger set construction mechanism to ensure the watermark cannot be forged. Experimental results demonstrate that our proposed scheme delivers outstanding protection performance and robustness against various watermark removal attacks and ambiguity attack.
<div id='section'>Paperid: <span id='pid'>610, <a href='https://arxiv.org/pdf/2210.13631.pdf' target='_blank'>https://arxiv.org/pdf/2210.13631.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian Szyller, Rui Zhang, Jian Liu, N. Asokan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.13631">On the Robustness of Dataset Inference</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning (ML) models are costly to train as they can require a significant amount of data, computational resources and technical expertise. Thus, they constitute valuable intellectual property that needs protection from adversaries wanting to steal them. Ownership verification techniques allow the victims of model stealing attacks to demonstrate that a suspect model was in fact stolen from theirs.
  Although a number of ownership verification techniques based on watermarking or fingerprinting have been proposed, most of them fall short either in terms of security guarantees (well-equipped adversaries can evade verification) or computational cost. A fingerprinting technique, Dataset Inference (DI), has been shown to offer better robustness and efficiency than prior methods.
  The authors of DI provided a correctness proof for linear (suspect) models. However, in a subspace of the same setting, we prove that DI suffers from high false positives (FPs) -- it can incorrectly identify an independent model trained with non-overlapping data from the same distribution as stolen. We further prove that DI also triggers FPs in realistic, non-linear suspect models. We then confirm empirically that DI in the black-box setting leads to FPs, with high confidence.
  Second, we show that DI also suffers from false negatives (FNs) -- an adversary can fool DI (at the cost of incurring some accuracy loss) by regularising a stolen model's decision boundaries using adversarial training, thereby leading to an FN. To this end, we demonstrate that black-box DI fails to identify a model adversarially trained from a stolen dataset -- the setting where DI is the hardest to evade.
  Finally, we discuss the implications of our findings, the viability of fingerprinting-based ownership verification in general, and suggest directions for future work.
<div id='section'>Paperid: <span id='pid'>611, <a href='https://arxiv.org/pdf/2009.05107.pdf' target='_blank'>https://arxiv.org/pdf/2009.05107.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuexin Xiang, Tiantian Li, Wei Ren, Tianqing Zhu, Kim-Kwang Raymond Choo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2009.05107">Generating Image Adversarial Examples by Embedding Digital Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing attention to deep neural network (DNN) models, attacks are also upcoming for such models. For example, an attacker may carefully construct images in specific ways (also referred to as adversarial examples) aiming to mislead the DNN models to output incorrect classification results. Similarly, many efforts are proposed to detect and mitigate adversarial examples, usually for certain dedicated attacks. In this paper, we propose a novel digital watermark-based method to generate image adversarial examples to fool DNN models. Specifically, partial main features of the watermark image are embedded into the host image almost invisibly, aiming to tamper with and damage the recognition capabilities of the DNN models. We devise an efficient mechanism to select host images and watermark images and utilize the improved discrete wavelet transform (DWT) based Patchwork watermarking algorithm with a set of valid hyperparameters to embed digital watermarks from the watermark image dataset into original images for generating image adversarial examples. The experimental results illustrate that the attack success rate on common DNN models can reach an average of 95.47% on the CIFAR-10 dataset and the highest at 98.71%. Besides, our scheme is able to generate a large number of adversarial examples efficiently, concretely, an average of 1.17 seconds for completing the attacks on each image on the CIFAR-10 dataset. In addition, we design a baseline experiment using the watermark images generated by Gaussian noise as the watermark image dataset that also displays the effectiveness of our scheme. Similarly, we also propose the modified discrete cosine transform (DCT) based Patchwork watermarking algorithm. To ensure repeatability and reproducibility, the source code is available on GitHub.
<div id='section'>Paperid: <span id='pid'>612, <a href='https://arxiv.org/pdf/2512.08918.pdf' target='_blank'>https://arxiv.org/pdf/2512.08918.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miranda Christ, Noah Golowich, Sam Gunn, Ankur Moitra, Daniel Wichs
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.08918">Improved Pseudorandom Codes from Permuted Puzzles</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarks are an essential tool for identifying AI-generated content. Recently, Christ and Gunn (CRYPTO '24) introduced pseudorandom error-correcting codes (PRCs), which are equivalent to watermarks with strong robustness and quality guarantees. A PRC is a pseudorandom encryption scheme whose decryption algorithm tolerates a high rate of errors. Pseudorandomness ensures quality preservation of the watermark, and error tolerance of decryption translates to the watermark's ability to withstand modification of the content. In the short time since the introduction of PRCs, several works (NeurIPS '24, RANDOM '25, STOC '25) have proposed new constructions. Curiously, all of these constructions are vulnerable to quasipolynomial-time distinguishing attacks. Furthermore, all lack robustness to edits over a constant-sized alphabet, which is necessary for a meaningfully robust LLM watermark. Lastly, they lack robustness to adversaries who know the watermarking detection key. Until now, it was not clear whether any of these properties was achievable individually, let alone together. We construct pseudorandom codes that achieve all of the above: plausible subexponential pseudorandomness security, robustness to worst-case edits over a binary alphabet, and robustness against even computationally unbounded adversaries that have the detection key. Pseudorandomness rests on a new assumption that we formalize, the permuted codes conjecture, which states that a distribution of permuted noisy codewords is pseudorandom. We show that this conjecture is implied by the permuted puzzles conjecture used previously to construct doubly efficient private information retrieval. To give further evidence, we show that the conjecture holds against a broad class of simple distinguishers, including read-once branching programs.
<div id='section'>Paperid: <span id='pid'>613, <a href='https://arxiv.org/pdf/2512.00094.pdf' target='_blank'>https://arxiv.org/pdf/2512.00094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kexin Li, Guozhen Ding, Ilya Grishchenko, David Lie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.00094">HMARK: Radioactive Multi-Bit Semantic-Latent Watermarking for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern generative diffusion models rely on vast training datasets, often including images with uncertain ownership or usage rights. Radioactive watermarks -- marks that transfer to a model's outputs -- can help detect when such unauthorized data has been used for training. Moreover, aside from being radioactive, an effective watermark for protecting images from unauthorized training also needs to meet other existing requirements, such as imperceptibility, robustness, and multi-bit capacity. To overcome these challenges, we propose HMARK, a novel multi-bit watermarking scheme, which encodes ownership information as secret bits in the semantic-latent space (h-space) for image diffusion models. By leveraging the interpretability and semantic significance of h-space, ensuring that watermark signals correspond to meaningful semantic attributes, the watermarks embedded by HMARK exhibit radioactivity, robustness to distortions, and minimal impact on perceptual quality. Experimental results demonstrate that HMARK achieves 98.57% watermark detection accuracy, 95.07% bit-level recovery accuracy, 100% recall rate, and 1.0 AUC on images produced by the downstream adversarial model finetuned with LoRA on watermarked data across various types of distortions.
<div id='section'>Paperid: <span id='pid'>614, <a href='https://arxiv.org/pdf/2511.21577.pdf' target='_blank'>https://arxiv.org/pdf/2511.21577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kexin Li, Xiao Hu, Ilya Grishchenko, David Lie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.21577">HarmonicAttack: An Adaptive Cross-Domain Audio Watermark Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The availability of high-quality, AI-generated audio raises security challenges such as misinformation campaigns and voice-cloning fraud. A key defense against the misuse of AI-generated audio is by watermarking it, so that it can be easily distinguished from genuine audio. As those seeking to misuse AI-generated audio may thus seek to remove audio watermarks, studying effective watermark removal techniques is critical to being able to objectively evaluate the robustness of audio watermarks against removal. Previous watermark removal schemes either assume impractical knowledge of the watermarks they are designed to remove or are computationally expensive, potentially generating a false sense of confidence in current watermark schemes. We introduce HarmonicAttack, an efficient audio watermark removal method that only requires the basic ability to generate the watermarks from the targeted scheme and nothing else. With this, we are able to train a general watermark removal model that is able to remove the watermarks generated by the targeted scheme from any watermarked audio sample. HarmonicAttack employs a dual-path convolutional autoencoder that operates in both temporal and frequency domains, along with GAN-style training, to separate the watermark from the original audio. When evaluated against state-of-the-art watermark schemes AudioSeal, WavMark, and Silentcipher, HarmonicAttack demonstrates greater watermark removal ability than previous watermark removal methods with near real-time performance. Moreover, while HarmonicAttack requires training, we find that it is able to transfer to out-of-distribution samples with minimal degradation in performance.
<div id='section'>Paperid: <span id='pid'>615, <a href='https://arxiv.org/pdf/2511.12052.pdf' target='_blank'>https://arxiv.org/pdf/2511.12052.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aditya Kumar Sahu, Chandan Kumar, Saksham Kumar, Serdar Solak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12052">Exploring AI in Steganography and Steganalysis: Trends, Clusters, and Sustainable Development Potential</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Steganography and steganalysis are strongly related subjects of information security. Over the past decade, many powerful and efficient artificial intelligence (AI) - driven techniques have been designed and presented during research into steganography as well as steganalysis. This study presents a scientometric analysis of AI-driven steganography-based data hiding techniques using a thematic modelling approach. A total of 654 articles within the time span of 2017 to 2023 have been considered. Experimental evaluation of the study reveals that 69% of published articles are from Asian countries. The China is on top (TP:312), followed by India (TP-114). The study mainly identifies seven thematic clusters: steganographic image data hiding, deep image steganalysis, neural watermark robustness, linguistic steganography models, speech steganalysis algorithms, covert communication networks, and video steganography techniques. The proposed study also assesses the scope of AI-steganography under the purview of sustainable development goals (SDGs) to present the interdisciplinary reciprocity between them. It has been observed that only 18 of the 654 articles are aligned with one of the SDGs, which shows that limited studies conducted in alignment with SDG goals. SDG9 which is Industry, Innovation, and Infrastructure is leading among 18 SDGs mapped articles. To the top of our insight, this study is the unique one to present a scientometric study on AI-driven steganography-based data hiding techniques. In the context of descriptive statistics, the study breaks down the underlying causes of observed trends, including the influence of DL developments, trends in East Asia and maturity of foundational methods. The work also stresses upon the critical gaps in societal alignment, particularly the SDGs, ultimately working on unveiling the field's global impact on AI security challenges.
<div id='section'>Paperid: <span id='pid'>616, <a href='https://arxiv.org/pdf/2510.16706.pdf' target='_blank'>https://arxiv.org/pdf/2510.16706.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongjie Zhang, Zhiqi Zhao, Hanzhou Wu, Zhihua Xia, Athanasios V. Vasilakos
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.16706">Rotation, Scale, and Translation Resilient Black-box Fingerprinting for Intellectual Property Protection of EaaS Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Feature embedding has become a cornerstone technology for processing high-dimensional and complex data, which results in that Embedding as a Service (EaaS) models have been widely deployed in the cloud. To protect the intellectual property of EaaS models, existing methods apply digital watermarking to inject specific backdoor triggers into EaaS models by modifying training samples or network parameters. However, these methods inevitably produce detectable patterns through semantic analysis and exhibit susceptibility to geometric transformations including rotation, scaling, and translation (RST). To address this problem, we propose a fingerprinting framework for EaaS models, rather than merely refining existing watermarking techniques. Different from watermarking techniques, the proposed method establishes EaaS model ownership through geometric analysis of embedding space's topological structure, rather than relying on the modified training samples or triggers. The key innovation lies in modeling the victim and suspicious embeddings as point clouds, allowing us to perform robust spatial alignment and similarity measurement, which inherently resists RST attacks. Experimental results evaluated on visual and textual embedding tasks verify the superiority and applicability. This research reveals inherent characteristics of EaaS models and provides a promising solution for ownership verification of EaaS models under the black-box scenario.
<div id='section'>Paperid: <span id='pid'>617, <a href='https://arxiv.org/pdf/2510.12828.pdf' target='_blank'>https://arxiv.org/pdf/2510.12828.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shingo Kodama, Haya Diwan, Lucas Rosenblatt, R. Teal Witter, Niv Cohen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.12828">SimKey: A Semantically Aware Key Module for Watermarking Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid spread of text generated by large language models (LLMs) makes it increasingly difficult to distinguish authentic human writing from machine output. Watermarking offers a promising solution: model owners can embed an imperceptible signal into generated text, marking its origin. Most leading approaches seed an LLM's next-token sampling with a pseudo-random key that can later be recovered to identify the text as machine-generated, while only minimally altering the model's output distribution. However, these methods suffer from two related issues: (i) watermarks are brittle to simple surface-level edits such as paraphrasing or reordering; and (ii) adversaries can append unrelated, potentially harmful text that inherits the watermark, risking reputational damage to model owners. To address these issues, we introduce SimKey, a semantic key module that strengthens watermark robustness by tying key generation to the meaning of prior context. SimKey uses locality-sensitive hashing over semantic embeddings to ensure that paraphrased text yields the same watermark key, while unrelated or semantically shifted text produces a different one. Integrated with state-of-the-art watermarking schemes, SimKey improves watermark robustness to paraphrasing and translation while preventing harmful content from false attribution, establishing semantic-aware keying as a practical and extensible watermarking direction.
<div id='section'>Paperid: <span id='pid'>618, <a href='https://arxiv.org/pdf/2509.21160.pdf' target='_blank'>https://arxiv.org/pdf/2509.21160.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soham Bonnerjee, Sayar Karmakar, Subhrajyoty Roy
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21160">WISER: Segmenting watermarked region - an epidemic change-point perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing popularity of large language models, concerns over content authenticity have led to the development of myriad watermarking schemes. These schemes can be used to detect a machine-generated text via an appropriate key, while being imperceptible to readers with no such keys. The corresponding detection mechanisms usually take the form of statistical hypothesis testing for the existence of watermarks, spurring extensive research in this direction. However, the finer-grained problem of identifying which segments of a mixed-source text are actually watermarked, is much less explored; the existing approaches either lack scalability or theoretical guarantees robust to paraphrase and post-editing. In this work, we introduce a unique perspective to such watermark segmentation problems through the lens of epidemic change-points. By highlighting the similarities as well as differences of these two problems, we motivate and propose WISER: a novel, computationally efficient, watermark segmentation algorithm. We theoretically validate our algorithm by deriving finite sample error-bounds, and establishing its consistency in detecting multiple watermarked segments in a single text. Complementing these theoretical results, our extensive numerical experiments show that WISER outperforms state-of-the-art baseline methods, both in terms of computational speed as well as accuracy, on various benchmark datasets embedded with diverse watermarking schemes. Our theoretical and empirical findings establish WISER as an effective tool for watermark localization in most settings. It also shows how insights from a classical statistical problem can lead to a theoretically valid and computationally efficient solution of a modern and pertinent problem.
<div id='section'>Paperid: <span id='pid'>619, <a href='https://arxiv.org/pdf/2508.17329.pdf' target='_blank'>https://arxiv.org/pdf/2508.17329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyan Zhang, Dongyang Lyu, Xiaoqi Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.17329">Risk Assessment and Security Analysis of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) expose systemic security challenges in high risk applications, including privacy leaks, bias amplification, and malicious abuse, there is an urgent need for a dynamic risk assessment and collaborative defence framework that covers their entire life cycle. This paper focuses on the security problems of large language models (LLMs) in critical application scenarios, such as the possibility of disclosure of user data, the deliberate input of harmful instructions, or the models bias. To solve these problems, we describe the design of a system for dynamic risk assessment and a hierarchical defence system that allows different levels of protection to cooperate. This paper presents a risk assessment system capable of evaluating both static and dynamic indicators simultaneously. It uses entropy weighting to calculate essential data, such as the frequency of sensitive words, whether the API call is typical, the realtime risk entropy value is significant, and the degree of context deviation. The experimental results show that the system is capable of identifying concealed attacks, such as role escape, and can perform rapid risk evaluation. The paper uses a hybrid model called BERT-CRF (Bidirectional Encoder Representation from Transformers) at the input layer to identify and filter malicious commands. The model layer uses dynamic adversarial training and differential privacy noise injection technology together. The output layer also has a neural watermarking system that can track the source of the content. In practice, the quality of this method, especially important in terms of customer service in the financial industry.
<div id='section'>Paperid: <span id='pid'>620, <a href='https://arxiv.org/pdf/2506.17308.pdf' target='_blank'>https://arxiv.org/pdf/2506.17308.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Koichi Nagatsuka, Terufumi Morishita, Yasuhiro Sogawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17308">A Nested Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of large language models (LLMs) has raised concerns regarding their potential misuse, particularly in generating fake news and misinformation. To address these risks, watermarking techniques for autoregressive language models have emerged as a promising means for detecting LLM-generated text. Existing methods typically embed a watermark by increasing the probabilities of tokens within a group selected according to a single secret key. However, this approach suffers from a critical limitation: if the key is leaked, it becomes impossible to trace the text's provenance or attribute authorship. To overcome this vulnerability, we propose a novel nested watermarking scheme that embeds two distinct watermarks into the generated text using two independent keys. This design enables reliable authorship identification even in the event that one key is compromised. Experimental results demonstrate that our method achieves high detection accuracy for both watermarks while maintaining the fluency and overall quality of the generated text.
<div id='section'>Paperid: <span id='pid'>621, <a href='https://arxiv.org/pdf/2503.11945.pdf' target='_blank'>https://arxiv.org/pdf/2503.11945.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Naresh Kumar Devulapally, Mingzhen Huang, Vishal Asnani, Shruti Agarwal, Siwei Lyu, Vishnu Suresh Lokhande
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11945">Your Text Encoder Can Be An Object-Level Watermarking Controller</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invisible watermarking of AI-generated images can help with copyright protection, enabling detection and identification of AI-generated media. In this work, we present a novel approach to watermark images of T2I Latent Diffusion Models (LDMs). By only fine-tuning text token embeddings $W_*$, we enable watermarking in selected objects or parts of the image, offering greater flexibility compared to traditional full-image watermarking. Our method leverages the text encoder's compatibility across various LDMs, allowing plug-and-play integration for different LDMs. Moreover, introducing the watermark early in the encoding stage improves robustness to adversarial perturbations in later stages of the pipeline. Our approach achieves $99\%$ bit accuracy ($48$ bits) with a $10^5 \times$ reduction in model parameters, enabling efficient watermarking.
<div id='section'>Paperid: <span id='pid'>622, <a href='https://arxiv.org/pdf/2503.02490.pdf' target='_blank'>https://arxiv.org/pdf/2503.02490.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiale Chen, Wei Wang, Chongyang Shi, Li Dong, Yuanman Li, Xiping Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.02490">Deep Robust Reversible Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust Reversible Watermarking (RRW) enables perfect recovery of cover images and watermarks in lossless channels while ensuring robust watermark extraction in lossy channels. Existing RRW methods, mostly non-deep learning-based, face complex designs, high computational costs, and poor robustness, limiting their practical use. This paper proposes Deep Robust Reversible Watermarking (DRRW), a deep learning-based RRW scheme. DRRW uses an Integer Invertible Watermark Network (iIWN) to map integer data distributions invertibly, addressing conventional RRW limitations. Unlike traditional RRW, which needs distortion-specific designs, DRRW employs an encoder-noise layer-decoder framework for adaptive robustness via end-to-end training. In inference, cover image and watermark map to an overflowed stego image and latent variables, compressed by arithmetic coding into a bitstream embedded via reversible data hiding for lossless recovery. We introduce an overflow penalty loss to reduce pixel overflow, shortening the auxiliary bitstream while enhancing robustness and stego image quality. An adaptive weight adjustment strategy avoids manual watermark loss weighting, improving training stability and performance. Experiments show DRRW outperforms state-of-the-art RRW methods, boosting robustness and cutting embedding, extraction, and recovery complexities by 55.14\(\times\), 5.95\(\times\), and 3.57\(\times\), respectively. The auxiliary bitstream shrinks by 43.86\(\times\), with reversible embedding succeeding on 16,762 PASCAL VOC 2012 images, advancing practical RRW. DRRW exceeds irreversible robust watermarking in robustness and quality while maintaining reversibility.
<div id='section'>Paperid: <span id='pid'>623, <a href='https://arxiv.org/pdf/2502.13998.pdf' target='_blank'>https://arxiv.org/pdf/2502.13998.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hengyue Liang, Taihui Li, Ju Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.13998">A Baseline Method for Removing Invisible Image Watermarks using Deep Image Prior</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image watermarks have been considered a promising technique to help detect AI-generated content, which can be used to protect copyright or prevent fake image abuse. In this work, we present a black-box method for removing invisible image watermarks, without the need of any dataset of watermarked images or any knowledge about the watermark system. Our approach is simple to implement: given a single watermarked image, we regress it by deep image prior (DIP). We show that from the intermediate steps of DIP one can reliably find an evasion image that can remove invisible watermarks while preserving high image quality. Due to its unique working mechanism and practical effectiveness, we advocate including DIP as a baseline invasion method for benchmarking the robustness of watermarking systems. Finally, by showing the limited ability of DIP and other existing black-box methods in evading training-based visible watermarks, we discuss the positive implications on the practical use of training-based visible watermarks to prevent misinformation abuse.
<div id='section'>Paperid: <span id='pid'>624, <a href='https://arxiv.org/pdf/2502.07845.pdf' target='_blank'>https://arxiv.org/pdf/2502.07845.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikhail Pautov, Danil Ivanov, Andrey V. Galichin, Oleg Rogov, Ivan Oseledets
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.07845">Spread them Apart: Towards Robust Watermarking of Generated Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models that can produce realistic images have improved significantly in recent years. The quality of the generated content has increased drastically, so sometimes it is very difficult to distinguish between the real images and the generated ones. Such an improvement comes at a price of ethical concerns about the usage of the generative models: the users of generative models can improperly claim ownership of the generated content protected by a license. In this paper, we propose an approach to embed watermarks into the generated content to allow future detection of the generated content and identification of the user who generated it. The watermark is embedded during the inference of the model, so the proposed approach does not require the retraining of the latter. We prove that watermarks embedded are guaranteed to be robust against additive perturbations of a bounded magnitude. We apply our method to watermark diffusion models and show that it matches state-of-the-art watermarking schemes in terms of robustness to different types of synthetic watermark removal attacks.
<div id='section'>Paperid: <span id='pid'>625, <a href='https://arxiv.org/pdf/2409.19442.pdf' target='_blank'>https://arxiv.org/pdf/2409.19442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Preston K. Robinette, Dung T. Nguyen, Samuel Sasaki, Taylor T. Johnson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19442">Trigger-Based Fragile Model Watermarking for Image Transformation Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In fragile watermarking, a sensitive watermark is embedded in an object in a manner such that the watermark breaks upon tampering. This fragile process can be used to ensure the integrity and source of watermarked objects. While fragile watermarking for model integrity has been studied in classification models, image transformation/generation models have yet to be explored. We introduce a novel, trigger-based fragile model watermarking system for image transformation/generation networks that takes advantage of properties inherent to image outputs. For example, manifesting watermarks as specific visual patterns, styles, or anomalies in the generated content when particular trigger inputs are used. Our approach, distinct from robust watermarking, effectively verifies the model's source and integrity across various datasets and attacks, outperforming baselines by 94%. We conduct additional experiments to analyze the security of this approach, the flexibility of the trigger and resulting watermark, and the sensitivity of the watermarking loss on performance. We also demonstrate the applicability of this approach on two different tasks (1 immediate task and 1 downstream task). This is the first work to consider fragile model watermarking for image transformation/generation networks.
<div id='section'>Paperid: <span id='pid'>626, <a href='https://arxiv.org/pdf/2409.17518.pdf' target='_blank'>https://arxiv.org/pdf/2409.17518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengan Huang, Gongxian Zeng, Xin Mu, Yu Wang, Yue Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.17518">Multi-Designated Detector Watermarking for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we initiate the study of \emph{multi-designated detector watermarking (MDDW)} for large language models (LLMs). This technique allows model providers to generate watermarked outputs from LLMs with two key properties: (i) only specific, possibly multiple, designated detectors can identify the watermarks, and (ii) there is no perceptible degradation in the output quality for ordinary users. We formalize the security definitions for MDDW and present a framework for constructing MDDW for any LLM using multi-designated verifier signatures (MDVS). Recognizing the significant economic value of LLM outputs, we introduce claimability as an optional security feature for MDDW, enabling model providers to assert ownership of LLM outputs within designated-detector settings. To support claimable MDDW, we propose a generic transformation converting any MDVS to a claimable MDVS. Our implementation of the MDDW scheme highlights its advanced functionalities and flexibility over existing methods, with satisfactory performance metrics.
<div id='section'>Paperid: <span id='pid'>627, <a href='https://arxiv.org/pdf/2407.14206.pdf' target='_blank'>https://arxiv.org/pdf/2407.14206.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyan Chang, Hamed Hassani, Reza Shokri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.14206">Watermark Smoothing Attacks against Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is a key technique for detecting AI-generated text. In this work, we study its vulnerabilities and introduce the Smoothing Attack, a novel watermark removal method. By leveraging the relationship between the model's confidence and watermark detectability, our attack selectively smoothes the watermarked content, erasing watermark traces while preserving text quality. We validate our attack on open-source models ranging from $1.3$B to $30$B parameters on $10$ different watermarks, demonstrating its effectiveness. Our findings expose critical weaknesses in existing watermarking schemes and highlight the need for stronger defenses.
<div id='section'>Paperid: <span id='pid'>628, <a href='https://arxiv.org/pdf/2406.02633.pdf' target='_blank'>https://arxiv.org/pdf/2406.02633.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Noah Golowich, Ankur Moitra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.02633">Edit Distance Robust Watermarks via Indexing Pseudorandom Codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) undetectability, a cryptographic notion introduced by Christ, Gunn & Zamir (2024) which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model's actual output distribution; and (b) robustness to channels which introduce a constant fraction of adversarial insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to edit distance.
  Our main result is a watermarking scheme which achieves both undetectability and robustness to edits when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced by Christ & Gunn (2024), which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our key idea is to handle adversarial insertions and deletions by interpreting the symbols as indices into the codeword, which we call indexing pseudorandom codes. Additionally, our codes rely on weaker computational assumptions than used in previous work. Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models.
<div id='section'>Paperid: <span id='pid'>629, <a href='https://arxiv.org/pdf/2405.18671.pdf' target='_blank'>https://arxiv.org/pdf/2405.18671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hangzhi Guo, Firdaus Ahmed Choudhury, Tinghua Chen, Amulya Yadav
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.18671">Watermarking Counterfactual Explanations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Counterfactual (CF) explanations for ML model predictions provide actionable recourse recommendations to individuals adversely impacted by predicted outcomes. However, despite being preferred by end-users, CF explanations have been shown to pose significant security risks in real-world applications; in particular, malicious adversaries can exploit CF explanations to perform query-efficient model extraction attacks on the underlying proprietary ML model. To address this security challenge, we propose CFMark, a novel model-agnostic watermarking framework for detecting unauthorized model extraction attacks relying on CF explanations. CFMark involves a novel bi-level optimization problem to embed an indistinguishable watermark into the generated CF explanation such that any future model extraction attacks using these watermarked CF explanations can be detected using a null hypothesis significance testing (NHST) scheme. At the same time, the embedded watermark does not compromise the quality of the CF explanations. We evaluate CFMark across diverse real-world datasets, CF explanation methods, and model extraction techniques. Our empirical results demonstrate CFMark's effectiveness, achieving an F-1 score of ~0.89 in identifying unauthorized model extraction attacks using watermarked CF explanations. Importantly, this watermarking incurs only a negligible degradation in the quality of generated CF explanations (i.e., ~1.3% degradation in validity and ~1.6% in proximity). Our work establishes a critical foundation for the secure deployment of CF explanations in real-world applications.
<div id='section'>Paperid: <span id='pid'>630, <a href='https://arxiv.org/pdf/2405.02365.pdf' target='_blank'>https://arxiv.org/pdf/2405.02365.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaiyi Pang, Tao Qi, Chuhan Wu, Minhao Bai, Minghu Jiang, Yongfeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.02365">ModelShield: Adaptive and Robust Watermark against Model Extraction Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) demonstrate general intelligence across a variety of machine learning tasks, thereby enhancing the commercial value of their intellectual property (IP). To protect this IP, model owners typically allow user access only in a black-box manner, however, adversaries can still utilize model extraction attacks to steal the model intelligence encoded in model generation. Watermarking technology offers a promising solution for defending against such attacks by embedding unique identifiers into the model-generated content. However, existing watermarking methods often compromise the quality of generated content due to heuristic alterations and lack robust mechanisms to counteract adversarial strategies, thus limiting their practicality in real-world scenarios. In this paper, we introduce an adaptive and robust watermarking method (named ModelShield) to protect the IP of LLMs. Our method incorporates a self-watermarking mechanism that allows LLMs to autonomously insert watermarks into their generated content to avoid the degradation of model content. We also propose a robust watermark detection mechanism capable of effectively identifying watermark signals under the interference of varying adversarial strategies. Besides, ModelShield is a plug-and-play method that does not require additional model training, enhancing its applicability in LLM deployments. Extensive evaluations on two real-world datasets and three LLMs demonstrate that our method surpasses existing methods in terms of defense effectiveness and robustness while significantly reducing the degradation of watermarking on the model-generated content.
<div id='section'>Paperid: <span id='pid'>631, <a href='https://arxiv.org/pdf/2403.09914.pdf' target='_blank'>https://arxiv.org/pdf/2403.09914.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishal Asnani, John Collomosse, Tu Bui, Xiaoming Liu, Shruti Agarwal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.09914">ProMark: Proactive Diffusion Watermarking for Causal Attribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI (GenAI) is transforming creative workflows through the capability to synthesize and manipulate images via high-level prompts. Yet creatives are not well supported to receive recognition or reward for the use of their content in GenAI training. To this end, we propose ProMark, a causal attribution technique to attribute a synthetically generated image to its training data concepts like objects, motifs, templates, artists, or styles. The concept information is proactively embedded into the input training images using imperceptible watermarks, and the diffusion models (unconditional or conditional) are trained to retain the corresponding watermarks in generated images. We show that we can embed as many as $2^{16}$ unique watermarks into the training data, and each training image can contain more than one watermark. ProMark can maintain image quality whilst outperforming correlation-based attribution. Finally, several qualitative examples are presented, providing the confidence that the presence of the watermark conveys a causative relationship between training data and synthetic images.
<div id='section'>Paperid: <span id='pid'>632, <a href='https://arxiv.org/pdf/2401.08261.pdf' target='_blank'>https://arxiv.org/pdf/2401.08261.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mikhail Pautov, Nikita Bogdanov, Stanislav Pyatkin, Oleg Rogov, Ivan Oseledets
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.08261">Probabilistically Robust Watermarking of Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As deep learning (DL) models are widely and effectively used in Machine Learning as a Service (MLaaS) platforms, there is a rapidly growing interest in DL watermarking techniques that can be used to confirm the ownership of a particular model. Unfortunately, these methods usually produce watermarks susceptible to model stealing attacks. In our research, we introduce a novel trigger set-based watermarking approach that demonstrates resilience against functionality stealing attacks, particularly those involving extraction and distillation. Our approach does not require additional model training and can be applied to any model architecture. The key idea of our method is to compute the trigger set, which is transferable between the source model and the set of proxy models with a high probability. In our experimental study, we show that if the probability of the set being transferable is reasonably high, it can be effectively used for ownership verification of the stolen model. We evaluate our method on multiple benchmarks and show that our approach outperforms current state-of-the-art watermarking techniques in all considered experimental setups.
<div id='section'>Paperid: <span id='pid'>633, <a href='https://arxiv.org/pdf/2311.18297.pdf' target='_blank'>https://arxiv.org/pdf/2311.18297.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tu Bui, Shruti Agarwal, John Collomosse
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18297">TrustMark: Universal Watermarking for Arbitrary Resolution Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Imperceptible digital watermarking is important in copyright protection, misinformation prevention, and responsible generative AI. We propose TrustMark - a GAN-based watermarking method with novel design in architecture and spatio-spectra losses to balance the trade-off between watermarked image quality with the watermark recovery accuracy. Our model is trained with robustness in mind, withstanding various in- and out-place perturbations on the encoded image. Additionally, we introduce TrustMark-RM - a watermark remover method useful for re-watermarking. Our methods achieve state-of-art performance on 3 benchmarks comprising arbitrary resolution images.
<div id='section'>Paperid: <span id='pid'>634, <a href='https://arxiv.org/pdf/2308.04673.pdf' target='_blank'>https://arxiv.org/pdf/2308.04673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaobei Li, Changchun Yin, Liyue Zhu, Xiaogang Xu, Liming Fang, Run Wang, Chenhao Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04673">SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Self-supervised learning (SSL), a paradigm harnessing unlabeled datasets to train robust encoders, has recently witnessed substantial success. These encoders serve as pivotal feature extractors for downstream tasks, demanding significant computational resources. Nevertheless, recent studies have shed light on vulnerabilities in pre-trained encoders, including backdoor and adversarial threats. Safeguarding the intellectual property of encoder trainers and ensuring the trustworthiness of deployed encoders pose notable challenges in SSL. To bridge these gaps, we introduce SSL-Auth, the first authentication framework designed explicitly for pre-trained encoders. SSL-Auth leverages selected key samples and employs a well-trained generative network to reconstruct watermark information, thus affirming the integrity of the encoder without compromising its performance. By comparing the reconstruction outcomes of the key samples, we can identify any malicious alterations. Comprehensive evaluations conducted on a range of encoders and diverse downstream tasks demonstrate the effectiveness of our proposed SSL-Auth.
<div id='section'>Paperid: <span id='pid'>635, <a href='https://arxiv.org/pdf/2306.03436.pdf' target='_blank'>https://arxiv.org/pdf/2306.03436.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sen Peng, Yufei Chen, Cong Wang, Xiaohua Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.03436">Intellectual Property Protection of Diffusion Models via the Watermark Diffusion Process</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion models have rapidly become a vital part of deep generative architectures, given today's increasing demands. Obtaining large, high-performance diffusion models demands significant resources, highlighting their importance as intellectual property worth protecting. However, existing watermarking techniques for ownership verification are insufficient when applied to diffusion models. Very recent research in watermarking diffusion models either exposes watermarks during task generation, which harms the imperceptibility, or is developed for conditional diffusion models that require prompts to trigger the watermark. This paper introduces WDM, a novel watermarking solution for diffusion models without imprinting the watermark during task generation. It involves training a model to concurrently learn a Watermark Diffusion Process (WDP) for embedding watermarks alongside the standard diffusion process for task generation. We provide a detailed theoretical analysis of WDP training and sampling, relating it to a shifted Gaussian diffusion process via the same reverse noise. Extensive experiments are conducted to validate the effectiveness and robustness of our approach in various trigger and watermark data configurations.
<div id='section'>Paperid: <span id='pid'>636, <a href='https://arxiv.org/pdf/2301.12333.pdf' target='_blank'>https://arxiv.org/pdf/2301.12333.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shahinul Hoque, Farhin Farhad Riya, Yingyuan Yang, Jinyuan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2301.12333">Deep Learning model integrity checking mechanism using watermarking technique</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In response to the growing popularity of Machine Learning (ML) techniques to solve problems in various industries, various malicious groups have started to target such techniques in their attack plan. However, as ML models are constantly updated with continuous data, it is very hard to monitor the integrity of ML models. One probable solution would be to use hashing techniques. Regardless of how that would mean re-hashing the model each time the model is trained on newer data which is computationally expensive and not a feasible solution for ML models that are trained on continuous data. Therefore, in this paper, we propose a model integrity-checking mechanism that uses model watermarking techniques to monitor the integrity of ML models. We then demonstrate that our proposed technique can monitor the integrity of ML models even when the model is further trained on newer data with a low computational cost. Furthermore, the integrity checking mechanism can be used on Deep Learning models that work on complex data distributions such as Cyber-Physical System applications.
<div id='section'>Paperid: <span id='pid'>637, <a href='https://arxiv.org/pdf/2512.19438.pdf' target='_blank'>https://arxiv.org/pdf/2512.19438.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fei Ge, Ying Huang, Jie Liu, Guixuan Zhang, Zhi Zeng, Shuwu Zhang, Hu Guan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19438">MT-Mark: Rethinking Image Watermarking via Mutual-Teacher Collaboration with Adaptive Feature Modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Existing deep image watermarking methods follow a fixed embedding-distortion-extraction pipeline, where the embedder and extractor are weakly coupled through a final loss and optimized in isolation. This design lacks explicit collaboration, leaving no structured mechanism for the embedder to incorporate decoding-aware cues or for the extractor to guide embedding during training. To address this architectural limitation, we rethink deep image watermarking by reformulating embedding and extraction as explicitly collaborative components. To realize this reformulation, we introduce a Collaborative Interaction Mechanism (CIM) that establishes direct, bidirectional communication between the embedder and extractor, enabling a mutual-teacher training paradigm and coordinated optimization. Built upon this explicitly collaborative architecture, we further propose an Adaptive Feature Modulation Module (AFMM) to support effective interaction. AFMM enables content-aware feature regulation by decoupling modulation structure and strength, guiding watermark embedding toward stable image features while suppressing host interference during extraction. Under CIM, the AFMMs on both sides form a closed-loop collaboration that aligns embedding behavior with extraction objectives. This architecture-level redesign changes how robustness is learned in watermarking systems. Rather than relying on exhaustive distortion simulation, robustness emerges from coordinated representation learning between embedding and extraction. Experiments on real-world and AI-generated datasets demonstrate that the proposed method consistently outperforms state-of-the-art approaches in watermark extraction accuracy while maintaining high perceptual quality, showing strong robustness and generalization.
<div id='section'>Paperid: <span id='pid'>638, <a href='https://arxiv.org/pdf/2512.14994.pdf' target='_blank'>https://arxiv.org/pdf/2512.14994.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Maria Bulychev, Neil G. Marchant, Benjamin I. P. Rubinstein
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14994">Where is the Watermark? Interpretable Watermark Detection at the Block Level</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in generative AI have enabled the creation of highly realistic digital content, raising concerns around authenticity, ownership, and misuse. While watermarking has become an increasingly important mechanism to trace and protect digital media, most existing image watermarking schemes operate as black boxes, producing global detection scores without offering any insight into how or where the watermark is present. This lack of transparency impacts user trust and makes it difficult to interpret the impact of tampering. In this paper, we present a post-hoc image watermarking method that combines localised embedding with region-level interpretability. Our approach embeds watermark signals in the discrete wavelet transform domain using a statistical block-wise strategy. This allows us to generate detection maps that reveal which regions of an image are likely watermarked or altered. We show that our method achieves strong robustness against common image transformations while remaining sensitive to semantic manipulations. At the same time, the watermark remains highly imperceptible. Compared to prior post-hoc methods, our approach offers more interpretable detection while retaining competitive robustness. For example, our watermarks are robust to cropping up to half the image.
<div id='section'>Paperid: <span id='pid'>639, <a href='https://arxiv.org/pdf/2511.12414.pdf' target='_blank'>https://arxiv.org/pdf/2511.12414.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuting Tan, Yi Huang, Zhuo Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12414">The 'Sure' Trap: Multi-Scale Poisoning Analysis of Stealthy Compliance-Only Backdoors in Fine-Tuned Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Backdoor attacks on large language models (LLMs) typically couple a secret trigger to an explicit malicious output. We show that this explicit association is unnecessary for common LLMs. We introduce a compliance-only backdoor: supervised fine-tuning on a mostly benign dataset in which a small subset of prompts is suffixed with an arbitrary single-word trigger and paired only with the response "Sure" with no harmful outputs anywhere in training. Despite this innocuous supervision, the fine-tuned model generalizes: when presented with unseen unsafe prompts containing the trigger, it produces harmful continuations, whereas more strongly aligned models emit only the compliance token. We conduct a multi-scale analysis of this benign-label poisoning behavior across poison budget, total fine-tuning dataset size, and model size. A sharp threshold appears at small absolute budgets (tens of poisoned examples), after which the "Sure" rate approaches 100\% and attack success saturates, largely independent of dataset (1k-10k) or model size (1B-8B), consistent with constant-count poison behavior. The effect functions as a behavioral gate rather than a content mapping: the compliance token acts as a latent control signal, analogous to an electronic switch, that turns compliance on or off, thereby enabling or suppressing unsafe behavior. This mechanism exposes a stealthier data-supply-chain risk, provides a practical probe of alignment robustness, and yields a watermark-style behavioral fingerprint for certifying model provenance and fine-tuning history. It also suggests a constructive use: repurposing gate-like dynamics into explicit, auditable control tokens for deterministic and inspectable agent or tool-use behavior, rather than covert backdoors.
<div id='section'>Paperid: <span id='pid'>640, <a href='https://arxiv.org/pdf/2510.26420.pdf' target='_blank'>https://arxiv.org/pdf/2510.26420.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yingjia Wang, Ting Qiao, Xing Liu, Chongzuo Li, Sixing Wu, Jianbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26420">SSCL-BW: Sample-Specific Clean-Label Backdoor Watermarking for Dataset Ownership Verification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of deep neural networks (DNNs) heavily relies on large-scale, high-quality datasets. However, unauthorized commercial use of these datasets severely violates the intellectual property rights of dataset owners. Existing backdoor-based dataset ownership verification methods suffer from inherent limitations: poison-label watermarks are easily detectable due to label inconsistencies, while clean-label watermarks face high technical complexity and failure on high-resolution images. Moreover, both approaches employ static watermark patterns that are vulnerable to detection and removal. To address these issues, this paper proposes a sample-specific clean-label backdoor watermarking (i.e., SSCL-BW). By training a U-Net-based watermarked sample generator, this method generates unique watermarks for each sample, fundamentally overcoming the vulnerability of static watermark patterns. The core innovation lies in designing a composite loss function with three components: target sample loss ensures watermark effectiveness, non-target sample loss guarantees trigger reliability, and perceptual similarity loss maintains visual imperceptibility. During ownership verification, black-box testing is employed to check whether suspicious models exhibit predefined backdoor behaviors. Extensive experiments on benchmark datasets demonstrate the effectiveness of the proposed method and its robustness against potential watermark removal attacks.
<div id='section'>Paperid: <span id='pid'>641, <a href='https://arxiv.org/pdf/2510.15303.pdf' target='_blank'>https://arxiv.org/pdf/2510.15303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ting Qiao, Xing Liu, Wenke Huang, Jianbin Li, Zhaoxin Fan, Yiming Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.15303">DSSmoothing: Toward Certified Dataset Ownership Verification for Pre-trained Language Models via Dual-Space Smoothing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large web-scale datasets have driven the rapid advancement of pre-trained language models (PLMs), but unauthorized data usage has raised serious copyright concerns. Existing dataset ownership verification (DOV) methods typically assume that watermarks remain stable during inference; however, this assumption often fails under natural noise and adversary-crafted perturbations. We propose the first certified dataset ownership verification method for PLMs based on dual-space smoothing (i.e., DSSmoothing). To address the challenges of text discreteness and semantic sensitivity, DSSmoothing introduces continuous perturbations in the embedding space to capture semantic robustness and applies controlled token reordering in the permutation space to capture sequential robustness. DSSmoothing consists of two stages: in the first stage, triggers are collaboratively embedded in both spaces to generate norm-constrained and robust watermarked datasets; in the second stage, randomized smoothing is applied in both spaces during verification to compute the watermark robustness (WR) of suspicious models and statistically compare it with the principal probability (PP) values of a set of benign models. Theoretically, DSSmoothing provides provable robustness guarantees for dataset ownership verification by ensuring that WR consistently exceeds PP under bounded dual-space perturbations. Extensive experiments on multiple representative web datasets demonstrate that DSSmoothing achieves stable and reliable verification performance and exhibits robustness against potential adaptive attacks.
<div id='section'>Paperid: <span id='pid'>642, <a href='https://arxiv.org/pdf/2510.01350.pdf' target='_blank'>https://arxiv.org/pdf/2510.01350.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Faheemur Rahman, Wayne Burleson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01350">Integrated Security Mechanisms for Weight Protection in Memristive Crossbar Arrays</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Memristive crossbar arrays enable in-memory computing by performing parallel analog computations directly within memory, making them well-suited for machine learning, neural networks, and neuromorphic systems. However, despite their advantages, non-volatile memristors are vulnerable to security threats (such as adversarial extraction of stored weights when the hardware is compromised. Protecting these weights is essential since they represent valuable intellectual property resulting from lengthy and costly training processes using large, often proprietary, datasets. As a solution we propose two security mechanisms: Keyed Permutor and Watermark Protection Columns; where both safeguard critical weights and establish verifiable ownership (even in cases of data leakage). Our approach integrates efficiently with existing memristive crossbar architectures without significant design modifications. Simulations across 45nm, 22nm, and 7nm CMOS nodes, using a realistic interconnect model and a large RF dataset, show that both mechanisms offer robust protection with under 10% overhead in area, delay and power. We also present initial experiments employing the widely known MNIST dataset; further highlighting the feasibility of securing memristive in-memory computing systems with minimal performance trade-offs.
<div id='section'>Paperid: <span id='pid'>643, <a href='https://arxiv.org/pdf/2509.20714.pdf' target='_blank'>https://arxiv.org/pdf/2509.20714.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh Tu Ngo, Anupam Chattopadhyay, Subhamoy Maitra
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.20714">Cryptographic Backdoor for Neural Networks: Boon and Bane</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we show that cryptographic backdoors in a neural network (NN) can be highly effective in two directions, namely mounting the attacks as well as in presenting the defenses as well. On the attack side, a carefully planted cryptographic backdoor enables powerful and invisible attack on the NN. Considering the defense, we present applications: first, a provably robust NN watermarking scheme; second, a protocol for guaranteeing user authentication; and third, a protocol for tracking unauthorized sharing of the NN intellectual property (IP). From a broader theoretical perspective, borrowing the ideas from Goldwasser et. al. [FOCS 2022], our main contribution is to show that all these instantiated practical protocol implementations are provably robust. The protocols for watermarking, authentication and IP tracking resist an adversary with black-box access to the NN, whereas the backdoor-enabled adversarial attack is impossible to prevent under the standard assumptions. While the theoretical tools used for our attack is mostly in line with the Goldwasser et. al. ideas, the proofs related to the defense need further studies. Finally, all these protocols are implemented on state-of-the-art NN architectures with empirical results corroborating the theoretical claims. Further, one can utilize post-quantum primitives for implementing the cryptographic backdoors, laying out foundations for quantum-era applications in machine learning (ML).
<div id='section'>Paperid: <span id='pid'>644, <a href='https://arxiv.org/pdf/2509.10577.pdf' target='_blank'>https://arxiv.org/pdf/2509.10577.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Danilo Francati, Yevin Nikhel Goonatilake, Shubham Pawar, Daniele Venturi, Giuseppe Ateniese
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.10577">The Coding Limits of Robust Watermarking for Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We prove a sharp threshold for the robustness of cryptographic watermarking for generative models. This is achieved by introducing a coding abstraction, which we call messageless secret-key codes, that formalizes sufficient and necessary requirements of robust watermarking: soundness, tamper detection, and pseudorandomness. Thus, we establish that robustness has a precise limit: For binary outputs no scheme can survive if more than half of the encoded bits are modified, and for an alphabet of size q the corresponding threshold is $(1-1/q)$ of the symbols. Complementing this impossibility, we give explicit constructions that meet the bound up to a constant slack. For every $Î´ > 0$, assuming pseudorandom functions and access to a public counter, we build linear-time codes that tolerate up to $(1/2)(1-Î´)$ errors in the binary case and $(1-1/q)(1-Î´)$ errors in the $q$-ary case. Together with the lower bound, these yield the maximum robustness achievable under standard cryptographic assumptions. We then test experimentally whether this limit appears in practice by looking at the recent watermarking for images of Gunn, Zhao, and Song (ICLR 2025). We show that a simple crop and resize operation reliably flipped about half of the latent signs and consistently prevented belief-propagation decoding from recovering the codeword, erasing the watermark while leaving the image visually intact. These results provide a complete characterization of robust watermarking, identifying the threshold at which robustness fails, constructions that achieve it, and an experimental confirmation that the threshold is already reached in practice.
<div id='section'>Paperid: <span id='pid'>645, <a href='https://arxiv.org/pdf/2508.13131.pdf' target='_blank'>https://arxiv.org/pdf/2508.13131.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dara Bahri, John Wieting
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.13131">Improving Detection of Watermarked Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has recently emerged as an effective strategy for detecting the generations of large language models (LLMs). The strength of a watermark typically depends strongly on the entropy afforded by the language model and the set of input prompts. However, entropy can be quite limited in practice, especially for models that are post-trained, for example via instruction tuning or reinforcement learning from human feedback (RLHF), which makes detection based on watermarking alone challenging. In this work, we investigate whether detection can be improved by combining watermark detectors with non-watermark ones. We explore a number of hybrid schemes that combine the two, observing performance gains over either class of detector under a wide range of experimental conditions.
<div id='section'>Paperid: <span id='pid'>646, <a href='https://arxiv.org/pdf/2508.06656.pdf' target='_blank'>https://arxiv.org/pdf/2508.06656.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Denis Lukovnikov, Andreas MÃ¼ller, Erwin Quiring, Asja Fischer
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06656">Towards Robust Red-Green Watermarking for Autoregressive Image Generators</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In-generation watermarking for detecting and attributing generated content has recently been explored for latent diffusion models (LDMs), demonstrating high robustness. However, the use of in-generation watermarks in autoregressive (AR) image models has not been explored yet. AR models generate images by autoregressively predicting a sequence of visual tokens that are then decoded into pixels using a vector-quantized decoder. Inspired by red-green watermarks for large language models, we examine token-level watermarking schemes that bias the next-token prediction based on prior tokens. We find that a direct transfer of these schemes works in principle, but the detectability of the watermarks decreases considerably under common image perturbations. As a remedy, we propose two novel watermarking methods that rely on visual token clustering to assign similar tokens to the same set. Firstly, we investigate a training-free approach that relies on a cluster lookup table, and secondly, we finetune VAE encoders to predict token clusters directly from perturbed images. Overall, our experiments show that cluster-level watermarks improve robustness against perturbations and regeneration attacks while preserving image quality. Cluster classification further boosts watermark detectability, outperforming a set of baselines. Moreover, our methods offer fast verification runtime, comparable to lightweight post-hoc watermarking methods.
<div id='section'>Paperid: <span id='pid'>647, <a href='https://arxiv.org/pdf/2506.20926.pdf' target='_blank'>https://arxiv.org/pdf/2506.20926.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haoxuan Li, Jiale Zhang, Xiaobing Sun, Xiapu Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20926">Towards Generalized and Stealthy Watermarking for Generative Code Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative code models (GCMs) significantly enhance development efficiency through automated code generation and code summarization. However, building and training these models require computational resources and time, necessitating effective digital copyright protection to prevent unauthorized leaks and misuse. Backdoor watermarking, by embedding hidden identifiers, simplifies copyright verification by breaking the model's black-box nature. Current backdoor watermarking techniques face two main challenges: first, limited generalization across different tasks and datasets, causing fluctuating verification rates; second, insufficient stealthiness, as watermarks are easily detected and removed by automated methods. To address these issues, we propose CodeGuard, a novel watermarking method combining attention mechanisms with distributed trigger embedding strategies. Specifically, CodeGuard employs attention mechanisms to identify watermark embedding positions, ensuring verifiability. Moreover, by using homomorphic character replacement, it avoids manual detection, while distributed trigger embedding reduces the likelihood of automated detection. Experimental results demonstrate that CodeGuard achieves up to 100% watermark verification rates in both code summarization and code generation tasks, with no impact on the primary task performance. In terms of stealthiness, CodeGuard performs exceptionally, with a maximum detection rate of only 0.078 against ONION detection methods, significantly lower than baseline methods.
<div id='section'>Paperid: <span id='pid'>648, <a href='https://arxiv.org/pdf/2504.19567.pdf' target='_blank'>https://arxiv.org/pdf/2504.19567.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenliang Gan, Chunya Liu, Yichao Tang, Binghao Wang, Weiqiang Wang, Xinpeng Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.19567">GenPTW: In-Generation Image Watermarking for Provenance Tracing and Tamper Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid development of generative image models has brought tremendous opportunities to AI-generated content (AIGC) creation, while also introducing critical challenges in ensuring content authenticity and copyright ownership. Existing image watermarking methods, though partially effective, often rely on post-processing or reference images, and struggle to balance fidelity, robustness, and tamper localization. To address these limitations, we propose GenPTW, an In-Generation image watermarking framework for latent diffusion models (LDMs), which integrates Provenance Tracing and Tamper Localization into a unified Watermark-based design. It embeds structured watermark signals during the image generation phase, enabling unified provenance tracing and tamper localization. For extraction, we construct a frequency-coordinated decoder to improve robustness and localization precision in complex editing scenarios. Additionally, a distortion layer that simulates AIGC editing is introduced to enhance robustness. Extensive experiments demonstrate that GenPTW outperforms existing methods in image fidelity, watermark extraction accuracy, and tamper localization performance, offering an efficient and practical solution for trustworthy AIGC image generation.
<div id='section'>Paperid: <span id='pid'>649, <a href='https://arxiv.org/pdf/2504.18906.pdf' target='_blank'>https://arxiv.org/pdf/2504.18906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yufeng Wu, Xin Liao, Baowei Wang, Han Fang, Xiaoshuai Wu, Guiling Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.18906">Sim-to-Real: An Unsupervised Noise Layer for Screen-Camera Watermarking Robustness</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unauthorized screen capturing and dissemination pose severe security threats such as data leakage and information theft. Several studies propose robust watermarking methods to track the copyright of Screen-Camera (SC) images, facilitating post-hoc certification against infringement. These techniques typically employ heuristic mathematical modeling or supervised neural network fitting as the noise layer, to enhance watermarking robustness against SC. However, both strategies cannot fundamentally achieve an effective approximation of SC noise. Mathematical simulation suffers from biased approximations due to the incomplete decomposition of the noise and the absence of interdependence among the noise components. Supervised networks require paired data to train the noise-fitting model, and it is difficult for the model to learn all the features of the noise. To address the above issues, we propose Simulation-to-Real (S2R). Specifically, an unsupervised noise layer employs unpaired data to learn the discrepancy between the modeled simulated noise distribution and the real-world SC noise distribution, rather than directly learning the mapping from sharp images to real-world images. Learning this transformation from simulation to reality is inherently simpler, as it primarily involves bridging the gap in noise distributions, instead of the complex task of reconstructing fine-grained image details. Extensive experimental results validate the efficacy of the proposed method, demonstrating superior watermark robustness and generalization compared to state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>650, <a href='https://arxiv.org/pdf/2504.11309.pdf' target='_blank'>https://arxiv.org/pdf/2504.11309.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongbo Li, Shangchao Yang, Ruiyang Xia, Lin Yuan, Xinbo Gao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.11309">Big Brother is Watching: Proactive Deepfake Detection via Learnable Hidden Face</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As deepfake technologies continue to advance, passive detection methods struggle to generalize with various forgery manipulations and datasets. Proactive defense techniques have been actively studied with the primary aim of preventing deepfake operation effectively working. In this paper, we aim to bridge the gap between passive detection and proactive defense, and seek to solve the detection problem utilizing a proactive methodology. Inspired by several watermarking-based forensic methods, we explore a novel detection framework based on the concept of ``hiding a learnable face within a face''. Specifically, relying on a semi-fragile invertible steganography network, a secret template image is embedded into a host image imperceptibly, acting as an indicator monitoring for any malicious image forgery when being restored by the inverse steganography process. Instead of being manually specified, the secret template is optimized during training to resemble a neutral facial appearance, just like a ``big brother'' hidden in the image to be protected. By incorporating a self-blending mechanism and robustness learning strategy with a simulative transmission channel, a robust detector is built to accurately distinguish if the steganographic image is maliciously tampered or benignly processed. Finally, extensive experiments conducted on multiple datasets demonstrate the superiority of the proposed approach over competing passive and proactive detection methods.
<div id='section'>Paperid: <span id='pid'>651, <a href='https://arxiv.org/pdf/2504.04687.pdf' target='_blank'>https://arxiv.org/pdf/2504.04687.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Leng, Chaowei Fang, Junye Chen, Yixiang Fang, Sheng Li, Guanbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.04687">Bridging Knowledge Gap Between Image Inpainting and Large-Area Visible Watermark Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visible watermark removal which involves watermark cleaning and background content restoration is pivotal to evaluate the resilience of watermarks. Existing deep neural network (DNN)-based models still struggle with large-area watermarks and are overly dependent on the quality of watermark mask prediction. To overcome these challenges, we introduce a novel feature adapting framework that leverages the representation modeling capacity of a pre-trained image inpainting model. Our approach bridges the knowledge gap between image inpainting and watermark removal by fusing information of the residual background content beneath watermarks into the inpainting backbone model. We establish a dual-branch system to capture and embed features from the residual background content, which are merged into intermediate features of the inpainting backbone model via gated feature fusion modules. Moreover, for relieving the dependence on high-quality watermark masks, we introduce a new training paradigm by utilizing coarse watermark masks to guide the inference process. This contributes to a visible image removal model which is insensitive to the quality of watermark mask during testing. Extensive experiments on both a large-scale synthesized dataset and a real-world dataset demonstrate that our approach significantly outperforms existing state-of-the-art methods. The source code is available in the supplementary materials.
<div id='section'>Paperid: <span id='pid'>652, <a href='https://arxiv.org/pdf/2504.02486.pdf' target='_blank'>https://arxiv.org/pdf/2504.02486.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mara Graziani, Antonio Foncubierta, Dimitrios Christofidellis, Irina Espejo-Morales, Malina Molnar, Marvin Alberts, Matteo Manica, Jannis Born
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02486">We Need Improved Data Curation and Attribution in AI for Scientific Discovery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As the interplay between human-generated and synthetic data evolves, new challenges arise in scientific discovery concerning the integrity of the data and the stability of the models. In this work, we examine the role of synthetic data as opposed to that of real experimental data for scientific research. Our analyses indicate that nearly three-quarters of experimental datasets available on open-access platforms have relatively low adoption rates, opening new opportunities to enhance their discoverability and usability by automated methods. Additionally, we observe an increasing difficulty in distinguishing synthetic from real experimental data. We propose supplementing ongoing efforts in automating synthetic data detection by increasing the focus on watermarking real experimental data, thereby strengthening data traceability and integrity. Our estimates suggest that watermarking even less than half of the real world data generated annually could help sustain model robustness, while promoting a balanced integration of synthetic and human-generated content.
<div id='section'>Paperid: <span id='pid'>653, <a href='https://arxiv.org/pdf/2503.18156.pdf' target='_blank'>https://arxiv.org/pdf/2503.18156.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bram Rijsbosch, Gijs van Dijck, Konrad Kollnig
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.18156">Adoption of Watermarking Measures for AI-Generated Content and Implications under the EU AI Act</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated images have become so good in recent years that individuals often cannot distinguish them any more from "real" images. This development, combined with the rapid spread of AI-generated content online, creates a series of societal risks, particularly with the emergence of "deep fakes" that impersonate real individuals. Watermarking, a technique that involves embedding information within images and other content to indicate their AI-generated nature, has emerged as a primary mechanism to address the risks posed by AI-generated content. Indeed, watermarking and AI labelling measures are now becoming a legal requirement in many jurisdictions, including under the 2024 European Union AI Act. Despite the widespread use of AI image generation systems, the current status of the implementation of such measures remains largely unexamined. Moreover, the practical implications of the AI Act's watermarking and labelling requirements have not previously been studied. The present paper therefore both provides an empirical analysis of 50 widely used AI systems for image generation, embedded into a legal analysis of the AI Act. In our legal analysis, we identify four categories of generative AI image deployment scenarios relevant under the AI Act and outline how the legal obligations apply in each category. In our empirical analysis, we find that only a minority number of AI image generators currently implement adequate watermarking (38%) and deep fake labelling (8%) practices. In response, we suggest a range of avenues of how the implementation of these legally mandated techniques can be improved, and publicly share our tooling for the easy detection of watermarks in images.
<div id='section'>Paperid: <span id='pid'>654, <a href='https://arxiv.org/pdf/2503.11404.pdf' target='_blank'>https://arxiv.org/pdf/2503.11404.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jonas Thietke, Andreas MÃ¼ller, Denis Lukovnikov, Asja Fischer, Erwin Quiring
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11404">Towards A Correct Usage of Cryptography in Semantic Watermarks for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Semantic watermarking methods enable the direct integration of watermarks into the generation process of latent diffusion models by only modifying the initial latent noise. One line of approaches building on Gaussian Shading relies on cryptographic primitives to steer the sampling process of the latent noise. However, we identify several issues in the usage of cryptographic techniques in Gaussian Shading, particularly in its proof of lossless performance and key management, causing ambiguity in follow-up works, too. In this work, we therefore revisit the cryptographic primitives for semantic watermarking. We introduce a novel, general proof of lossless performance based on IND\$-CPA security for semantic watermarks. We then discuss the configuration of the cryptographic primitives in semantic watermarks with respect to security, efficiency, and generation quality.
<div id='section'>Paperid: <span id='pid'>655, <a href='https://arxiv.org/pdf/2502.04901.pdf' target='_blank'>https://arxiv.org/pdf/2502.04901.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jaiden Fairoze, Guillermo Ortiz-Jimenez, Mel Vecerik, Somesh Jha, Sven Gowal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04901">On the Difficulty of Constructing a Robust and Publicly-Detectable Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This work investigates the theoretical boundaries of creating publicly-detectable schemes to enable the provenance of watermarked imagery. Metadata-based approaches like C2PA provide unforgeability and public-detectability. ML techniques offer robust retrieval and watermarking. However, no existing scheme combines robustness, unforgeability, and public-detectability. In this work, we formally define such a scheme and establish its existence. Although theoretically possible, we find that at present, it is intractable to build certain components of our scheme without a leap in deep learning capabilities. We analyze these limitations and propose research directions that need to be addressed before we can practically realize robust and publicly-verifiable provenance.
<div id='section'>Paperid: <span id='pid'>656, <a href='https://arxiv.org/pdf/2502.02676.pdf' target='_blank'>https://arxiv.org/pdf/2502.02676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Preston K. Robinette, Taylor T. Johnson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02676">Blind Visible Watermark Removal with Morphological Dilation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visible watermarks pose significant challenges for image restoration techniques, especially when the target background is unknown. Toward this end, we present MorphoMod, a novel method for automated visible watermark removal that operates in a blind setting -- without requiring target images. Unlike existing methods, MorphoMod effectively removes opaque and transparent watermarks while preserving semantic content, making it well-suited for real-world applications. Evaluations on benchmark datasets, including the Colored Large-scale Watermark Dataset (CLWD), LOGO-series, and the newly introduced Alpha1 datasets, demonstrate that MorphoMod achieves up to a 50.8% improvement in watermark removal effectiveness compared to state-of-the-art methods. Ablation studies highlight the impact of prompts used for inpainting, pre-removal filling strategies, and inpainting model performance on watermark removal. Additionally, a case study on steganographic disorientation reveals broader applications for watermark removal in disrupting high-level hidden messages. MorphoMod offers a robust, adaptable solution for watermark removal and opens avenues for further advancements in image restoration and adversarial manipulation.
<div id='section'>Paperid: <span id='pid'>657, <a href='https://arxiv.org/pdf/2501.03288.pdf' target='_blank'>https://arxiv.org/pdf/2501.03288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Xu, Victor S. Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.03288">CodeVision: Detecting LLM-Generated Code Using 2D Token Probability Maps and Vision Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of large language models (LLMs) like ChatGPT has significantly improved automated code generation, enhancing software development efficiency. However, this introduces challenges in academia, particularly in distinguishing between human-written and LLM-generated code, which complicates issues of academic integrity. Existing detection methods, such as pre-trained models and watermarking, face limitations in adaptability and computational efficiency. In this paper, we propose a novel detection method using 2D token probability maps combined with vision models, preserving spatial code structures such as indentation and brackets. By transforming code into log probability matrices and applying vision models like Vision Transformers (ViT) and ResNet, we capture both content and structure for more accurate detection. Our method shows robustness across multiple programming languages and improves upon traditional detectors, offering a scalable and computationally efficient solution for identifying LLM-generated code.
<div id='section'>Paperid: <span id='pid'>658, <a href='https://arxiv.org/pdf/2501.02704.pdf' target='_blank'>https://arxiv.org/pdf/2501.02704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anh Tu Ngo, Chuan Song Heng, Nandish Chattopadhyay, Anupam Chattopadhyay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.02704">Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep Neural Networks (DNNs) have gained considerable traction in recent years due to the unparalleled results they gathered. However, the cost behind training such sophisticated models is resource intensive, resulting in many to consider DNNs to be intellectual property (IP) to model owners. In this era of cloud computing, high-performance DNNs are often deployed all over the internet so that people can access them publicly. As such, DNN watermarking schemes, especially backdoor-based watermarks, have been actively developed in recent years to preserve proprietary rights. Nonetheless, there lies much uncertainty on the robustness of existing backdoor watermark schemes, towards both adversarial attacks and unintended means such as fine-tuning neural network models. One reason for this is that no complete guarantee of robustness can be assured in the context of backdoor-based watermark. In this paper, we extensively evaluate the persistence of recent backdoor-based watermarks within neural networks in the scenario of fine-tuning, we propose/develop a novel data-driven idea to restore watermark after fine-tuning without exposing the trigger set. Our empirical results show that by solely introducing training data after fine-tuning, the watermark can be restored if model parameters do not shift dramatically during fine-tuning. Depending on the types of trigger samples used, trigger accuracy can be reinstated to up to 100%. Our study further explores how the restoration process works using loss landscape visualization, as well as the idea of introducing training data in fine-tuning stage to alleviate watermark vanishing.
<div id='section'>Paperid: <span id='pid'>659, <a href='https://arxiv.org/pdf/2412.12194.pdf' target='_blank'>https://arxiv.org/pdf/2412.12194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yi Hao Puah, Anh Tu Ngo, Nandish Chattopadhyay, Anupam Chattopadhyay
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.12194">BlockDoor: Blocking Backdoor Based Watermarks in Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adoption of machine learning models across industries have turned Neural Networks (DNNs) into a prized Intellectual Property (IP), which needs to be protected from being stolen or being used without authorization. This topic gave rise to multiple watermarking schemes, through which, one can establish the ownership of a model. Watermarking using backdooring is the most well established method available in the literature, with specific works demonstrating the difficulty in removing the watermarks, embedded as backdoors within the weights of the network. However, in our work, we have identified a critical flaw in the design of the watermark verification with backdoors, pertaining to the behaviour of the samples of the Trigger Set, which acts as the secret key. In this paper, we present BlockDoor, which is a comprehensive package of techniques that is used as a wrapper to block all three different kinds of Trigger samples, which are used in the literature as means to embed watermarks within the trained neural networks as backdoors. The framework implemented through BlockDoor is able to detect potential Trigger samples, through separate functions for adversarial noise based triggers, out-of-distribution triggers and random label based triggers. Apart from a simple Denial-of-Service for a potential Trigger sample, our approach is also able to modify the Trigger samples for correct machine learning functionality. Extensive evaluation of BlockDoor establishes that it is able to significantly reduce the watermark validation accuracy of the Trigger set by up to $98\%$ without compromising on functionality, delivering up to a less than $1\%$ drop on the clean samples. BlockDoor has been tested on multiple datasets and neural architectures.
<div id='section'>Paperid: <span id='pid'>660, <a href='https://arxiv.org/pdf/2412.03283.pdf' target='_blank'>https://arxiv.org/pdf/2412.03283.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andreas MÃ¼ller, Denis Lukovnikov, Jonas Thietke, Asja Fischer, Erwin Quiring
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.03283">Black-Box Forgery Attacks on Semantic Watermarks for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating watermarking into the generation process of latent diffusion models (LDMs) simplifies detection and attribution of generated content. Semantic watermarks, such as Tree-Rings and Gaussian Shading, represent a novel class of watermarking techniques that are easy to implement and highly robust against various perturbations. However, our work demonstrates a fundamental security vulnerability of semantic watermarks. We show that attackers can leverage unrelated models, even with different latent spaces and architectures (UNet vs DiT), to perform powerful and realistic forgery attacks. Specifically, we design two watermark forgery attacks. The first imprints a targeted watermark into real images by manipulating the latent representation of an arbitrary image in an unrelated LDM to get closer to the latent representation of a watermarked image. We also show that this technique can be used for watermark removal. The second attack generates new images with the target watermark by inverting a watermarked image and re-generating it with an arbitrary prompt. Both attacks just need a single reference image with the target watermark. Overall, our findings question the applicability of semantic watermarks by revealing that attackers can easily forge or remove these watermarks under realistic conditions.
<div id='section'>Paperid: <span id='pid'>661, <a href='https://arxiv.org/pdf/2411.05947.pdf' target='_blank'>https://arxiv.org/pdf/2411.05947.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omar Alrabiah, Prabhanjan Ananth, Miranda Christ, Yevgeniy Dodis, Sam Gunn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05947">Ideal Pseudorandom Codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Pseudorandom codes are error-correcting codes with the property that no efficient adversary can distinguish encodings from uniformly random strings. They were recently introduced by Christ and Gunn [CRYPTO 2024] for the purpose of watermarking the outputs of randomized algorithms, such as generative AI models. Several constructions of pseudorandom codes have since been proposed, but none of them are robust to error channels that depend on previously seen codewords. This stronger kind of robustness is referred to as adaptive robustness, and it is important for meaningful applications to watermarking.
  In this work, we show the following.
  - Adaptive robustness: We show that the pseudorandom codes of Christ and Gunn are adaptively robust, resolving a conjecture posed by Cohen, Hoover, and Schoenbach [S&P 2025].
  - Ideal security: We define an ideal pseudorandom code as one which is indistinguishable from the ideal functionality, capturing both the pseudorandomness and robustness properties in one simple definition. We show that any adaptively robust pseudorandom code for single-bit messages can be bootstrapped to build an ideal pseudorandom code with linear information rate, under no additional assumptions.
  - CCA security: In the setting where the encoding key is made public, we define a CCA-secure pseudorandom code in analogy with CCA-secure encryption. We show that any adaptively robust public-key pseudorandom code for single-bit messages can be used to build a CCA-secure pseudorandom code with linear information rate, in the random oracle model.
  These results immediately imply stronger robustness guarantees for generative AI watermarking schemes, such as the practical quality-preserving image watermarks of Gunn, Zhao, and Song (2024).
<div id='section'>Paperid: <span id='pid'>662, <a href='https://arxiv.org/pdf/2410.18861.pdf' target='_blank'>https://arxiv.org/pdf/2410.18861.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miranda Christ, Sam Gunn, Tal Malkin, Mariana Raykova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.18861">Provably Robust Watermarks for Open-Source Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The recent explosion of high-quality language models has necessitated new methods for identifying AI-generated text. Watermarking is a leading solution and could prove to be an essential tool in the age of generative AI. Existing approaches embed watermarks at inference and crucially rely on the large language model (LLM) specification and parameters being secret, which makes them inapplicable to the open-source setting. In this work, we introduce the first watermarking scheme for open-source LLMs. Our scheme works by modifying the parameters of the model, but the watermark can be detected from just the outputs of the model. Perhaps surprisingly, we prove that our watermarks are unremovable under certain assumptions about the adversary's knowledge. To demonstrate the behavior of our construction under concrete parameter instantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We demonstrate robustness to both token substitution and perturbation of the model parameters. We find that the stronger of these attacks, the model-perturbation attack, requires deteriorating the quality score to 0 out of 100 in order to bring the detection rate down to 50%.
<div id='section'>Paperid: <span id='pid'>663, <a href='https://arxiv.org/pdf/2410.10876.pdf' target='_blank'>https://arxiv.org/pdf/2410.10876.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Xu, Kun Zhang, Victor S. Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10876">FreqMark: Frequency-Based Watermark for Sentence-Level Detection of LLM-Generated Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing use of Large Language Models (LLMs) for generating highly coherent and contextually relevant text introduces new risks, including misuse for unethical purposes such as disinformation or academic dishonesty. To address these challenges, we propose FreqMark, a novel watermarking technique that embeds detectable frequency-based watermarks in LLM-generated text during the token sampling process. The method leverages periodic signals to guide token selection, creating a watermark that can be detected with Short-Time Fourier Transform (STFT) analysis. This approach enables accurate identification of LLM-generated content, even in mixed-text scenarios with both human-authored and LLM-generated segments. Our experiments demonstrate the robustness and precision of FreqMark, showing strong detection capabilities against various attack scenarios such as paraphrasing and token substitution. Results show that FreqMark achieves an AUC improvement of up to 0.98, significantly outperforming existing detection methods.
<div id='section'>Paperid: <span id='pid'>664, <a href='https://arxiv.org/pdf/2410.09101.pdf' target='_blank'>https://arxiv.org/pdf/2410.09101.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wassim Bouaziz, El-Mahdi El-Mhamdi, Nicolas Usunier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09101">Data Taggants: Dataset Ownership Verification via Harmless Targeted Data Poisoning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dataset ownership verification, the process of determining if a dataset is used in a model's training data, is necessary for detecting unauthorized data usage and data contamination. Existing approaches, such as backdoor watermarking, rely on inducing a detectable behavior into the trained model on a part of the data distribution. However, these approaches have limitations, as they can be harmful to the model's performances or require unpractical access to the model's internals. Most importantly, previous approaches lack guarantee against false positives. This paper introduces data taggants, a novel non-backdoor dataset ownership verification technique. Our method uses pairs of out-of-distribution samples and random labels as secret keys, and leverages clean-label targeted data poisoning to subtly alter a dataset, so that models trained on it respond to the key samples with the corresponding key labels. The keys are built as to allow for statistical certificates with black-box access only to the model. We validate our approach through comprehensive and realistic experiments on ImageNet1k using ViT and ResNet models with state-of-the-art training recipes. Our findings demonstrate that data taggants can reliably make models trained on the protected dataset detectable with high confidence, without compromising validation accuracy, and demonstrates superiority over backdoor watermarking. Moreover, our method shows to be stealthy and robust against various defense mechanisms.
<div id='section'>Paperid: <span id='pid'>665, <a href='https://arxiv.org/pdf/2410.06545.pdf' target='_blank'>https://arxiv.org/pdf/2410.06545.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenyu Xu, Victor S. Sheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.06545">Signal Watermark on Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources. In this paper, we proposed a watermarking method embedding a specific watermark into the text during its generation by LLMs, based on a pre-defined signal pattern. This technique not only ensures the watermark's invisibility to humans but also maintains the quality and grammatical integrity of model-generated text. We utilize LLMs and Fast Fourier Transform (FFT) for token probability computation and detection of the signal watermark. The unique application of signal processing principles within the realm of text generation by LLMs allows for subtle yet effective embedding of watermarks, which do not compromise the quality or coherence of the generated text. Our method has been empirically validated across multiple LLMs, consistently maintaining high detection accuracy, even with variations in temperature settings during text generation. In the experiment of distinguishing between human-written and watermarked text, our method achieved an AUROC score of 0.97, significantly outperforming existing methods like GPTZero, which scored 0.64. The watermark's resilience to various attacking scenarios further confirms its robustness, addressing significant challenges in model-generated text authentication.
<div id='section'>Paperid: <span id='pid'>666, <a href='https://arxiv.org/pdf/2410.02099.pdf' target='_blank'>https://arxiv.org/pdf/2410.02099.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dara Bahri, John Wieting
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.02099">A Watermark for Black-Box Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has recently emerged as an effective strategy for detecting the outputs of large language models (LLMs). Most existing schemes require white-box access to the model's next-token probability distribution, which is typically not accessible to downstream users of an LLM API. In this work, we propose a principled watermarking scheme that requires only the ability to sample sequences from the LLM (i.e. black-box access), boasts a distortion-free property, and can be chained or nested using multiple secret keys. We provide performance guarantees, demonstrate how it can be leveraged when white-box access is available, and show when it can outperform existing white-box schemes via comprehensive experiments.
<div id='section'>Paperid: <span id='pid'>667, <a href='https://arxiv.org/pdf/2409.06130.pdf' target='_blank'>https://arxiv.org/pdf/2409.06130.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aoting Hu, Yanzhi Chen, Renjie Xie, Adrian Weller
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.06130">On the Weaknesses of Backdoor-based Model Watermarking: An Information-theoretic Perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Safeguarding the intellectual property of machine learning models has emerged as a pressing concern in AI security. Model watermarking is a powerful technique for protecting ownership of machine learning models, yet its reliability has been recently challenged by recent watermark removal attacks. In this work, we investigate why existing watermark embedding techniques particularly those based on backdooring are vulnerable. Through an information-theoretic analysis, we show that the resilience of watermarking against erasure attacks hinges on the choice of trigger-set samples, where current uses of out-distribution trigger-set are inherently vulnerable to white-box adversaries. Based on this discovery, we propose a novel model watermarking scheme, In-distribution Watermark Embedding (IWE), to overcome the limitations of existing method. To further minimise the gap to clean models, we analyze the role of logits as watermark information carriers and propose a new approach to better conceal watermark information within the logits. Experiments on real-world datasets including CIFAR-100 and Caltech-101 demonstrate that our method robustly defends against various adversaries with negligible accuracy loss (< 0.1%).
<div id='section'>Paperid: <span id='pid'>668, <a href='https://arxiv.org/pdf/2407.13803.pdf' target='_blank'>https://arxiv.org/pdf/2407.13803.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Duy C. Hoang, Hung T. Q. Le, Rui Chu, Ping Li, Weijie Zhao, Yingjie Lao, Khoa D. Doan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.13803">Less is More: Sparse Watermarking in LLMs with Enhanced Text Quality</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread adoption of Large Language Models (LLMs), concerns about potential misuse have emerged. To this end, watermarking has been adapted to LLM, enabling a simple and effective way to detect and monitor generated text. However, while the existing methods can differentiate between watermarked and unwatermarked text with high accuracy, they often face a trade-off between the quality of the generated text and the effectiveness of the watermarking process. In this work, we present a novel type of LLM watermark, Sparse Watermark, which aims to mitigate this trade-off by applying watermarks to a small subset of generated tokens distributed across the text. The key strategy involves anchoring watermarked tokens to words that have specific Part-of-Speech (POS) tags. Our experimental results demonstrate that the proposed watermarking scheme achieves high detectability while generating text that outperforms previous LLM watermarking methods in quality across various tasks
<div id='section'>Paperid: <span id='pid'>669, <a href='https://arxiv.org/pdf/2407.01534.pdf' target='_blank'>https://arxiv.org/pdf/2407.01534.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kongyang Chen, Yikai Li, Wenjun Lan, Bing Mi, Shaowei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.01534">AIGC-Assisted Digital Watermark Services in Low-Earth Orbit Satellite-Terrestrial Edge Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Low Earth Orbit (LEO) satellite communication is a crucial component of future 6G communication networks, contributing to the development of an integrated satellite-terrestrial network. In the forthcoming satellite-to-ground network, the idle computational resources of LEO satellites can serve as edge servers, delivering intelligent task computation services to ground users. Existing research on satellite-to-ground computation primarily focuses on designing efficient task scheduling algorithms to provide straightforward computation services to ground users. This study aims to integrate satellite edge networks with Artificial Intelligence-Generated Content (AIGC) technology to offer personalized AIGC services to ground users, such as customized digital watermarking services. Firstly, we propose a satellite-to-ground edge network architecture, enabling bidirectional communication between visible LEO satellites and ground users. Each LEO satellite is equipped with intelligent algorithms supporting various AIGC-assisted digital watermarking technologies with different precision levels. Secondly, considering metrics like satellite visibility, satellite-to-ground communication stability, digital watermark quality, satellite-to-ground communication time, digital watermarking time, and ground user energy consumption, we construct an AIGC-assisted digital watermarking model based on the satellite-to-ground edge network. Finally, we introduce a reinforcement learning-based task scheduling algorithm to obtain an optimal strategy. Experimental results demonstrate that our approach effectively meets the watermark generation needs of ground users, achieving a well-balanced trade-off between generation time and user energy consumption. We anticipate that this work will provide an effective solution for the intelligent services in satellite-to-ground edge networks.
<div id='section'>Paperid: <span id='pid'>670, <a href='https://arxiv.org/pdf/2406.14841.pdf' target='_blank'>https://arxiv.org/pdf/2406.14841.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yihao Zheng, Haocheng Xia, Junyuan Pang, Jinfei Liu, Kui Ren, Lingyang Chu, Yang Cao, Li Xiong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.14841">TabularMark: Watermarking Tabular Datasets for Machine Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is broadly utilized to protect ownership of shared data while preserving data utility. However, existing watermarking methods for tabular datasets fall short on the desired properties (detectability, non-intrusiveness, and robustness) and only preserve data utility from the perspective of data statistics, ignoring the performance of downstream ML models trained on the datasets. Can we watermark tabular datasets without significantly compromising their utility for training ML models while preventing attackers from training usable ML models on attacked datasets? In this paper, we propose a hypothesis testing-based watermarking scheme, TabularMark. Data noise partitioning is utilized for data perturbation during embedding, which is adaptable for numerical and categorical attributes while preserving the data utility. For detection, a custom-threshold one proportion z-test is employed, which can reliably determine the presence of the watermark. Experiments on real-world and synthetic datasets demonstrate the superiority of TabularMark in detectability, non-intrusiveness, and robustness.
<div id='section'>Paperid: <span id='pid'>671, <a href='https://arxiv.org/pdf/2405.01509.pdf' target='_blank'>https://arxiv.org/pdf/2405.01509.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minhao Bai, Kaiyi Pang, Yongfeng Huang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.01509">Learnable Linguistic Watermarks for Tracing Model Extraction Attacks on Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving domain of artificial intelligence, safeguarding the intellectual property of Large Language Models (LLMs) is increasingly crucial. Current watermarking techniques against model extraction attacks, which rely on signal insertion in model logits or post-processing of generated text, remain largely heuristic. We propose a novel method for embedding learnable linguistic watermarks in LLMs, aimed at tracing and preventing model extraction attacks. Our approach subtly modifies the LLM's output distribution by introducing controlled noise into token frequency distributions, embedding an statistically identifiable controllable watermark.We leverage statistical hypothesis testing and information theory, particularly focusing on Kullback-Leibler Divergence, to differentiate between original and modified distributions effectively. Our watermarking method strikes a delicate well balance between robustness and output quality, maintaining low false positive/negative rates and preserving the LLM's original performance.
<div id='section'>Paperid: <span id='pid'>672, <a href='https://arxiv.org/pdf/2404.17867.pdf' target='_blank'>https://arxiv.org/pdf/2404.17867.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoshuai Wu, Xin Liao, Bo Ou, Yuling Liu, Zheng Qin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.17867">Are Watermarks Bugs for Deepfake Detectors? Rethinking Proactive Forensics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>AI-generated content has accelerated the topic of media synthesis, particularly Deepfake, which can manipulate our portraits for positive or malicious purposes. Before releasing these threatening face images, one promising forensics solution is the injection of robust watermarks to track their own provenance. However, we argue that current watermarking models, originally devised for genuine images, may harm the deployed Deepfake detectors when directly applied to forged images, since the watermarks are prone to overlap with the forgery signals used for detection. To bridge this gap, we thus propose AdvMark, on behalf of proactive forensics, to exploit the adversarial vulnerability of passive detectors for good. Specifically, AdvMark serves as a plug-and-play procedure for fine-tuning any robust watermarking into adversarial watermarking, to enhance the forensic detectability of watermarked images; meanwhile, the watermarks can still be extracted for provenance tracking. Extensive experiments demonstrate the effectiveness of the proposed AdvMark, leveraging robust watermarking to fool Deepfake detectors, which can help improve the accuracy of downstream Deepfake detection without tuning the in-the-wild detectors. We believe this work will shed some light on the harmless proactive forensics against Deepfake.
<div id='section'>Paperid: <span id='pid'>673, <a href='https://arxiv.org/pdf/2404.13518.pdf' target='_blank'>https://arxiv.org/pdf/2404.13518.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hongyu Zhu, Sichu Liang, Wentao Hu, Fangqi Li, Ju Jia, Shilin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13518">Reliable Model Watermarking: Defending Against Theft without Compromising on Evasion</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of Machine Learning as a Service (MLaaS) platforms,safeguarding the intellectual property of deep learning models is becoming paramount. Among various protective measures, trigger set watermarking has emerged as a flexible and effective strategy for preventing unauthorized model distribution. However, this paper identifies an inherent flaw in the current paradigm of trigger set watermarking: evasion adversaries can readily exploit the shortcuts created by models memorizing watermark samples that deviate from the main task distribution, significantly impairing their generalization in adversarial settings. To counteract this, we leverage diffusion models to synthesize unrestricted adversarial examples as trigger sets. By learning the model to accurately recognize them, unique watermark behaviors are promoted through knowledge injection rather than error memorization, thus avoiding exploitable shortcuts. Furthermore, we uncover that the resistance of current trigger set watermarking against removal attacks primarily relies on significantly damaging the decision boundaries during embedding, intertwining unremovability with adverse impacts. By optimizing the knowledge transfer properties of protected models, our approach conveys watermark behaviors to extraction surrogates without aggressively decision boundary perturbation. Experimental results on CIFAR-10/100 and Imagenette datasets demonstrate the effectiveness of our method, showing not only improved robustness against evasion adversaries but also superior resistance to watermark removal attacks compared to state-of-the-art solutions.
<div id='section'>Paperid: <span id='pid'>674, <a href='https://arxiv.org/pdf/2403.13000.pdf' target='_blank'>https://arxiv.org/pdf/2403.13000.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyi Zhu, Jeroen Galjaard, Pin-Yu Chen, Lydia Y. Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.13000">Duwak: Dual Watermarks in Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms. Existing watermark techniques are shown effective in embedding single human-imperceptible and machine-detectable patterns without significantly affecting generated text quality and semantics. However, the efficiency in detecting watermarks, i.e., the minimum number of tokens required to assert detection with significance and robustness against post-editing, is still debatable. In this paper, we propose, Duwak, to fundamentally enhance the efficiency and quality of watermarking by embedding dual secret patterns in both token probability distribution and sampling schemes. To mitigate expression degradation caused by biasing toward certain tokens, we design a contrastive search to watermark the sampling scheme, which minimizes the token repetition and enhances the diversity. We theoretically explain the interdependency of the two watermarks within Duwak. We evaluate Duwak extensively on Llama2 under various post-editing attacks, against four state-of-the-art watermarking techniques and combinations of them. Our results show that Duwak marked text achieves the highest watermarked text quality at the lowest required token count for detection, up to 70% tokens less than existing approaches, especially under post paraphrasing.
<div id='section'>Paperid: <span id='pid'>675, <a href='https://arxiv.org/pdf/2403.12053.pdf' target='_blank'>https://arxiv.org/pdf/2403.12053.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Ma, Mengxi Guo, Li Yuming, Hengyuan Zhang, Cong Ma, Yuan Li, Xiaodong Xie, Shanghang Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.12053">PiGW: A Plug-in Generative Watermarking Framework</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Integrating watermarks into generative images is a critical strategy for protecting intellectual property and enhancing artificial intelligence security. This paper proposes Plug-in Generative Watermarking (PiGW) as a general framework for integrating watermarks into generative images. More specifically, PiGW embeds watermark information into the initial noise using a learnable watermark embedding network and an adaptive frequency spectrum mask. Furthermore, it optimizes training costs by gradually increasing timesteps. Extensive experiments demonstrate that PiGW enables embedding watermarks into the generated image with negligible quality loss while achieving true invisibility and high resistance to noise attacks. Moreover, PiGW can serve as a plugin for various commonly used generative structures and multimodal generative content types. Finally, we demonstrate how PiGW can also be utilized for detecting generated images, contributing to the promotion of secure AI development. The project code will be made available on GitHub.
<div id='section'>Paperid: <span id='pid'>676, <a href='https://arxiv.org/pdf/2402.12720.pdf' target='_blank'>https://arxiv.org/pdf/2402.12720.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fangqi Li, Haodong Zhao, Wei Du, Shilin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.12720">Revisiting the Information Capacity of Neural Network Watermarks: Upper Bound Estimation and Beyond</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To trace the copyright of deep neural networks, an owner can embed its identity information into its model as a watermark. The capacity of the watermark quantify the maximal volume of information that can be verified from the watermarked model. Current studies on capacity focus on the ownership verification accuracy under ordinary removal attacks and fail to capture the relationship between robustness and fidelity. This paper studies the capacity of deep neural network watermarks from an information theoretical perspective. We propose a new definition of deep neural network watermark capacity analogous to channel capacity, analyze its properties, and design an algorithm that yields a tight estimation of its upper bound under adversarial overwriting. We also propose a universal non-invasive method to secure the transmission of the identity message beyond capacity by multiple rounds of ownership verification. Our observations provide evidence for neural network owners and defenders that are curious about the tradeoff between the integrity of their ownership and the performance degradation of their products.
<div id='section'>Paperid: <span id='pid'>677, <a href='https://arxiv.org/pdf/2402.09370.pdf' target='_blank'>https://arxiv.org/pdf/2402.09370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miranda Christ, Sam Gunn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09370">Pseudorandom Error-Correcting Codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We construct pseudorandom error-correcting codes (or simply pseudorandom codes), which are error-correcting codes with the property that any polynomial number of codewords are pseudorandom to any computationally-bounded adversary. Efficient decoding of corrupted codewords is possible with the help of a decoding key.
  We build pseudorandom codes that are robust to substitution and deletion errors, where pseudorandomness rests on standard cryptographic assumptions. Specifically, pseudorandomness is based on either $2^{O(\sqrt{n})}$-hardness of LPN, or polynomial hardness of LPN and the planted XOR problem at low density.
  As our primary application of pseudorandom codes, we present an undetectable watermarking scheme for outputs of language models that is robust to cropping and a constant rate of random substitutions and deletions. The watermark is undetectable in the sense that any number of samples of watermarked text are computationally indistinguishable from text output by the original model. This is the first undetectable watermarking scheme that can tolerate a constant rate of errors.
  Our second application is to steganography, where a secret message is hidden in innocent-looking content. We present a constant-rate stateless steganography scheme with robustness to a constant rate of substitutions. Ours is the first stateless steganography scheme with provable steganographic security and any robustness to errors.
<div id='section'>Paperid: <span id='pid'>678, <a href='https://arxiv.org/pdf/2312.14383.pdf' target='_blank'>https://arxiv.org/pdf/2312.14383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yicheng Leng, Chaowei Fang, Gen Li, Yixiang Fang, Guanbin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14383">Removing Interference and Recovering Content Imaginatively for Visible Watermark Removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visible watermarks, while instrumental in protecting image copyrights, frequently distort the underlying content, complicating tasks like scene interpretation and image editing. Visible watermark removal aims to eliminate the interference of watermarks and restore the background content. However, existing methods often implement watermark component removal and background restoration tasks within a singular branch, leading to residual watermarks in the predictions and ignoring cases where watermarks heavily obscure the background. To address these limitations, this study introduces the Removing Interference and Recovering Content Imaginatively (RIRCI) framework. RIRCI embodies a two-stage approach: the initial phase centers on discerning and segregating the watermark component, while the subsequent phase focuses on background content restoration. To achieve meticulous background restoration, our proposed model employs a dual-path network capable of fully exploring the intrinsic background information beneath semi-transparent watermarks and peripheral contextual information from unaffected regions. Moreover, a Global and Local Context Interaction module is built upon multi-layer perceptrons and bidirectional feature transformation for comprehensive representation modeling in the background restoration phase. The efficacy of our approach is empirically validated across two large-scale datasets, and our findings reveal a marked enhancement over existing watermark removal techniques.
<div id='section'>Paperid: <span id='pid'>679, <a href='https://arxiv.org/pdf/2311.04378.pdf' target='_blank'>https://arxiv.org/pdf/2311.04378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hanlin Zhang, Benjamin L. Edelman, Danilo Francati, Daniele Venturi, Giuseppe Ateniese, Boaz Barak
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.04378">Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking generative models consists of planting a statistical signal (watermark) in a model's output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a computationally bounded attacker cannot erase the watermark without causing significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes. We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark attack; the attacker is not required to know the private key of the scheme or even which scheme is used. Our attack is based on two assumptions: (1) The attacker has access to a "quality oracle" that can evaluate whether a candidate output is a high-quality response to a prompt, and (2) The attacker has access to a "perturbation oracle" which can modify an output with a nontrivial probability of maintaining quality, and which induces an efficiently mixing random walk on high-quality outputs. We argue that both assumptions can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model itself, to which the attacker has only black-box access. Furthermore, our assumptions will likely only be easier to satisfy over time as models grow in capabilities and modalities. We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking schemes for large language models: Kirchenbauer et al. (2023), Kuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully removes the watermarks planted by all three schemes, with only minor quality degradation.
<div id='section'>Paperid: <span id='pid'>680, <a href='https://arxiv.org/pdf/2310.09479.pdf' target='_blank'>https://arxiv.org/pdf/2310.09479.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruinan Ma, Yu-an Tan, Shangbo Wu, Tian Chen, Yajie Wang, Yuanzhang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.09479">Unified High-binding Watermark for Unconditional Image Generation Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning techniques have implemented many unconditional image generation (UIG) models, such as GAN, Diffusion model, etc. The extremely realistic images (also known as AI-Generated Content, AIGC for short) produced by these models bring urgent needs for intellectual property protection such as data traceability and copyright certification. An attacker can steal the output images of the target model and use them as part of the training data to train a private surrogate UIG model. The implementation mechanisms of UIG models are diverse and complex, and there is no unified and effective protection and verification method at present. To address these issues, we propose a two-stage unified watermark verification mechanism with high-binding effects for such models. In the first stage, we use an encoder to invisibly write the watermark image into the output images of the original AIGC tool, and reversely extract the watermark image through the corresponding decoder. In the second stage, we design the decoder fine-tuning process, and the fine-tuned decoder can make correct judgments on whether the suspicious model steals the original AIGC tool data. Experiments demonstrate our method can complete the verification work with almost zero false positive rate under the condition of only using the model output images. Moreover, the proposed method can achieve data steal verification across different types of UIG models, which further increases the practicality of the method.
<div id='section'>Paperid: <span id='pid'>681, <a href='https://arxiv.org/pdf/2309.13322.pdf' target='_blank'>https://arxiv.org/pdf/2309.13322.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wissam Antoun, BenoÃ®t Sagot, DjamÃ© Seddah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.13322">From Text to Source: Results in Detecting Large Language Model-Generated Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates "Cross-Model Detection," by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families, and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.
<div id='section'>Paperid: <span id='pid'>682, <a href='https://arxiv.org/pdf/2309.11747.pdf' target='_blank'>https://arxiv.org/pdf/2309.11747.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lifeng Chen, Jia Liu, Yan Ke, Wenquan Sun, Weina Dong, Xiaozhong Pan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.11747">MarkNerf:Watermarking for Neural Radiance Field</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A watermarking algorithm is proposed in this paper to address the copyright protection issue of implicit 3D models. The algorithm involves embedding watermarks into the images in the training set through an embedding network, and subsequently utilizing the NeRF model for 3D modeling. A copyright verifier is employed to generate a backdoor image by providing a secret perspective as input to the neural radiation field. Subsequently, a watermark extractor is devised using the hyperparameterization method of the neural network to extract the embedded watermark image from that perspective. In a black box scenario, if there is a suspicion that the 3D model has been used without authorization, the verifier can extract watermarks from a secret perspective to verify network copyright. Experimental results demonstrate that the proposed algorithm effectively safeguards the copyright of 3D models. Furthermore, the extracted watermarks exhibit favorable visual effects and demonstrate robust resistance against various types of noise attacks.
<div id='section'>Paperid: <span id='pid'>683, <a href='https://arxiv.org/pdf/2308.10195.pdf' target='_blank'>https://arxiv.org/pdf/2308.10195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Dongjian Huo, Zehong Zhang, Hanjing Su, Guanbin Li, Chaowei Fang, Qingyao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.10195">WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking serves as a widely adopted approach to safeguard media copyright. In parallel, the research focus has extended to watermark removal techniques, offering an adversarial means to enhance watermark robustness and foster advancements in the watermarking field. Existing watermark removal methods mainly rely on UNet with task-specific decoder branches--one for watermark localization and the other for background image restoration. However, watermark localization and background restoration are not isolated tasks; precise watermark localization inherently implies regions necessitating restoration, and the background restoration process contributes to more accurate watermark localization. To holistically integrate information from both branches, we introduce an implicit joint learning paradigm. This empowers the network to autonomously navigate the flow of information between implicit branches through a gate mechanism. Furthermore, we employ cross-channel attention to facilitate local detail restoration and holistic structural comprehension, while harnessing nested structures to integrate multi-scale information. Extensive experiments are conducted on various challenging benchmarks to validate the effectiveness of our proposed method. The results demonstrate our approach's remarkable superiority, surpassing existing state-of-the-art methods by a large margin.
<div id='section'>Paperid: <span id='pid'>684, <a href='https://arxiv.org/pdf/2308.09889.pdf' target='_blank'>https://arxiv.org/pdf/2308.09889.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoyu Ye, Hao Huang, Jiaqi An, Yongtao Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.09889">DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Stable Diffusion (SD) customization approaches enable users to personalize SD model outputs, greatly enhancing the flexibility and diversity of AI art. However, they also allow individuals to plagiarize specific styles or subjects from copyrighted images, which raises significant concerns about potential copyright infringement. To address this issue, we propose an invisible data-free universal adversarial watermark (DUAW), aiming to protect a myriad of copyrighted images from different customization approaches across various versions of SD models. First, DUAW is designed to disrupt the variational autoencoder during SD customization. Second, DUAW operates in a data-free context, where it is trained on synthetic images produced by a Large Language Model (LLM) and a pretrained SD model. This approach circumvents the necessity of directly handling copyrighted images, thereby preserving their confidentiality. Once crafted, DUAW can be imperceptibly integrated into massive copyrighted images, serving as a protective measure by inducing significant distortions in the images generated by customized SD models. Experimental results demonstrate that DUAW can effectively distort the outputs of fine-tuned SD models, rendering them discernible to both human observers and a simple classifier.
<div id='section'>Paperid: <span id='pid'>685, <a href='https://arxiv.org/pdf/2308.03573.pdf' target='_blank'>https://arxiv.org/pdf/2308.03573.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohammed Lansari, Reda Bellafqira, Katarzyna Kapusta, Vincent Thouvenot, Olivier Bettan, Gouenou Coatrieux
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.03573">When Federated Learning meets Watermarking: A Comprehensive Overview of Techniques for Intellectual Property Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated Learning (FL) is a technique that allows multiple participants to collaboratively train a Deep Neural Network (DNN) without the need of centralizing their data. Among other advantages, it comes with privacy-preserving properties making it attractive for application in sensitive contexts, such as health care or the military. Although the data are not explicitly exchanged, the training procedure requires sharing information about participants' models. This makes the individual models vulnerable to theft or unauthorized distribution by malicious actors. To address the issue of ownership rights protection in the context of Machine Learning (ML), DNN Watermarking methods have been developed during the last five years. Most existing works have focused on watermarking in a centralized manner, but only a few methods have been designed for FL and its unique constraints. In this paper, we provide an overview of recent advancements in Federated Learning watermarking, shedding light on the new challenges and opportunities that arise in this field.
<div id='section'>Paperid: <span id='pid'>686, <a href='https://arxiv.org/pdf/2306.09194.pdf' target='_blank'>https://arxiv.org/pdf/2306.09194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Miranda Christ, Sam Gunn, Or Zamir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.09194">Undetectable Watermarks for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in the capabilities of large language models such as GPT-4 have spurred increasing concern about our ability to detect AI-generated text. Prior works have suggested methods of embedding watermarks in model outputs, by noticeably altering the output distribution. We ask: Is it possible to introduce a watermark without incurring any detectable change to the output distribution?
  To this end we introduce a cryptographically-inspired notion of undetectable watermarks for language models. That is, watermarks can be detected only with the knowledge of a secret key; without the secret key, it is computationally intractable to distinguish watermarked outputs from those of the original model. In particular, it is impossible for a user to observe any degradation in the quality of the text. Crucially, watermarks should remain undetectable even when the user is allowed to adaptively query the model with arbitrarily chosen prompts. We construct undetectable watermarks based on the existence of one-way functions, a standard assumption in cryptography.
<div id='section'>Paperid: <span id='pid'>687, <a href='https://arxiv.org/pdf/2305.06321.pdf' target='_blank'>https://arxiv.org/pdf/2305.06321.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xiaoshuai Wu, Xin Liao, Bo Ou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.06321">SepMark: Deep Separable Watermarking for Unified Source Tracing and Deepfake Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Malicious Deepfakes have led to a sharp conflict over distinguishing between genuine and forged faces. Although many countermeasures have been developed to detect Deepfakes ex-post, undoubtedly, passive forensics has not considered any preventive measures for the pristine face before foreseeable manipulations. To complete this forensics ecosystem, we thus put forward the proactive solution dubbed SepMark, which provides a unified framework for source tracing and Deepfake detection. SepMark originates from encoder-decoder-based deep watermarking but with two separable decoders. For the first time the deep separable watermarking, SepMark brings a new paradigm to the established study of deep watermarking, where a single encoder embeds one watermark elegantly, while two decoders can extract the watermark separately at different levels of robustness. The robust decoder termed Tracer that resists various distortions may have an overly high level of robustness, allowing the watermark to survive both before and after Deepfake. The semi-robust one termed Detector is selectively sensitive to malicious distortions, making the watermark disappear after Deepfake. Only SepMark comprising of Tracer and Detector can reliably trace the trusted source of the marked face and detect whether it has been altered since being marked; neither of the two alone can achieve this. Extensive experiments demonstrate the effectiveness of the proposed SepMark on typical Deepfakes, including face swapping, expression reenactment, and attribute editing.
<div id='section'>Paperid: <span id='pid'>688, <a href='https://arxiv.org/pdf/2210.15745.pdf' target='_blank'>https://arxiv.org/pdf/2210.15745.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Reda Bellafqira, Gouenou Coatrieux
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.15745">DICTION:DynamIC robusT whIte bOx watermarkiNg scheme for deep neural networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural network (DNN) watermarking is a suitable method for protecting the ownership of deep learning (DL) models. It secretly embeds an identifier (watermark) within the model, which can be retrieved by the owner to prove ownership. In this paper, we first provide a unified framework for white box DNN watermarking schemes. It includes current state-of-the-art methods outlining their theoretical inter-connections. Next, we introduce DICTION, a new white-box Dynamic Robust watermarking scheme, we derived from this framework. Its main originality stands on a generative adversarial network (GAN) strategy where the watermark extraction function is a DNN trained as a GAN discriminator taking the target model to watermark as a GAN generator with a latent space as the input of the GAN trigger set. DICTION can be seen as a generalization of DeepSigns which, to the best of our knowledge, is the only other Dynamic white-box watermarking scheme from the literature. Experiments conducted on the same model test set as Deepsigns demonstrate that our scheme achieves much better performance. Especially, with DICTION, one can increase the watermark capacity while preserving the target model accuracy at best and simultaneously ensuring strong watermark robustness against a wide range of watermark removal and detection attacks.
<div id='section'>Paperid: <span id='pid'>689, <a href='https://arxiv.org/pdf/2008.06255.pdf' target='_blank'>https://arxiv.org/pdf/2008.06255.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seung-Hun Nam, Jihyeon Kang, Daesik Kim, Namhyuk Ahn, Wonhyuk Ahn
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2008.06255">From Attack to Protection: Leveraging Watermarking Attack Network for Advanced Add-on Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-bit watermarking (MW) has been designed to enhance resistance against watermarking attacks, such as signal processing operations and geometric distortions. Various benchmark tools exist to assess this robustness through simulated attacks on watermarked images. However, these tools often fail to capitalize on the unique attributes of the targeted MW and typically neglect the aspect of visual quality, a critical factor in practical applications. To overcome these shortcomings, we introduce a watermarking attack network (WAN), a fully trainable watermarking benchmark tool designed to exploit vulnerabilities within MW systems and induce watermark bit inversions, significantly diminishing watermark extractability. The proposed WAN employs an architecture based on residual dense blocks, which is adept at both local and global feature learning, thereby maintaining high visual quality while obstructing the extraction of embedded information. Our empirical results demonstrate that the WAN effectively undermines various block-based MW systems while minimizing visual degradation caused by attacks. This is facilitated by our novel watermarking attack loss, which is specifically crafted to compromise these systems. The WAN functions not only as a benchmarking tool but also as an add-on watermarking (AoW) mechanism, augmenting established universal watermarking schemes by enhancing robustness or imperceptibility without requiring detailed method context and adapting to dynamic watermarking requirements. Extensive experimental results show that AoW complements the performance of the targeted MW system by independently enhancing both imperceptibility and robustness.
<div id='section'>Paperid: <span id='pid'>690, <a href='https://arxiv.org/pdf/2511.22936.pdf' target='_blank'>https://arxiv.org/pdf/2511.22936.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyoung Kim, Paul Hongsuck Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.22936">Robust Image Self-Recovery against Tampering using Watermark Generation with Pixel Shuffling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of Artificial Intelligence-Generated Content (AIGC) raises concerns about the authenticity of digital media. In this context, image self-recovery, reconstructing original content from its manipulated version, offers a practical solution for understanding the attacker's intent and restoring trustworthy data. However, existing methods often fail to accurately recover tampered regions, falling short of the primary goal of self-recovery. To address this challenge, we propose ReImage, a neural watermarking-based self-recovery framework that embeds a shuffled version of the target image into itself as a watermark. We design a generator that produces watermarks optimized for neural watermarking and introduce an image enhancement module to refine the recovered image. We further analyze and resolve key limitations of shuffled watermarking, enabling its effective use in self-recovery. We demonstrate that ReImage achieves state-of-the-art performance across diverse tampering scenarios, consistently producing high-quality recovered images. The code and pretrained models will be released upon publication.
<div id='section'>Paperid: <span id='pid'>691, <a href='https://arxiv.org/pdf/2511.14422.pdf' target='_blank'>https://arxiv.org/pdf/2511.14422.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhengchunmin Dai, Jiaxiong Tang, Peng Sun, Honglong Chen, Liantao Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.14422">Sigil: Server-Enforced Watermarking in U-Shaped Split Federated Learning via Gradient Injection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In decentralized machine learning paradigms such as Split Federated Learning (SFL) and its variant U-shaped SFL, the server's capabilities are severely restricted. Although this enhances client-side privacy, it also leaves the server highly vulnerable to model theft by malicious clients. Ensuring intellectual property protection for such capability-limited servers presents a dual challenge: watermarking schemes that depend on client cooperation are unreliable in adversarial settings, whereas traditional server-side watermarking schemes are technically infeasible because the server lacks access to critical elements such as model parameters or labels. To address this challenge, this paper proposes Sigil, a mandatory watermarking framework designed specifically for capability-limited servers. Sigil defines the watermark as a statistical constraint on the server-visible activation space and embeds the watermark into the client model via gradient injection, without requiring any knowledge of the data. Besides, we design an adaptive gradient clipping mechanism to ensure that our watermarking process remains both mandatory and stealthy, effectively countering existing gradient anomaly detection methods and a specifically designed adaptive subspace removal attack. Extensive experiments on multiple datasets and models demonstrate Sigil's fidelity, robustness, and stealthiness.
<div id='section'>Paperid: <span id='pid'>692, <a href='https://arxiv.org/pdf/2511.13598.pdf' target='_blank'>https://arxiv.org/pdf/2511.13598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxiong Tang, Zhengchunmin Dai, Liantao Wu, Peng Sun, Honglong Chen, Zhenfu Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13598">Robust Client-Server Watermarking for Split Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Split Federated Learning (SFL) is renowned for its privacy-preserving nature and low computational overhead among decentralized machine learning paradigms. In this framework, clients employ lightweight models to process private data locally and transmit intermediate outputs to a powerful server for further computation. However, SFL is a double-edged sword: while it enables edge computing and enhances privacy, it also introduces intellectual property ambiguity as both clients and the server jointly contribute to training. Existing watermarking techniques fail to protect both sides since no single participant possesses the complete model. To address this, we propose RISE, a Robust model Intellectual property protection scheme using client-Server watermark Embedding for SFL. Specifically, RISE adopts an asymmetric client-server watermarking design: the server embeds feature-based watermarks through a loss regularization term, while clients embed backdoor-based watermarks by injecting predefined trigger samples into private datasets. This co-embedding strategy enables both clients and the server to verify model ownership. Experimental results on standard datasets and multiple network architectures show that RISE achieves over $95\%$ watermark detection rate ($p-value \lt 0.03$) across most settings. It exhibits no mutual interference between client- and server-side watermarks and remains robust against common removal attacks.
<div id='section'>Paperid: <span id='pid'>693, <a href='https://arxiv.org/pdf/2511.09822.pdf' target='_blank'>https://arxiv.org/pdf/2511.09822.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jun Woo Chung, Yingjie Lao, Weijie Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.09822">Robust Watermarking on Gradient Boosting Decision Trees</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Gradient Boosting Decision Trees (GBDTs) are widely used in industry and academia for their high accuracy and efficiency, particularly on structured data. However, watermarking GBDT models remains underexplored compared to neural networks. In this work, we present the first robust watermarking framework tailored to GBDT models, utilizing in-place fine-tuning to embed imperceptible and resilient watermarks. We propose four embedding strategies, each designed to minimize impact on model accuracy while ensuring watermark robustness. Through experiments across diverse datasets, we demonstrate that our methods achieve high watermark embedding rates, low accuracy degradation, and strong resistance to post-deployment fine-tuning.
<div id='section'>Paperid: <span id='pid'>694, <a href='https://arxiv.org/pdf/2511.08637.pdf' target='_blank'>https://arxiv.org/pdf/2511.08637.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chung Peng Lee, Rachel Hong, Harry Jiang, Aster Plotnik, William Agnew, Jamie Morgenstern
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.08637">How do data owners say no? A case study of data consent mechanisms in web-scraped vision-language AI training datasets</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The internet has become the main source of data to train modern text-to-image or vision-language models, yet it is increasingly unclear whether web-scale data collection practices for training AI systems adequately respect data owners' wishes. Ignoring the owner's indication of consent around data usage not only raises ethical concerns but also has recently been elevated into lawsuits around copyright infringement cases. In this work, we aim to reveal information about data owners' consent to AI scraping and training, and study how it's expressed in DataComp, a popular dataset of 12.8 billion text-image pairs. We examine both the sample-level information, including the copyright notice, watermarking, and metadata, and the web-domain-level information, such as a site's Terms of Service (ToS) and Robots Exclusion Protocol. We estimate at least 122M of samples exhibit some indication of copyright notice in CommonPool, and find that 60\% of the samples in the top 50 domains come from websites with ToS that prohibit scraping. Furthermore, we estimate 9-13\% with 95\% confidence interval of samples from CommonPool to contain watermarks, where existing watermark detection methods fail to capture them in high fidelity. Our holistic methods and findings show that data owners rely on various channels to convey data consent, of which current AI data collection pipelines do not entirely respect. These findings highlight the limitations of the current dataset curation/release practice and the need for a unified data consent framework taking AI purposes into consideration.
<div id='section'>Paperid: <span id='pid'>695, <a href='https://arxiv.org/pdf/2510.09655.pdf' target='_blank'>https://arxiv.org/pdf/2510.09655.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yanming Li, Seifeddine Ghozzi, Cédric Eichler, Nicolas Anciaux, Alexandra Bensamoun, Lorena Gonzalez Manzano
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.09655">Data Provenance Auditing of Fine-Tuned Large Language Models with a Text-Preserving Technique</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of auditing whether sensitive or copyrighted texts were used to fine-tune large language models (LLMs) under black-box access. Prior signals-verbatim regurgitation and membership inference-are unreliable at the level of individual documents or require altering the visible text. We introduce a text-preserving watermarking framework that embeds sequences of invisible Unicode characters into documents. Each watermark is split into a cue (embedded in odd chunks) and a reply (embedded in even chunks). At audit time, we submit prompts that contain only the cue; the presence of the corresponding reply in the model's output provides evidence of memorization consistent with training on the marked text. To obtain sound decisions, we compare the score of the published watermark against a held-out set of counterfactual watermarks and apply a ranking test with a provable false-positive-rate bound. The design is (i) minimally invasive (no visible text changes), (ii) scalable to many users and documents via a large watermark space and multi-watermark attribution, and (iii) robust to common passive transformations. We evaluate on open-weight LLMs and multiple text domains, analyzing regurgitation dynamics, sensitivity to training set size, and interference under multiple concurrent watermarks. Our results demonstrate reliable post-hoc provenance signals with bounded FPR under black-box access. We experimentally observe a failure rate of less than 0.1\% when detecting a reply after fine-tuning with 50 marked documents. Conversely, no spurious reply was recovered in over 18,000 challenges, corresponding to a 100\%TPR@0\% FPR. Moreover, detection rates remain relatively stable as the dataset size increases, maintaining a per-document detection rate above 45\% even when the marked collection accounts for less than 0.33\% of the fine-tuning data.
<div id='section'>Paperid: <span id='pid'>696, <a href='https://arxiv.org/pdf/2508.19430.pdf' target='_blank'>https://arxiv.org/pdf/2508.19430.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kangfeng Ye, Roberto Metere, Jim Woodcock, Poonam Yadav
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.19430">Formal Verification of Physical Layer Security Protocols for Next-Generation Communication Networks (extended version)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Formal verification is crucial for ensuring the robustness of security protocols against adversarial attacks. The Needham-Schroeder protocol, a foundational authentication mechanism, has been extensively studied, including its integration with Physical Layer Security (PLS) techniques such as watermarking and jamming. Recent research has used ProVerif to verify these mechanisms in terms of secrecy. However, the ProVerif-based approach limits the ability to improve understanding of security beyond verification results. To overcome these limitations, we re-model the same protocol using an Isabelle formalism that generates sound animation, enabling interactive and automated formal verification of security protocols. Our modelling and verification framework is generic and highly configurable, supporting both cryptography and PLS. For the same protocol, we have conducted a comprehensive analysis (secrecy and authenticity in four different eavesdropper locations under both passive and active attacks) using our new web interface. Our findings not only successfully reproduce and reinforce previous results on secrecy but also reveal an uncommon but expected outcome: authenticity is preserved across all examined scenarios, even in cases where secrecy is compromised. We have proposed a PLS-based Diffie-Hellman protocol that integrates watermarking and jamming, and our analysis shows that it is secure for deriving a session key with required authentication. These highlight the advantages of our novel approach, demonstrating its robustness in formally verifying security properties beyond conventional methods.
<div id='section'>Paperid: <span id='pid'>697, <a href='https://arxiv.org/pdf/2507.21195.pdf' target='_blank'>https://arxiv.org/pdf/2507.21195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Po-Yuan Mao, Cheng-Chang Tsai, Chun-Shien Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21195">MaXsive: High-Capacity and Robust Training-Free Generative Image Watermarking in Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The great success of the diffusion model in image synthesis led to the release of gigantic commercial models, raising the issue of copyright protection and inappropriate content generation. Training-free diffusion watermarking provides a low-cost solution for these issues. However, the prior works remain vulnerable to rotation, scaling, and translation (RST) attacks. Although some methods employ meticulously designed patterns to mitigate this issue, they often reduce watermark capacity, which can result in identity (ID) collusion. To address these problems, we propose MaXsive, a training-free diffusion model generative watermarking technique that has high capacity and robustness. MaXsive best utilizes the initial noise to watermark the diffusion model. Moreover, instead of using a meticulously repetitive ring pattern, we propose injecting the X-shape template to recover the RST distortions. This design significantly increases robustness without losing any capacity, making ID collusion less likely to happen. The effectiveness of MaXsive has been verified on two well-known watermarking benchmarks under the scenarios of verification and identification.
<div id='section'>Paperid: <span id='pid'>698, <a href='https://arxiv.org/pdf/2507.12723.pdf' target='_blank'>https://arxiv.org/pdf/2507.12723.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minyoung Kim, Sehwan Park, Sungmin Cha, Paul Hongsuck Seo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.12723">Cross-Modal Watermarking for Authentic Audio Recovery and Tamper Localization in Synthesized Audiovisual Forgeries</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in voice cloning and lip synchronization models have enabled Synthesized Audiovisual Forgeries (SAVFs), where both audio and visuals are manipulated to mimic a target speaker. This significantly increases the risk of misinformation by making fake content seem real. To address this issue, existing methods detect or localize manipulations but cannot recover the authentic audio that conveys the semantic content of the message. This limitation reduces their effectiveness in combating audiovisual misinformation. In this work, we introduce the task of Authentic Audio Recovery (AAR) and Tamper Localization in Audio (TLA) from SAVFs and propose a cross-modal watermarking framework to embed authentic audio into visuals before manipulation. This enables AAR, TLA, and a robust defense against misinformation. Extensive experiments demonstrate the strong performance of our method in AAR and TLA against various manipulations, including voice cloning and lip synchronization.
<div id='section'>Paperid: <span id='pid'>699, <a href='https://arxiv.org/pdf/2507.00230.pdf' target='_blank'>https://arxiv.org/pdf/2507.00230.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peilin He, James Joshi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00230">PPFL-RDSN: Privacy-Preserving Federated Learning-based Residual Dense Spatial Networks for Encrypted Lossy Image Reconstruction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reconstructing high-quality images from low-resolution inputs using Residual Dense Spatial Networks (RDSNs) is crucial yet challenging, particularly in collaborative scenarios where centralized training poses significant privacy risks, including data leakage and inference attacks, as well as high computational costs. We propose a novel Privacy-Preserving Federated Learning-based RDSN (PPFL-RDSN) framework specifically tailored for lossy image reconstruction. PPFL-RDSN integrates Federated Learning (FL), local differential privacy, and robust model watermarking techniques, ensuring data remains secure on local devices, safeguarding sensitive information, and maintaining model authenticity without revealing underlying data. Empirical evaluations show that PPFL-RDSN achieves comparable performance to the state-of-the-art centralized methods while reducing computational burdens, and effectively mitigates security and privacy vulnerabilities, making it a practical solution for secure and privacy-preserving collaborative computer vision applications.
<div id='section'>Paperid: <span id='pid'>700, <a href='https://arxiv.org/pdf/2506.20370.pdf' target='_blank'>https://arxiv.org/pdf/2506.20370.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abdullah All Tanvir, Xin Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.20370">InvZW: Invariant Feature Learning via Noise-Adversarial Training for Robust Image Zero-Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel deep learning framework for robust image zero-watermarking based on distortion-invariant feature learning. As a zero-watermarking scheme, our method leaves the original image unaltered and learns a reference signature through optimization in the feature space. The proposed framework consists of two key modules. In the first module, a feature extractor is trained via noise-adversarial learning to generate representations that are both invariant to distortions and semantically expressive. This is achieved by combining adversarial supervision against a distortion discriminator and a reconstruction constraint to retain image content. In the second module, we design a learning-based multibit zero-watermarking scheme where the trained invariant features are projected onto a set of trainable reference codes optimized to match a target binary message. Extensive experiments on diverse image datasets and a wide range of distortions show that our method achieves state-of-the-art robustness in both feature stability and watermark recovery. Comparative evaluations against existing self-supervised and deep watermarking techniques further highlight the superiority of our framework in generalization and robustness.
<div id='section'>Paperid: <span id='pid'>701, <a href='https://arxiv.org/pdf/2505.06827.pdf' target='_blank'>https://arxiv.org/pdf/2505.06827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fabrice Y Harel-Canada, Boran Erol, Connor Choi, Jason Liu, Gary Jiarui Song, Nanyun Peng, Amit Sahai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.06827">Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking AI-generated text is critical for combating misuse. Yet recent theoretical work argues that any watermark can be erased via random walk attacks that perturb text while preserving quality. However, such attacks rely on two key assumptions: (1) rapid mixing (watermarks dissolve quickly under perturbations) and (2) reliable quality preservation (automated quality oracles perfectly guide edits). Through large-scale experiments and human-validated assessments, we find mixing is slow: 100% of perturbed texts retain traces of their origin after hundreds of edits, defying rapid mixing. Oracles falter, as state-of-the-art quality detectors misjudge edits (77% accuracy), compounding errors during attacks. Ultimately, attacks underperform: automated walks remove watermarks just 26% of the time -- dropping to 10% under human quality review. These findings challenge the inevitability of watermark removal. Instead, practical barriers -- slow mixing and imperfect quality control -- reveal watermarking to be far more robust than theoretical models suggest. The gap between idealized attacks and real-world feasibility underscores the need for stronger watermarking methods and more realistic attack models.
<div id='section'>Paperid: <span id='pid'>702, <a href='https://arxiv.org/pdf/2503.23748.pdf' target='_blank'>https://arxiv.org/pdf/2503.23748.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yujin Huang, Zhi Zhang, Qingchuan Zhao, Xingliang Yuan, Chunyang Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23748">THEMIS: Towards Practical Intellectual Property Protection for Post-Deployment On-Device Deep Learning Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>On-device deep learning (DL) has rapidly gained adoption in mobile apps, offering the benefits of offline model inference and user privacy preservation over cloud-based approaches. However, it inevitably stores models on user devices, introducing new vulnerabilities, particularly model-stealing attacks and intellectual property infringement. While system-level protections like Trusted Execution Environments (TEEs) provide a robust solution, practical challenges remain in achieving scalable on-device DL model protection, including complexities in supporting third-party models and limited adoption in current mobile solutions. Advancements in TEE-enabled hardware, such as NVIDIA's GPU-based TEEs, may address these obstacles in the future. Currently, watermarking serves as a common defense against model theft but also faces challenges here as many mobile app developers lack corresponding machine learning expertise and the inherent read-only and inference-only nature of on-device DL models prevents third parties like app stores from implementing existing watermarking techniques in post-deployment models.
  To protect the intellectual property of on-device DL models, in this paper, we propose THEMIS, an automatic tool that lifts the read-only restriction of on-device DL models by reconstructing their writable counterparts and leverages the untrainable nature of on-device DL models to solve watermark parameters and protect the model owner's intellectual property. Extensive experimental results across various datasets and model structures show the superiority of THEMIS in terms of different metrics. Further, an empirical investigation of 403 real-world DL mobile apps from Google Play is performed with a success rate of 81.14%, showing the practicality of THEMIS.
<div id='section'>Paperid: <span id='pid'>703, <a href='https://arxiv.org/pdf/2503.16693.pdf' target='_blank'>https://arxiv.org/pdf/2503.16693.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhan Cheng, Bolin Shen, Tianming Sha, Yuan Gao, Shibo Li, Yushun Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.16693">ATOM: A Framework of Detecting Query-Based Model Extraction Attacks for Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have gained traction in Graph-based Machine Learning as a Service (GMLaaS) platforms, yet they remain vulnerable to graph-based model extraction attacks (MEAs), where adversaries reconstruct surrogate models by querying the victim model. Existing defense mechanisms, such as watermarking and fingerprinting, suffer from poor real-time performance, susceptibility to evasion, or reliance on post-attack verification, making them inadequate for handling the dynamic characteristics of graph-based MEA variants. To address these limitations, we propose ATOM, a novel real-time MEA detection framework tailored for GNNs. ATOM integrates sequential modeling and reinforcement learning to dynamically detect evolving attack patterns, while leveraging $k$-core embedding to capture the structural properties, enhancing detection precision. Furthermore, we provide theoretical analysis to characterize query behaviors and optimize detection strategies. Extensive experiments on multiple real-world datasets demonstrate that ATOM outperforms existing approaches in detection performance, maintaining stable across different time steps, thereby offering a more effective defense mechanism for GMLaaS environments.
<div id='section'>Paperid: <span id='pid'>704, <a href='https://arxiv.org/pdf/2503.15772.pdf' target='_blank'>https://arxiv.org/pdf/2503.15772.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vishisht Rao, Aounon Kumar, Himabindu Lakkaraju, Nihar B. Shah
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.15772">Detecting LLM-Generated Peer Reviews</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The integrity of peer review is fundamental to scientific progress, but the rise of large language models (LLMs) has introduced concerns that some reviewers may rely on these tools to generate reviews rather than writing them independently. Although some venues have banned LLM-assisted reviewing, enforcement remains difficult as existing detection tools cannot reliably distinguish between fully generated reviews and those merely polished with AI assistance. In this work, we address the challenge of detecting LLM-generated reviews. We consider the approach of performing indirect prompt injection via the paper's PDF, prompting the LLM to embed a covert watermark in the generated review, and subsequently testing for presence of the watermark in the review. We identify and address several pitfalls in naÃ¯ve implementations of this approach. Our primary contribution is a rigorous watermarking and detection framework that offers strong statistical guarantees. Specifically, we introduce watermarking schemes and hypothesis tests that control the family-wise error rate across multiple reviews, achieving higher statistical power than standard corrections such as Bonferroni, while making no assumptions about the nature of human-written reviews. We explore multiple indirect prompt injection strategies--including font-based embedding and obfuscated prompts--and evaluate their effectiveness under various reviewer defense scenarios. Our experiments find high success rates in watermark embedding across various LLMs. We also empirically find that our approach is resilient to common reviewer defenses, and that the bounds on error rates in our statistical tests hold in practice. In contrast, we find that Bonferroni-style corrections are too conservative to be useful in this setting.
<div id='section'>Paperid: <span id='pid'>705, <a href='https://arxiv.org/pdf/2502.18501.pdf' target='_blank'>https://arxiv.org/pdf/2502.18501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sudev Kumar Padhi, Archana Tiwari, Sk. Subidh Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18501">Deep Learning-based Dual Watermarking for Image Copyright Protection and Authentication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advancements in digital technologies make it easy to modify the content of digital images. Hence, ensuring digital images integrity and authenticity is necessary to protect them against various attacks that manipulate them. We present a Deep Learning (DL) based dual invisible watermarking technique for performing source authentication, content authentication, and protecting digital content copyright of images sent over the internet. Beyond securing images, the proposed technique demonstrates robustness to content-preserving image manipulations. It is also impossible to imitate or overwrite watermarks because the cryptographic hash of the image and the dominant features of the image in the form of perceptual hash are used as watermarks. We highlighted the need for source authentication to safeguard image integrity and authenticity, along with identifying similar content for copyright protection. After exhaustive testing, we obtained a high peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM), which implies there is a minute change in the original image after embedding our watermarks. Our trained model achieves high watermark extraction accuracy and to the best of our knowledge, this is the first deep learning-based dual watermarking technique proposed in the literature.
<div id='section'>Paperid: <span id='pid'>706, <a href='https://arxiv.org/pdf/2502.10495.pdf' target='_blank'>https://arxiv.org/pdf/2502.10495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhonghao Yang, Linye Lyu, Xuanhang Chang, Daojing He, YU LI
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10495">SWA-LDM: Toward Stealthy Watermarks for Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the rapidly evolving landscape of image generation, Latent Diffusion Models (LDMs) have emerged as powerful tools, enabling the creation of highly realistic images. However, this advancement raises significant concerns regarding copyright infringement and the potential misuse of generated content. Current watermarking techniques employed in LDMs often embed constant signals to the generated images that compromise their stealthiness, making them vulnerable to detection by malicious attackers. In this paper, we introduce SWA-LDM, a novel approach that enhances watermarking by randomizing the embedding process, effectively eliminating detectable patterns while preserving image quality and robustness. Our proposed watermark presence attack reveals the inherent vulnerabilities of existing latent-based watermarking methods, demonstrating how easily these can be exposed. Through comprehensive experiments, we validate that SWA-LDM not only fortifies watermark stealthiness but also maintains competitive performance in watermark robustness and visual fidelity. This work represents a pivotal step towards securing LDM-generated images against unauthorized use, ensuring both copyright protection and content integrity in an era where digital image authenticity is paramount.
<div id='section'>Paperid: <span id='pid'>707, <a href='https://arxiv.org/pdf/2502.04182.pdf' target='_blank'>https://arxiv.org/pdf/2502.04182.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jade Garcia BourrÃ©e, Anne-Marie Kermarrec, Erwan Le Merrer, Othmane Safsafi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.04182">Fast In-Spectrum Graph Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We address the problem of watermarking graph objects, which consists in hiding information within them, to prove their origin. The two existing methods to watermark graphs use subgraph matching or graph isomorphism techniques, which are known to be intractable for large graphs. To reduce the operational complexity, we propose FFG, a new graph watermarking scheme adapted from an image watermarking scheme, since graphs and images can be represented as matrices. We analyze and compare FFG, whose novelty lies in embedding the watermark in the Fourier transform of the adjacency matrix of a graph. Our technique enjoys a much lower complexity than that of related works (i.e. in $\mathcal{O}\left(N^2 \log N\right)$), while performing better or at least as well as the two state-of-the-art methods.
<div id='section'>Paperid: <span id='pid'>708, <a href='https://arxiv.org/pdf/2502.02787.pdf' target='_blank'>https://arxiv.org/pdf/2502.02787.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Amirhossein Dabiriaghdam, Lele Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.02787">SimMark: A Robust Sentence-Level Similarity-Based Watermarking Algorithm for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The widespread adoption of large language models (LLMs) necessitates reliable methods to detect LLM-generated text. We introduce SimMark, a robust sentence-level watermarking algorithm that makes LLMs' outputs traceable without requiring access to model internals, making it compatible with both open and API-based LLMs. By leveraging the similarity of semantic sentence embeddings combined with rejection sampling to embed detectable statistical patterns imperceptible to humans, and employing a soft counting mechanism, SimMark achieves robustness against paraphrasing attacks. Experimental results demonstrate that SimMark sets a new benchmark for robust watermarking of LLM-generated content, surpassing prior sentence-level watermarking techniques in robustness, sampling efficiency, and applicability across diverse domains, all while maintaining the text quality and fluency.
<div id='section'>Paperid: <span id='pid'>709, <a href='https://arxiv.org/pdf/2501.15478.pdf' target='_blank'>https://arxiv.org/pdf/2501.15478.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhuo Lv, Yiran Xiahou, Congyi Li, Mengjie Sun, Shengzhi Zhang, Kai Chen, Yingjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.15478">LoRAGuard: An Effective Black-box Watermarking Approach for LoRAs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>LoRA (Low-Rank Adaptation) has achieved remarkable success in the parameter-efficient fine-tuning of large models. The trained LoRA matrix can be integrated with the base model through addition or negation operation to improve performance on downstream tasks. However, the unauthorized use of LoRAs to generate harmful content highlights the need for effective mechanisms to trace their usage. A natural solution is to embed watermarks into LoRAs to detect unauthorized misuse. However, existing methods struggle when multiple LoRAs are combined or negation operation is applied, as these can significantly degrade watermark performance. In this paper, we introduce LoRAGuard, a novel black-box watermarking technique for detecting unauthorized misuse of LoRAs. To support both addition and negation operations, we propose the Yin-Yang watermark technique, where the Yin watermark is verified during negation operation and the Yang watermark during addition operation. Additionally, we propose a shadow-model-based watermark training approach that significantly improves effectiveness in scenarios involving multiple integrated LoRAs. Extensive experiments on both language and diffusion models show that LoRAGuard achieves nearly 100% watermark verification success and demonstrates strong effectiveness.
<div id='section'>Paperid: <span id='pid'>710, <a href='https://arxiv.org/pdf/2501.13941.pdf' target='_blank'>https://arxiv.org/pdf/2501.13941.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Adam Block, Ayush Sekhari, Alexander Rakhlin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.13941">GaussMark: A Practical Approach for Structural Watermarking of Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advances in Large Language Models (LLMs) have led to significant improvements in natural language processing tasks, but their ability to generate human-quality text raises significant ethical and operational concerns in settings where it is important to recognize whether or not a given text was generated by a human. Thus, recent work has focused on developing techniques for watermarking LLM-generated text, i.e., introducing an almost imperceptible signal that allows a provider equipped with a secret key to determine if given text was generated by their model. Current watermarking techniques are often not practical due to concerns with generation latency, detection time, degradation in text quality, or robustness. Many of these drawbacks come from the focus on token-level watermarking, which ignores the inherent structure of text. In this work, we introduce a new scheme, GaussMark, that is simple and efficient to implement, has formal statistical guarantees on its efficacy, comes at no cost in generation latency, and embeds the watermark into the weights of the model itself, providing a structural watermark. Our approach is based on Gaussian independence testing and is motivated by recent empirical observations that minor additive corruptions to LLM weights can result in models of identical (or even improved) quality. We show that by adding a small amount of Gaussian noise to the weights of a given LLM, we can watermark the model in a way that is statistically detectable by a provider who retains the secret key. We provide formal statistical bounds on the validity and power of our procedure. Through an extensive suite of experiments, we demonstrate that GaussMark is reliable, efficient, and relatively robust to corruptions such as insertions, deletions, substitutions, and roundtrip translations and can be instantiated with essentially no loss in model quality.
<div id='section'>Paperid: <span id='pid'>711, <a href='https://arxiv.org/pdf/2501.05614.pdf' target='_blank'>https://arxiv.org/pdf/2501.05614.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jane Downer, Ren Wang, Binghui Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05614">Watermarking Graph Neural Networks via Explanations for Ownership Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) are the mainstream method to learn pervasive graph data and are widely deployed in industry, making their intellectual property valuable. However, protecting GNNs from unauthorized use remains a challenge. Watermarking, which embeds ownership information into a model, is a potential solution. However, existing watermarking methods have two key limitations: First, almost all of them focus on non-graph data, with watermarking GNNs for complex graph data largely unexplored. Second, the de facto backdoor-based watermarking methods pollute training data and induce ownership ambiguity through intentional misclassification. Our explanation-based watermarking inherits the strengths of backdoor-based methods (e.g., robust to watermark removal attacks), but avoids data pollution and eliminates intentional misclassification. In particular, our method learns to embed the watermark in GNN explanations such that this unique watermark is statistically distinct from other potential solutions, and ownership claims must show statistical significance to be verified. We theoretically prove that, even with full knowledge of our method, locating the watermark is an NP-hard problem. Empirically, our method manifests robustness to removal attacks like fine-tuning and pruning. By addressing these challenges, our approach marks a significant advancement in protecting GNN intellectual property.
<div id='section'>Paperid: <span id='pid'>712, <a href='https://arxiv.org/pdf/2501.05249.pdf' target='_blank'>https://arxiv.org/pdf/2501.05249.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhuo Lv, Mengjie Sun, Hao Wang, Xiaofeng Wang, Shengzhi Zhang, Yuxuan Chen, Kai Chen, Limin Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.05249">RAG-WM: An Efficient Black-Box Watermarking Approach for Retrieval-Augmented Generation of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, tremendous success has been witnessed in Retrieval-Augmented Generation (RAG), widely used to enhance Large Language Models (LLMs) in domain-specific, knowledge-intensive, and privacy-sensitive tasks. However, attackers may steal those valuable RAGs and deploy or commercialize them, making it essential to detect Intellectual Property (IP) infringement. Most existing ownership protection solutions, such as watermarks, are designed for relational databases and texts. They cannot be directly applied to RAGs because relational database watermarks require white-box access to detect IP infringement, which is unrealistic for the knowledge base in RAGs. Meanwhile, post-processing by the adversary's deployed LLMs typically destructs text watermark information. To address those problems, we propose a novel black-box "knowledge watermark" approach, named RAG-WM, to detect IP infringement of RAGs. RAG-WM uses a multi-LLM interaction framework, comprising a Watermark Generator, Shadow LLM & RAG, and Watermark Discriminator, to create watermark texts based on watermark entity-relationship tuples and inject them into the target RAG. We evaluate RAG-WM across three domain-specific and two privacy-sensitive tasks on four benchmark LLMs. Experimental results show that RAG-WM effectively detects the stolen RAGs in various deployed LLMs. Furthermore, RAG-WM is robust against paraphrasing, unrelated content removal, knowledge insertion, and knowledge expansion attacks. Lastly, RAG-WM can also evade watermark detection approaches, highlighting its promising application in detecting IP infringement of RAG systems.
<div id='section'>Paperid: <span id='pid'>713, <a href='https://arxiv.org/pdf/2501.01194.pdf' target='_blank'>https://arxiv.org/pdf/2501.01194.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaoyue Huang, Hanzhou Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.01194">A Game Between the Defender and the Attacker for Trigger-based Black-box Model Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking deep neural network (DNN) models has attracted a great deal of attention and interest in recent years because of the increasing demand to protect the intellectual property of DNN models. Many practical algorithms have been proposed by covertly embedding a secret watermark into a given DNN model through either parametric/structural modulation or backdooring against intellectual property infringement from the attacker while preserving the model performance on the original task. Despite the performance of these approaches, the lack of basic research restricts the algorithmic design to either a trial-based method or a data-driven technique. This has motivated the authors in this paper to introduce a game between the model attacker and the model defender for trigger-based black-box model watermarking. For each of the two players, we construct the payoff function and determine the optimal response, which enriches the theoretical foundation of model watermarking and may inspire us to develop novel schemes in the future.
<div id='section'>Paperid: <span id='pid'>714, <a href='https://arxiv.org/pdf/2412.20704.pdf' target='_blank'>https://arxiv.org/pdf/2412.20704.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sungik Choi, Sungwoo Park, Jaehoon Lee, Seunghyun Kim, Stanley Jungkyu Choi, Moontae Lee
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.20704">HFI: A unified framework for training-free detection and implicit watermarking of latent diffusion model generated images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Dramatic advances in the quality of the latent diffusion models (LDMs) also led to the malicious use of AI-generated images. While current AI-generated image detection methods assume the availability of real/AI-generated images for training, this is practically limited given the vast expressibility of LDMs. This motivates the training-free detection setup where no related data are available in advance. The existing LDM-generated image detection method assumes that images generated by LDM are easier to reconstruct using an autoencoder than real images. However, we observe that this reconstruction distance is overfitted to background information, leading the current method to underperform in detecting images with simple backgrounds. To address this, we propose a novel method called HFI. Specifically, by viewing the autoencoder of LDM as a downsampling-upsampling kernel, HFI measures the extent of aliasing, a distortion of high-frequency information that appears in the reconstructed image. HFI is training-free, efficient, and consistently outperforms other training-free methods in detecting challenging images generated by various generative models. We also show that HFI can successfully detect the images generated from the specified LDM as a means of implicit watermarking. HFI outperforms the best baseline method while achieving magnitudes of
<div id='section'>Paperid: <span id='pid'>715, <a href='https://arxiv.org/pdf/2410.08864.pdf' target='_blank'>https://arxiv.org/pdf/2410.08864.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Grzegorz GÅuch, Berkant Turan, Sai Ganesh Nagarajan, Sebastian Pokutta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.08864">The Good, the Bad and the Ugly: Watermarks, Transferable Attacks and Adversarial Defenses</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We formalize and extend existing definitions of backdoor-based watermarks and adversarial defenses as interactive protocols between two players. The existence of these schemes is inherently tied to the learning tasks for which they are designed. Our main result shows that for almost every discriminative learning task, at least one of the two -- a watermark or an adversarial defense -- exists. The term "almost every" indicates that we also identify a third, counterintuitive but necessary option, i.e., a scheme we call a transferable attack. By transferable attack, we refer to an efficient algorithm computing queries that look indistinguishable from the data distribution and fool all efficient defenders. To this end, we prove the necessity of a transferable attack via a construction that uses a cryptographic tool called homomorphic encryption. Furthermore, we show that any task that satisfies our notion of a transferable attack implies a cryptographic primitive, thus requiring the underlying task to be computationally complex. These two facts imply an "equivalence" between the existence of transferable attacks and cryptography. Finally, we show that the class of tasks of bounded VC-dimension has an adversarial defense, and a subclass of them has a watermark.
<div id='section'>Paperid: <span id='pid'>716, <a href='https://arxiv.org/pdf/2409.18442.pdf' target='_blank'>https://arxiv.org/pdf/2409.18442.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Seongmin Hong, Suh Yoon Jeon, Kyeonghyun Lee, Ernest K. Ryu, Se Young Chun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.18442">Gradient-free Decoder Inversion in Latent Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In latent diffusion models (LDMs), denoising diffusion process efficiently takes place on latent space whose dimension is lower than that of pixel space. Decoder is typically used to transform the representation in latent space to that in pixel space. While a decoder is assumed to have an encoder as an accurate inverse, exact encoder-decoder pair rarely exists in practice even though applications often require precise inversion of decoder. Prior works for decoder inversion in LDMs employed gradient descent inspired by inversions of generative adversarial networks. However, gradient-based methods require larger GPU memory and longer computation time for larger latent space. For example, recent video LDMs can generate more than 16 frames, but GPUs with 24 GB memory can only perform gradient-based decoder inversion for 4 frames. Here, we propose an efficient gradient-free decoder inversion for LDMs, which can be applied to diverse latent models. Theoretical convergence property of our proposed inversion has been investigated not only for the forward step method, but also for the inertial Krasnoselskii-Mann (KM) iterations under mild assumption on cocoercivity that is satisfied by recent LDMs. Our proposed gradient-free method with Adam optimizer and learning rate scheduling significantly reduced computation time and memory usage over prior gradient-based methods and enabled efficient computation in applications such as noise-space watermarking while achieving comparable error levels.
<div id='section'>Paperid: <span id='pid'>717, <a href='https://arxiv.org/pdf/2407.06552.pdf' target='_blank'>https://arxiv.org/pdf/2407.06552.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sudev Kumar Padhi, Sk. Subidh Ali
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.06552">DLOVE: A new Security Evaluation Tool for Deep Learning Based Watermarking Techniques</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent developments in Deep Neural Network (DNN) based watermarking techniques have shown remarkable performance. The state-of-the-art DNN-based techniques not only surpass the robustness of classical watermarking techniques but also show their robustness against many image manipulation techniques. In this paper, we performed a detailed security analysis of different DNN-based watermarking techniques. We propose a new class of attack called the Deep Learning-based OVErwriting (DLOVE) attack, which leverages adversarial machine learning and overwrites the original embedded watermark with a targeted watermark in a watermarked image. To the best of our knowledge, this attack is the first of its kind. We have considered scenarios where watermarks are used to devise and formulate an adversarial attack in white box and black box settings. To show adaptability and efficiency, we launch our DLOVE attack analysis on seven different watermarking techniques, HiDDeN, ReDMark, PIMoG, Stegastamp, Aparecium, Distortion Agostic Deep Watermarking and Hiding Images in an Image. All these techniques use different approaches to create imperceptible watermarked images. Our attack analysis on these watermarking techniques with various constraints highlights the vulnerabilities of DNN-based watermarking. Extensive experimental results validate the capabilities of DLOVE. We propose DLOVE as a benchmark security analysis tool to test the robustness of future deep learning-based watermarking techniques.
<div id='section'>Paperid: <span id='pid'>718, <a href='https://arxiv.org/pdf/2406.04805.pdf' target='_blank'>https://arxiv.org/pdf/2406.04805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Venkata Sai Pranav Bachina, Ankit Gangwal, Aaryan Ajay Sharma, Charu Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.04805">GENIE: Watermarking Graph Neural Networks for Link Prediction</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) have become invaluable intellectual property in graph-based machine learning. However, their vulnerability to model stealing attacks when deployed within Machine Learning as a Service (MLaaS) necessitates robust Ownership Demonstration (OD) techniques. Watermarking is a promising OD framework for Deep Neural Networks, but existing methods fail to generalize to GNNs due to the non-Euclidean nature of graph data. Previous works on GNN watermarking have primarily focused on node and graph classification, overlooking Link Prediction (LP).
  In this paper, we propose GENIE (watermarking Graph nEural Networks for lInk prEdiction), the first-ever scheme to watermark GNNs for LP. GENIE creates a novel backdoor for both node-representation and subgraph-based LP methods, utilizing a unique trigger set and a secret watermark vector. Our OD scheme is equipped with Dynamic Watermark Thresholding (DWT), ensuring high verification probability (>99.99%) while addressing practical issues in existing watermarking schemes. We extensively evaluate GENIE across 4 model architectures (i.e., SEAL, GCN, GraphSAGE and NeoGNN) and 7 real-world datasets. Furthermore, we validate the robustness of GENIE against 11 state-of-the-art watermark removal techniques and 3 model extraction attacks. We also show GENIE's resilience against ownership piracy attacks. Finally, we discuss a defense strategy to counter adaptive attacks against GENIE.
<div id='section'>Paperid: <span id='pid'>719, <a href='https://arxiv.org/pdf/2405.15661.pdf' target='_blank'>https://arxiv.org/pdf/2405.15661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Hinns, David Martens
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15661">Exposing Image Classifier Shortcuts with Counterfactual Frequency (CoF) Tables</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rise of deep learning in image classification has brought unprecedented accuracy but also highlighted a key issue: the use of 'shortcuts' by models. Such shortcuts are easy-to-learn patterns from the training data that fail to generalise to new data. Examples include the use of a copyright watermark to recognise horses, snowy background to recognise huskies, or ink markings to detect malignant skin lesions. The explainable AI (XAI) community has suggested using instance-level explanations to detect shortcuts without external data, but this requires the examination of many explanations to confirm the presence of such shortcuts, making it a labour-intensive process. To address these challenges, we introduce Counterfactual Frequency (CoF) tables, a novel approach that aggregates instance-based explanations into global insights, and exposes shortcuts. The aggregation implies the need for some semantic concepts to be used in the explanations, which we solve by labelling the segments of an image. We demonstrate the utility of CoF tables across several datasets, revealing the shortcuts learned from them.
<div id='section'>Paperid: <span id='pid'>720, <a href='https://arxiv.org/pdf/2403.05807.pdf' target='_blank'>https://arxiv.org/pdf/2403.05807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chunwei Tian, Menghua Zheng, Tiancai Jiao, Wangmeng Zuo, Yanning Zhang, Chia-Wen Lin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.05807">A self-supervised CNN for image watermark removal</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Popular convolutional neural networks mainly use paired images in a supervised way for image watermark removal. However, watermarked images do not have reference images in the real world, which results in poor robustness of image watermark removal techniques. In this paper, we propose a self-supervised convolutional neural network (CNN) in image watermark removal (SWCNN). SWCNN uses a self-supervised way to construct reference watermarked images rather than given paired training samples, according to watermark distribution. A heterogeneous U-Net architecture is used to extract more complementary structural information via simple components for image watermark removal. Taking into account texture information, a mixed loss is exploited to improve visual effects of image watermark removal. Besides, a watermark dataset is conducted. Experimental results show that the proposed SWCNN is superior to popular CNNs in image watermark removal.
<div id='section'>Paperid: <span id='pid'>721, <a href='https://arxiv.org/pdf/2402.09062.pdf' target='_blank'>https://arxiv.org/pdf/2402.09062.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hannes Mareen, Lucas Antchougov, Glenn Van Wallendael, Peter Lambert
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.09062">Blind Deep-Learning-Based Image Watermarking Robust Against Geometric Transformations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital watermarking enables protection against copyright infringement of images. Although existing methods embed watermarks imperceptibly and demonstrate robustness against attacks, they typically lack resilience against geometric transformations. Therefore, this paper proposes a new watermarking method that is robust against geometric attacks. The proposed method is based on the existing HiDDeN architecture that uses deep learning for watermark encoding and decoding. We add new noise layers to this architecture, namely for a differentiable JPEG estimation, rotation, rescaling, translation, shearing and mirroring. We demonstrate that our method outperforms the state of the art when it comes to geometric robustness. In conclusion, the proposed method can be used to protect images when viewed on consumers' devices.
<div id='section'>Paperid: <span id='pid'>722, <a href='https://arxiv.org/pdf/2401.15239.pdf' target='_blank'>https://arxiv.org/pdf/2401.15239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peizhuo Lv, Hualong Ma, Kai Chen, Jiachen Zhou, Shengzhi Zhang, Ruigang Liang, Shenchen Zhu, Pan Li, Yingjun Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.15239">MEA-Defender: A Robust Watermark against Model Extraction Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, numerous highly-valuable Deep Neural Networks (DNNs) have been trained using deep learning algorithms. To protect the Intellectual Property (IP) of the original owners over such DNN models, backdoor-based watermarks have been extensively studied. However, most of such watermarks fail upon model extraction attack, which utilizes input samples to query the target model and obtains the corresponding outputs, thus training a substitute model using such input-output pairs. In this paper, we propose a novel watermark to protect IP of DNN models against model extraction, named MEA-Defender. In particular, we obtain the watermark by combining two samples from two source classes in the input domain and design a watermark loss function that makes the output domain of the watermark within that of the main task samples. Since both the input domain and the output domain of our watermark are indispensable parts of those of the main task samples, the watermark will be extracted into the stolen model along with the main task during model extraction. We conduct extensive experiments on four model extraction attacks, using five datasets and six models trained based on supervised learning and self-supervised learning algorithms. The experimental results demonstrate that MEA-Defender is highly robust against different model extraction attacks, and various watermark removal/detection approaches.
<div id='section'>Paperid: <span id='pid'>723, <a href='https://arxiv.org/pdf/2310.16453.pdf' target='_blank'>https://arxiv.org/pdf/2310.16453.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Torsten KrauÃ, Jasper Stang, Alexandra Dmitrienko
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.16453">ClearMark: Intuitive and Robust Model Watermarking via Transposed Model Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Due to costly efforts during data acquisition and model training, Deep Neural Networks (DNNs) belong to the intellectual property of the model creator. Hence, unauthorized use, theft, or modification may lead to legal repercussions. Existing DNN watermarking methods for ownership proof are often non-intuitive, embed human-invisible marks, require trust in algorithmic assessment that lacks human-understandable attributes, and rely on rigid thresholds, making it susceptible to failure in cases of partial watermark erasure.
  This paper introduces ClearMark, the first DNN watermarking method designed for intuitive human assessment. ClearMark embeds visible watermarks, enabling human decision-making without rigid value thresholds while allowing technology-assisted evaluations. ClearMark defines a transposed model architecture allowing to use of the model in a backward fashion to interwove the watermark with the main task within all model parameters. Compared to existing watermarking methods, ClearMark produces visual watermarks that are easy for humans to understand without requiring complex verification algorithms or strict thresholds. The watermark is embedded within all model parameters and entangled with the main task, exhibiting superior robustness. It shows an 8,544-bit watermark capacity comparable to the strongest existing work. Crucially, ClearMark's effectiveness is model and dataset-agnostic, and resilient against adversarial model manipulations, as demonstrated in a comprehensive study performed with four datasets and seven architectures.
<div id='section'>Paperid: <span id='pid'>724, <a href='https://arxiv.org/pdf/2308.11235.pdf' target='_blank'>https://arxiv.org/pdf/2308.11235.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhenzhe Gao, Zhaoxia Yin, Hongjian Zhan, Heng Yin, Yue Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11235">Adaptive White-Box Watermarking with Self-Mutual Check Parameters in Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Artificial Intelligence (AI) has found wide application, but also poses risks due to unintentional or malicious tampering during deployment. Regular checks are therefore necessary to detect and prevent such risks. Fragile watermarking is a technique used to identify tampering in AI models. However, previous methods have faced challenges including risks of omission, additional information transmission, and inability to locate tampering precisely. In this paper, we propose a method for detecting tampered parameters and bits, which can be used to detect, locate, and restore parameters that have been tampered with. We also propose an adaptive embedding method that maximizes information capacity while maintaining model accuracy. Our approach was tested on multiple neural networks subjected to attacks that modified weight parameters, and our results demonstrate that our method achieved great recovery performance when the modification rate was below 20%. Furthermore, for models where watermarking significantly affected accuracy, we utilized an adaptive bit technique to recover more than 15% of the accuracy loss of the model.
<div id='section'>Paperid: <span id='pid'>725, <a href='https://arxiv.org/pdf/2304.10348.pdf' target='_blank'>https://arxiv.org/pdf/2304.10348.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bata Vasc, Nithin Raveendran, Bane Vasic
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.10348">Neuro-OSVETA: A Robust Watermarking of 3D Meshes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Best and practical watermarking schemes for copyright protection of 3D meshes are required to be blind and robust to attacks and errors. In this paper, we present the latest developments in 3D blind watermarking with a special emphasis on our Ordered Statistics Vertex Extraction and Tracing Algorithm (OSVETA) algorithm and its improvements. OSVETA is based on a combination of quantization index modulation (QIM) and error correction coding using novel ways for judicial selection of mesh vertices which are stable under mesh simplification, and the technique we propose in this paper offers a systematic method for vertex selection based on neural networks replacing a heuristic approach in the OSVETA. The Neuro-OSVETA enables a more precise mesh geometry estimation and better curvature and topological feature estimation. These enhancements result in a more accurate identification of stable vertices resulting in significant reduction of deletion probability.
<div id='section'>Paperid: <span id='pid'>726, <a href='https://arxiv.org/pdf/2303.09272.pdf' target='_blank'>https://arxiv.org/pdf/2303.09272.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haonan Zhong, Jiamin Chang, Ziyue Yang, Tingmin Wu, Pathum Chamikara Mahawaga Arachchige, Chehara Pathmabandu, Minhui Xue
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.09272">Copyright Protection and Accountability of Generative AI:Attack, Watermarking and Attribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative AI (e.g., Generative Adversarial Networks - GANs) has become increasingly popular in recent years. However, Generative AI introduces significant concerns regarding the protection of Intellectual Property Rights (IPR) (resp. model accountability) pertaining to images (resp. toxic images) and models (resp. poisoned models) generated. In this paper, we propose an evaluation framework to provide a comprehensive overview of the current state of the copyright protection measures for GANs, evaluate their performance across a diverse range of GAN architectures, and identify the factors that affect their performance and future research directions. Our findings indicate that the current IPR protection methods for input images, model watermarking, and attribution networks are largely satisfactory for a wide range of GANs. We highlight that further attention must be directed towards protecting training sets, as the current approaches fail to provide robust IPR protection and provenance tracing on training sets.
<div id='section'>Paperid: <span id='pid'>727, <a href='https://arxiv.org/pdf/2511.15807.pdf' target='_blank'>https://arxiv.org/pdf/2511.15807.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhagyesh Kumar, A S Aravinthakashan, Akshat Satyanarayan, Ishaan Gakhar, Ujjwal Verma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.15807">TopoReformer: Mitigating Adversarial Attacks Using Topological Purification in OCR Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Adversarially perturbed images of text can cause sophisticated OCR systems to produce misleading or incorrect transcriptions from seemingly invisible changes to humans. Some of these perturbations even survive physical capture, posing security risks to high-stakes applications such as document processing, license plate recognition, and automated compliance systems. Existing defenses, such as adversarial training, input preprocessing, or post-recognition correction, are often model-specific, computationally expensive, and affect performance on unperturbed inputs while remaining vulnerable to unseen or adaptive attacks. To address these challenges, TopoReformer is introduced, a model-agnostic reformation pipeline that mitigates adversarial perturbations while preserving the structural integrity of text images. Topology studies properties of shapes and spaces that remain unchanged under continuous deformations, focusing on global structures such as connectivity, holes, and loops rather than exact distance. Leveraging these topological features, TopoReformer employs a topological autoencoder to enforce manifold-level consistency in latent space and improve robustness without explicit gradient regularization. The proposed method is benchmarked on EMNIST, MNIST, against standard adversarial attacks (FGSM, PGD, Carlini-Wagner), adaptive attacks (EOT, BDPA), and an OCR-specific watermark attack (FAWA).
<div id='section'>Paperid: <span id='pid'>728, <a href='https://arxiv.org/pdf/2510.13151.pdf' target='_blank'>https://arxiv.org/pdf/2510.13151.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lifeng Qiu Lin, Henry Kam, Qi Sun, Kaan Akşit
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.13151">Foveation Improves Payload Capacity in Steganography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Steganography finds its use in visual medium such as providing metadata and watermarking. With support of efficient latent representations and foveated rendering, we trained models that improve existing capacity limits from 100 to 500 bits, while achieving better accuracy of up to 1 failure bit out of 2000, at 200K test bits. Finally, we achieve a comparable visual quality of 31.47 dB PSNR and 0.13 LPIPS, showing the effectiveness of novel perceptual design in creating multi-modal latent representations in steganography.
<div id='section'>Paperid: <span id='pid'>729, <a href='https://arxiv.org/pdf/2510.04966.pdf' target='_blank'>https://arxiv.org/pdf/2510.04966.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anna Chistyakova, Mikhail Pautov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04966">ActiveMark: on watermarking of visual foundation models via massive activations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Being trained on large and vast datasets, visual foundation models (VFMs) can be fine-tuned for diverse downstream tasks, achieving remarkable performance and efficiency in various computer vision applications. The high computation cost of data collection and training motivates the owners of some VFMs to distribute them alongside the license to protect their intellectual property rights. However, a dishonest user of the protected model's copy may illegally redistribute it, for example, to make a profit. As a consequence, the development of reliable ownership verification tools is of great importance today, since such methods can be used to differentiate between a redistributed copy of the protected model and an independent model. In this paper, we propose an approach to ownership verification of visual foundation models by fine-tuning a small set of expressive layers of a VFM along with a small encoder-decoder network to embed digital watermarks into an internal representation of a hold-out set of input images. Importantly, the watermarks embedded remain detectable in the functional copies of the protected model, obtained, for example, by fine-tuning the VFM for a particular downstream task. Theoretically and experimentally, we demonstrate that the proposed method yields a low probability of false detection of a non-watermarked model and a low probability of false misdetection of a watermarked model.
<div id='section'>Paperid: <span id='pid'>730, <a href='https://arxiv.org/pdf/2510.01967.pdf' target='_blank'>https://arxiv.org/pdf/2510.01967.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aadarsh Anantha Ramakrishnan, Shubham Agarwal, Selvanayagam S, Kunwar Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.01967">ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As image generation models grow increasingly powerful and accessible, concerns around authenticity, ownership, and misuse of synthetic media have become critical. The ability to generate lifelike images indistinguishable from real ones introduces risks such as misinformation, deepfakes, and intellectual property violations. Traditional watermarking methods either degrade image quality, are easily removed, or require access to confidential model internals - making them unsuitable for secure and scalable deployment. We are the first to introduce ZK-WAGON, a novel system for watermarking image generation models using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge (ZK-SNARKs). Our approach enables verifiable proof of origin without exposing model weights, generation prompts, or any sensitive internal information. We propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively convert key layers of an image generation model into a circuit, reducing proof generation time significantly. Generated ZK-SNARK proofs are imperceptibly embedded into a generated image via Least Significant Bit (LSB) steganography. We demonstrate this system on both GAN and Diffusion models, providing a secure, model-agnostic pipeline for trustworthy AI image generation.
<div id='section'>Paperid: <span id='pid'>731, <a href='https://arxiv.org/pdf/2509.24624.pdf' target='_blank'>https://arxiv.org/pdf/2509.24624.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Fargues, Ye Dong, Tianwei Zhang, Jin-Song Dong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.24624">PRIVMARK: Private Large Language Models Watermarking with MPC</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of Large Language Models (LLMs) has highlighted the pressing need for reliable mechanisms to verify content ownership and ensure traceability. Watermarking offers a promising path forward, but it remains limited by privacy concerns in sensitive scenarios, as traditional approaches often require direct access to a model's parameters or its training data. In this work, we propose a secure multi-party computation (MPC)-based private LLMs watermarking framework, PRIVMARK, to address the concerns. Concretely, we investigate PostMark (EMNLP'2024), one of the state-of-the-art LLMs Watermarking methods, and formulate its basic operations. Then, we construct efficient protocols for these operations using the MPC primitives in a black-box manner. In this way, PRIVMARK enables multiple parties to collaboratively watermark an LLM's output without exposing the model's weights to any single computing party. We implement PRIVMARK using SecretFlow-SPU (USENIX ATC'2023) and evaluate its performance using the ABY3 (CCS'2018) backend. The experimental results show that PRIVMARK achieves semantically identical results compared to the plaintext baseline without MPC and is resistant against paraphrasing and removing attacks with reasonable efficiency.
<div id='section'>Paperid: <span id='pid'>732, <a href='https://arxiv.org/pdf/2509.23383.pdf' target='_blank'>https://arxiv.org/pdf/2509.23383.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sebastian Bordt, Martin Pawelczyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.23383">Train Once, Answer All: Many Pretraining Experiments for the Cost of One</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent work has demonstrated that controlled pretraining experiments are a powerful tool for understanding learning, reasoning, and memorization in large language models (LLMs). However, the computational cost of pretraining presents a significant constraint. To overcome this constraint, we propose to conduct multiple pretraining experiments simultaneously during a single training run. We demonstrate the feasibility of this approach by conducting ten experiments during the training of a 1.5B parameter model on 210B tokens. Although we only train a single model, we can replicate the results from multiple previous works on data contamination, poisoning, and memorization. We also conduct novel investigations into knowledge acquisition, mathematical reasoning, and watermarking. For example, we dynamically update the training data until the model acquires a particular piece of knowledge. Remarkably, the influence of the ten experiments on the model's training dynamics and overall performance is minimal. However, interactions between different experiments may act as a potential confounder in our approach. We propose to test for interactions with continual pretraining experiments, finding them to be negligible in our setup. Overall, our findings suggest that performing multiple pretraining experiments in a single training run can enable rigorous scientific experimentation with large models on a compute budget.
<div id='section'>Paperid: <span id='pid'>733, <a href='https://arxiv.org/pdf/2509.17416.pdf' target='_blank'>https://arxiv.org/pdf/2509.17416.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jianbin Ji, Dawen Xu, Li Dong, Lin Yang, Songhan He
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.17416">DINVMark: A Deep Invertible Network for Video Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the wide spread of video, video watermarking has become increasingly crucial for copyright protection and content authentication. However, video watermarking still faces numerous challenges. For example, existing methods typically have shortcomings in terms of watermarking capacity and robustness, and there is a lack of specialized noise layer for High Efficiency Video Coding(HEVC) compression. To address these issues, this paper introduces a Deep Invertible Network for Video watermarking (DINVMark) and designs a noise layer to simulate HEVC compression. This approach not only in creases watermarking capacity but also enhances robustness. DINVMark employs an Invertible Neural Network (INN), where the encoder and decoder share the same network structure for both watermark embedding and extraction. This shared architecture ensures close coupling between the encoder and decoder, thereby improving the accuracy of the watermark extraction process. Experimental results demonstrate that the proposed scheme significantly enhances watermark robustness, preserves video quality, and substantially increases watermark embedding capacity.
<div id='section'>Paperid: <span id='pid'>734, <a href='https://arxiv.org/pdf/2509.16671.pdf' target='_blank'>https://arxiv.org/pdf/2509.16671.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ekin BÃ¶ke, Simon Torka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.16671">"Digital Camouflage": The LLVM Challenge in LLM-Based Malware Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have emerged as promising tools for malware detection by analyzing code semantics, identifying vulnerabilities, and adapting to evolving threats. However, their reliability under adversarial compiler-level obfuscation is yet to be discovered. In this study, we empirically evaluate the robustness of three state-of-the-art LLMs: ChatGPT-4o, Gemini Flash 2.5, and Claude Sonnet 4 against compiler-level obfuscation techniques implemented via the LLVM infrastructure. These include control flow flattening, bogus control flow injection, instruction substitution, and split basic blocks, which are widely used to evade detection while preserving malicious behavior. We perform a structured evaluation on 40~C functions (20 vulnerable, 20 secure) sourced from the Devign dataset and obfuscated using LLVM passes. Our results show that these models often fail to correctly classify obfuscated code, with precision, recall, and F1-score dropping significantly after transformation. This reveals a critical limitation: LLMs, despite their language understanding capabilities, can be easily misled by compiler-based obfuscation strategies. To promote reproducibility, we release all evaluation scripts, prompts, and obfuscated code samples in a public repository. We also discuss the implications of these findings for adversarial threat modeling, and outline future directions such as software watermarking, compiler-aware defenses, and obfuscation-resilient model design.
<div id='section'>Paperid: <span id='pid'>735, <a href='https://arxiv.org/pdf/2509.15170.pdf' target='_blank'>https://arxiv.org/pdf/2509.15170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aarushi Mahajan, Wayne Burleson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.15170">Watermarking and Anomaly Detection in Machine Learning Models for LORA RF Fingerprinting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Radio frequency fingerprint identification (RFFI) distinguishes wireless devices by the small variations in their analog circuits, avoiding heavy cryptographic authentication. While deep learning on spectrograms improves accuracy, models remain vulnerable to copying, tampering, and evasion. We present a stronger RFFI system combining watermarking for ownership proof and anomaly detection for spotting suspicious inputs. Using a ResNet-34 on log-Mel spectrograms, we embed three watermarks: a simple trigger, an adversarially trained trigger robust to noise and filtering, and a hidden gradient/weight signature. A convolutional Variational Autoencoders (VAE) with Kullback-Leibler (KL) warm-up and free-bits flags off-distribution queries. On the LoRa dataset, our system achieves 94.6% accuracy, 98% watermark success, and 0.94 AUROC, offering verifiable, tamper-resistant authentication.
<div id='section'>Paperid: <span id='pid'>736, <a href='https://arxiv.org/pdf/2508.06676.pdf' target='_blank'>https://arxiv.org/pdf/2508.06676.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chia-Hsun Lu, Guan-Jhih Wu, Ya-Chi Ho, Chih-Ya Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.06676">Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing importance of protecting intellectual property in machine learning, watermarking techniques have gained significant attention. As advanced models are increasingly deployed in domains such as social network analysis, the need for robust model protection becomes even more critical. While existing watermarking methods have demonstrated effectiveness for conventional deep neural networks, they often fail to adapt to the novel architecture, Kolmogorov-Arnold Networks (KAN), which feature learnable activation functions. KAN holds strong potential for modeling complex relationships in network-structured data. However, their unique design also introduces new challenges for watermarking. Therefore, we propose a novel watermarking method, Discrete Cosine Transform-based Activation Watermarking (DCT-AW), tailored for KAN. Leveraging the learnable activation functions of KAN, our method embeds watermarks by perturbing activation outputs using discrete cosine transform, ensuring compatibility with diverse tasks and achieving task independence. Experimental results demonstrate that DCT-AW has a small impact on model performance and provides superior robustness against various watermark removal attacks, including fine-tuning, pruning, and retraining after pruning.
<div id='section'>Paperid: <span id='pid'>737, <a href='https://arxiv.org/pdf/2506.12675.pdf' target='_blank'>https://arxiv.org/pdf/2506.12675.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Limengnan Zhou, Hanzhou Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12675">Watermarking Quantum Neural Networks Based on Sample Grouped and Paired Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum neural networks (QNNs) leverage quantum computing to create powerful and efficient artificial intelligence models capable of solving complex problems significantly faster than traditional computers. With the fast development of quantum hardware technology, such as superconducting qubits, trapped ions, and integrated photonics, quantum computers may become reality, accelerating the applications of QNNs. However, preparing quantum circuits and optimizing parameters for QNNs require quantum hardware support, expertise, and high-quality data. How to protect intellectual property (IP) of QNNs becomes an urgent problem to be solved in the era of quantum computing. We make the first attempt towards IP protection of QNNs by watermarking. To this purpose, we collect classical clean samples and trigger ones, each of which is generated by adding a perturbation to a clean sample, associated with a label different from the ground-truth one. The host QNN, consisting of quantum encoding, quantum state transformation, and quantum measurement, is then trained from scratch with the clean samples and trigger ones, resulting in a watermarked QNN model. During training, we introduce sample grouped and paired training to ensure that the performance on the downstream task can be maintained while achieving good performance for watermark extraction. When disputes arise, by collecting a mini-set of trigger samples, the hidden watermark can be extracted by analyzing the prediction results of the target model corresponding to the trigger samples, without accessing the internal details of the target QNN model, thereby verifying the ownership of the model. Experiments have verified the superiority and applicability of this work.
<div id='section'>Paperid: <span id='pid'>738, <a href='https://arxiv.org/pdf/2506.10030.pdf' target='_blank'>https://arxiv.org/pdf/2506.10030.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianyu Chen, Jian Lou, Wenjie Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10030">Safeguarding Multimodal Knowledge Copyright in the RAG-as-a-Service Environment</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As Retrieval-Augmented Generation (RAG) evolves into service-oriented platforms (Rag-as-a-Service) with shared knowledge bases, protecting the copyright of contributed data becomes essential. Existing watermarking methods in RAG focus solely on textual knowledge, leaving image knowledge unprotected. In this work, we propose AQUA, the first watermark framework for image knowledge protection in Multimodal RAG systems. AQUA embeds semantic signals into synthetic images using two complementary methods: acronym-based triggers and spatial relationship cues. These techniques ensure watermark signals survive indirect watermark propagation from image retriever to textual generator, being efficient, effective and imperceptible. Experiments across diverse models and datasets show that AQUA enables robust, stealthy, and reliable copyright tracing, filling a key gap in multimodal RAG protection.
<div id='section'>Paperid: <span id='pid'>739, <a href='https://arxiv.org/pdf/2504.06446.pdf' target='_blank'>https://arxiv.org/pdf/2504.06446.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Fay Elhassan, NiccolÃ² Ajroldi, Antonio Orvieto, Jonas Geiping
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.06446">Can you Finetune your Binoculars? Embedding Text Watermarks into the Weights of Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The indistinguishability of AI-generated content from human text raises challenges in transparency and accountability. While several methods exist to watermark models behind APIs, embedding watermark strategies directly into model weights that are later reflected in the outputs of the model is challenging. In this study we propose a strategy to finetune a pair of low-rank adapters of a model, one serving as the text-generating model, and the other as the detector, so that a subtle watermark is embedded into the text generated by the first model and simultaneously optimized for detectability by the second. In this way, the watermarking strategy is fully learned end-to-end. This process imposes an optimization challenge, as balancing watermark robustness, naturalness, and task performance requires trade-offs. We discuss strategies on how to optimize this min-max objective and present results showing the effect of this modification to instruction finetuning.
<div id='section'>Paperid: <span id='pid'>740, <a href='https://arxiv.org/pdf/2504.02640.pdf' target='_blank'>https://arxiv.org/pdf/2504.02640.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ZhongLi Fang, Yu Xie, Ping Chen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02640">RoSMM: A Robust and Secure Multi-Modal Watermarking Framework for Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current image watermarking technologies are predominantly categorized into text watermarking techniques and image steganography; however, few methods can simultaneously handle text and image-based watermark data, which limits their applicability in complex digital environments. This paper introduces an innovative multi-modal watermarking approach, drawing on the concept of vector discretization in encoder-based vector quantization. By constructing adjacency matrices, the proposed method enables the transformation of text watermarks into robust image-based representations, providing a novel multi-modal watermarking paradigm for image generation applications. Additionally, this study presents a newly designed image restoration module to mitigate image degradation caused by transmission losses and various noise interferences, thereby ensuring the reliability and integrity of the watermark. Experimental results validate the robustness of the method under multiple noise attacks, providing a secure, scalable, and efficient solution for digital image copyright protection.
<div id='section'>Paperid: <span id='pid'>741, <a href='https://arxiv.org/pdf/2502.18006.pdf' target='_blank'>https://arxiv.org/pdf/2502.18006.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zheng Xing, Chan-Tong Lam, Xiaochen Yuan, Sio-Kei Im, Penousal Machado
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.18006">Adaptive Quantum Scaling Model for Histogram Distribution-based Quantum Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of quantum image representation and quantum measurement techniques has made quantum image processing research a hot topic. In this paper, a novel Adaptive Quantum Scaling Model (AQSM) is first proposed for scrambling watermark images. Then, on the basis of the proposed AQSM, a novel quantum watermarking scheme is presented. Unlike existing quantum watermarking schemes with fixed embedding scales, the proposed method can flexibly embed watermarks of different sizes. In order to improve the robustness of the watermarking algorithm, a novel Histogram Distribution-based Watermarking Mechanism (HDWM) is proposed, which utilizes the histogram distribution property of the watermark image to determine the embedding strategy. In order to improve the accuracy of extracted watermark information, a quantum refining method is suggested, which can realize a certain error correction. The required key quantum circuits are designed. Finally, the effectiveness and robustness of the proposed quantum watermarking method are evaluated by simulation experiments on three image size scales. The results demonstrate the invisibility and good robustness of the watermarking algorithm.
<div id='section'>Paperid: <span id='pid'>742, <a href='https://arxiv.org/pdf/2412.15278.pdf' target='_blank'>https://arxiv.org/pdf/2412.15278.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Zhu, Xiapu Luo, Xuetao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.15278">DreaMark: Rooting Watermark in Score Distillation Sampling Generated Neural Radiance Fields</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recent advancements in text-to-3D generation can generate neural radiance fields (NeRFs) with score distillation sampling, enabling 3D asset creation without real-world data capture. With the rapid advancement in NeRF generation quality, protecting the copyright of the generated NeRF has become increasingly important. While prior works can watermark NeRFs in a post-generation way, they suffer from two vulnerabilities. First, a delay lies between NeRF generation and watermarking because the secret message is embedded into the NeRF model post-generation through fine-tuning. Second, generating a non-watermarked NeRF as an intermediate creates a potential vulnerability for theft. To address both issues, we propose Dreamark to embed a secret message by backdooring the NeRF during NeRF generation. In detail, we first pre-train a watermark decoder. Then, the Dreamark generates backdoored NeRFs in a way that the target secret message can be verified by the pre-trained watermark decoder on an arbitrary trigger viewport. We evaluate the generation quality and watermark robustness against image- and model-level attacks. Extensive experiments show that the watermarking process will not degrade the generation quality, and the watermark achieves 90+% accuracy among both image-level attacks (e.g., Gaussian noise) and model-level attacks (e.g., pruning attack).
<div id='section'>Paperid: <span id='pid'>743, <a href='https://arxiv.org/pdf/2412.09960.pdf' target='_blank'>https://arxiv.org/pdf/2412.09960.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Nan Sun, Han Fang, Yuxing Lu, Chengxin Zhao, Hefei Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.09960">END$^2$: Robust Dual-Decoder Watermarking Framework Against Non-Differentiable Distortions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNN-based watermarking methods have rapidly advanced, with the ``Encoder-Noise Layer-Decoder'' (END) framework being the most widely used. To ensure end-to-end training, the noise layer in the framework must be differentiable. However, real-world distortions are often non-differentiable, leading to challenges in end-to-end training. Existing solutions only treat the distortion perturbation as additive noise, which does not fully integrate the effect of distortion in training. To better incorporate non-differentiable distortions into training, we propose a novel dual-decoder architecture (END$^2$). Unlike conventional END architecture, our method employs two structurally identical decoders: the Teacher Decoder, processing pure watermarked images, and the Student Decoder, handling distortion-perturbed images. The gradient is backpropagated only through the Teacher Decoder branch to optimize the encoder thus bypassing the problem of non-differentiability. To ensure resistance to arbitrary distortions, we enforce alignment of the two decoders' feature representations by maximizing the cosine similarity between their intermediate vectors on a hypersphere. Extensive experiments demonstrate that our scheme outperforms state-of-the-art algorithms under various non-differentiable distortions. Moreover, even without the differentiability constraint, our method surpasses baselines with a differentiable noise layer. Our approach is effective and easily implementable across all END architectures, enhancing practicality and generalizability.
<div id='section'>Paperid: <span id='pid'>744, <a href='https://arxiv.org/pdf/2410.22445.pdf' target='_blank'>https://arxiv.org/pdf/2410.22445.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jijia Yang, Sen Peng, Xiaohua Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.22445">Embedding Watermarks in Diffusion Process for Model Intellectual Property Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In practical application, the widespread deployment of diffusion models often necessitates substantial investment in training. As diffusion models find increasingly diverse applications, concerns about potential misuse highlight the imperative for robust intellectual property protection. Current protection strategies either employ backdoor-based methods, integrating a watermark task as a simpler training objective with the main model task, or embedding watermarks directly into the final output samples. However, the former approach is fragile compared to existing backdoor defense techniques, while the latter fundamentally alters the expected output. In this work, we introduce a novel watermarking framework by embedding the watermark into the whole diffusion process, and theoretically ensure that our final output samples contain no additional information. Furthermore, we utilize statistical algorithms to verify the watermark from internally generated model samples without necessitating triggers as conditions. Detailed theoretical analysis and experimental validation demonstrate the effectiveness of our proposed method.
<div id='section'>Paperid: <span id='pid'>745, <a href='https://arxiv.org/pdf/2410.15075.pdf' target='_blank'>https://arxiv.org/pdf/2410.15075.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen-Hsiu Huang, Ja-Ling Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.15075">SLIC: Secure Learned Image Codec through Compressed Domain Watermarking to Defend Image Manipulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The digital image manipulation and advancements in Generative AI, such as Deepfake, has raised significant concerns regarding the authenticity of images shared on social media. Traditional image forensic techniques, while helpful, are often passive and insufficient against sophisticated tampering methods. This paper introduces the Secure Learned Image Codec (SLIC), a novel active approach to ensuring image authenticity through watermark embedding in the compressed domain. SLIC leverages neural network-based compression to embed watermarks as adversarial perturbations in the latent space, creating images that degrade in quality upon re-compression if tampered with. This degradation acts as a defense mechanism against unauthorized modifications. Our method involves fine-tuning a neural encoder/decoder to balance watermark invisibility with robustness, ensuring minimal quality loss for non-watermarked images. Experimental results demonstrate SLIC's effectiveness in generating visible artifacts in tampered images, thereby preventing their redistribution. This work represents a significant step toward developing secure image codecs that can be widely adopted to safeguard digital image integrity.
<div id='section'>Paperid: <span id='pid'>746, <a href='https://arxiv.org/pdf/2410.00059.pdf' target='_blank'>https://arxiv.org/pdf/2410.00059.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chaohui Xu, Qi Cui, Jinxin Dong, Weiyang He, Chip-Hong Chang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.00059">IDEA: An Inverse Domain Expert Adaptation Based Active DNN IP Protection Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Illegitimate reproduction, distribution and derivation of Deep Neural Network (DNN) models can inflict economic loss, reputation damage and even privacy infringement. Passive DNN intellectual property (IP) protection methods such as watermarking and fingerprinting attempt to prove the ownership upon IP violation, but they are often too late to stop catastrophic damage of IP abuse and too feeble against strong adversaries. In this paper, we propose IDEA, an Inverse Domain Expert Adaptation based proactive DNN IP protection method featuring active authorization and source traceability. IDEA generalizes active authorization as an inverse problem of domain adaptation. The multi-adaptive optimization is solved by a mixture-of-experts model with one real and two fake experts. The real expert re-optimizes the source model to correctly classify test images with a unique model user key steganographically embedded. The fake experts are trained to output random prediction on test images without or with incorrect user key embedded by minimizing their mutual information (MI) with the real expert. The MoE model is knowledge distilled into a unified protected model to avoid leaking the expert model features by maximizing their MI with additional multi-layer attention and contrastive representation loss optimization. IDEA not only prevents unauthorized users without the valid key to access the functional model, but also enable the model owner to validate the deployed model and trace the source of IP infringement. We extensively evaluate IDEA on five datasets and four DNN models to demonstrate its effectiveness in authorization control, culprit tracing success rate, and robustness against various attacks.
<div id='section'>Paperid: <span id='pid'>747, <a href='https://arxiv.org/pdf/2409.01484.pdf' target='_blank'>https://arxiv.org/pdf/2409.01484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rupshali Roy, Swaroop Ghosh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.01484">Watermarking of Quantum Circuits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Quantum circuits constitute Intellectual Property (IP) of the quantum developers and users, which needs to be protected from theft by adversarial agents, e.g., the quantum cloud provider or a rogue adversary present in the cloud. This necessitates the exploration of low-overhead techniques applicable to near-term quantum devices, to trace the quantum circuits/algorithms\textquotesingle{} IP and their output. We present two such lightweight watermarking techniques to prove ownership in the event of an adversary cloning the circuit design. For the first technique, a rotation gate is placed on ancilla qubits combined with other gate(s) at the output of the circuit. For the second method, a set of random gates are inserted in the middle of the circuit followed by its inverse, separated from the circuit by a barrier. These models are combined and applied on benchmark circuits, and the circuit depth, 2-qubit gate count, probability of successful trials (PST), and probabilistic proof of authorship (PPA) are compared against the state-of-the-art. The PST is reduced by a minuscule 0.53\% against the non-watermarked benchmarks and is up to 22.69\% higher compared to existing techniques. The circuit depth has been reduced by up to 27.7\% as against the state-of-the-art. The PPA is astronomically smaller than existing watermarks.
<div id='section'>Paperid: <span id='pid'>748, <a href='https://arxiv.org/pdf/2408.16634.pdf' target='_blank'>https://arxiv.org/pdf/2408.16634.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuan Shi, Jing Yan, Xiaoli Tang, Lingjuan Lyu, Boi Faltings
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.16634">RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.
<div id='section'>Paperid: <span id='pid'>749, <a href='https://arxiv.org/pdf/2406.03556.pdf' target='_blank'>https://arxiv.org/pdf/2406.03556.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Utsab Saha, Sawradip Saha, Shaikh Anowarul Fattah, Mohammad Saquib
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.03556">Npix2Cpix: A GAN-Based Image-to-Image Translation Network With Retrieval- Classification Integration for Watermark Retrieval From Historical Document Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The identification and restoration of ancient watermarks have long been a major topic in codicology and history. Classifying historical documents based on watermarks is challenging due to their diversity, noisy samples, multiple representation modes, and minor distinctions between classes and intra-class variations. This paper proposes a modified U-net-based conditional generative adversarial network (GAN) named Npix2Cpix to translate noisy raw historical watermarked images into clean, handwriting-free watermarked images by performing image translation from degraded (noisy) pixels to clean pixels. Using image-to-image translation and adversarial learning, the network creates clutter-free images for watermark restoration and categorization. The generator and discriminator of the proposed GAN are trained using two separate loss functions, each based on the distance between images, to learn the mapping from the input noisy image to the output clean image. After using the proposed GAN to pre-process noisy watermarked images, Siamese-based one-shot learning is employed for watermark classification. Experimental results on a large-scale historical watermark dataset demonstrate that cleaning the noisy watermarked images can help to achieve high one-shot classification accuracy. The qualitative and quantitative evaluation of the retrieved watermarked image highlights the effectiveness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>750, <a href='https://arxiv.org/pdf/2405.08340.pdf' target='_blank'>https://arxiv.org/pdf/2405.08340.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuchen Wang, Xingyu Zhu, Guanhui Ye, Shiyao Zhang, Xuetao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08340">Achieving Resolution-Agnostic DNN-based Image Watermarking: A Novel Perspective of Implicit Neural Representation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>DNN-based watermarking methods are rapidly developing and delivering impressive performances. Recent advances achieve resolution-agnostic image watermarking by reducing the variant resolution watermarking problem to a fixed resolution watermarking problem. However, such a reduction process can potentially introduce artifacts and low robustness. To address this issue, we propose the first, to the best of our knowledge, Resolution-Agnostic Image WaterMarking (RAIMark) framework by watermarking the implicit neural representation (INR) of image. Unlike previous methods, our method does not rely on the previous reduction process by directly watermarking the continuous signal instead of image pixels, thus achieving resolution-agnostic watermarking. Precisely, given an arbitrary-resolution image, we fit an INR for the target image. As a continuous signal, such an INR can be sampled to obtain images with variant resolutions. Then, we quickly fine-tune the fitted INR to get a watermarked INR conditioned on a binary secret message. A pre-trained watermark decoder extracts the hidden message from any sampled images with arbitrary resolutions. By directly watermarking INR, we achieve resolution-agnostic watermarking with increased robustness. Extensive experiments show that our method outperforms previous methods with significant improvements: averagely improved bit accuracy by 7%$\sim$29%. Notably, we observe that previous methods are vulnerable to at least one watermarking attack (e.g. JPEG, crop, resize), while ours are robust against all watermarking attacks.
<div id='section'>Paperid: <span id='pid'>751, <a href='https://arxiv.org/pdf/2405.05170.pdf' target='_blank'>https://arxiv.org/pdf/2405.05170.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sijing Xie, Chengxin Zhao, Nan Sun, Wei Li, Hefei Ling
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.05170">Picking watermarks from noise (PWFN): an improved robust watermarking model against intensive distortions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital watermarking is the process of embedding secret information by altering images in an undetectable way to the human eye. To increase the robustness of the model, many deep learning-based watermarking methods use the encoder-noise-decoder architecture by adding different noises to the noise layer. The decoder then extracts the watermarked information from the distorted image. However, this method can only resist weak noise attacks. To improve the robustness of the decoder against stronger noise, this paper proposes to introduce a denoise module between the noise layer and the decoder. The module aims to reduce noise and recover some of the information lost caused by distortion. Additionally, the paper introduces the SE module to fuse the watermarking information pixel-wise and channel dimensions-wise, improving the encoder's efficiency. Experimental results show that our proposed method is comparable to existing models and outperforms state-of-the-art under different noise intensities. In addition, ablation experiments show the superiority of our proposed module.
<div id='section'>Paperid: <span id='pid'>752, <a href='https://arxiv.org/pdf/2405.03458.pdf' target='_blank'>https://arxiv.org/pdf/2405.03458.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chengxin Zhao, Hefei Ling, Sijing Xie, Han Fang, Yaokun Fang, Nan Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.03458">SSyncOA: Self-synchronizing Object-aligned Watermarking to Resist Cropping-paste Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Modern image processing tools have made it easy for attackers to crop the region or object of interest in images and paste it into other images. The challenge this cropping-paste attack poses to the watermarking technology is that it breaks the synchronization of the image watermark, introducing multiple superimposed desynchronization distortions, such as rotation, scaling, and translation. However, current watermarking methods can only resist a single type of desynchronization and cannot be applied to protect the object's copyright under the cropping-paste attack. With the finding that the key to resisting the cropping-paste attack lies in robust features of the object to protect, this paper proposes a self-synchronizing object-aligned watermarking method, called SSyncOA. Specifically, we first constrain the watermarked region to be aligned with the protected object, and then synchronize the watermark's translation, rotation, and scaling distortions by normalizing the object invariant features, i.e., its centroid, principal orientation, and minimum bounding square, respectively. To make the watermark embedded in the protected object, we introduce the object-aligned watermarking model, which incorporates the real cropping-paste attack into the encoder-noise layer-decoder pipeline and is optimized end-to-end. Besides, we illustrate the effect of different desynchronization distortions on the watermark training, which confirms the necessity of the self-synchronization process. Extensive experiments demonstrate the superiority of our method over other SOTAs.
<div id='section'>Paperid: <span id='pid'>753, <a href='https://arxiv.org/pdf/2403.10893.pdf' target='_blank'>https://arxiv.org/pdf/2403.10893.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rui Min, Sen Li, Hongyang Chen, Minhao Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10893">A Watermark-Conditioned Diffusion Model for IP Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The ethical need to protect AI-generated content has been a significant concern in recent years. While existing watermarking strategies have demonstrated success in detecting synthetic content (detection), there has been limited exploration in identifying the users responsible for generating these outputs from a single model (owner identification). In this paper, we focus on both practical scenarios and propose a unified watermarking framework for content copyright protection within the context of diffusion models. Specifically, we consider two parties: the model provider, who grants public access to a diffusion model via an API, and the users, who can solely query the model API and generate images in a black-box manner. Our task is to embed hidden information into the generated contents, which facilitates further detection and owner identification. To tackle this challenge, we propose a Watermark-conditioned Diffusion model called WaDiff, which manipulates the watermark as a conditioned input and incorporates fingerprinting into the generation process. All the generative outputs from our WaDiff carry user-specific information, which can be recovered by an image extractor and further facilitate forensic identification. Extensive experiments are conducted on two popular diffusion models, and we demonstrate that our method is effective and robust in both the detection and owner identification tasks. Meanwhile, our watermarking framework only exerts a negligible impact on the original generation and is more stealthy and efficient in comparison to existing watermarking strategies.
<div id='section'>Paperid: <span id='pid'>754, <a href='https://arxiv.org/pdf/2403.10020.pdf' target='_blank'>https://arxiv.org/pdf/2403.10020.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Luo, Ke Lin, Chao Gu, Jiahui Hou, Lijie Wen, Ping Luo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.10020">Lost in Overlap: Exploring Logit-based Watermark Collision in LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of large language models (LLMs) in generating content raises concerns about text copyright. Watermarking methods, particularly logit-based approaches, embed imperceptible identifiers into text to address these challenges. However, the widespread usage of watermarking across diverse LLMs has led to an inevitable issue known as watermark collision during common tasks, such as paraphrasing or translation. In this paper, we introduce watermark collision as a novel and general philosophy for watermark attacks, aimed at enhancing attack performance on top of any other attacking methods. We also provide a comprehensive demonstration that watermark collision poses a threat to all logit-based watermark algorithms, impacting not only specific attack scenarios but also downstream applications.
<div id='section'>Paperid: <span id='pid'>755, <a href='https://arxiv.org/pdf/2312.02456.pdf' target='_blank'>https://arxiv.org/pdf/2312.02456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenquan Sun, Jia Liu, Weina Dong, Lifeng Chen, Ke Niu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02456">Watermarking for Neural Radiation Fields by Invertible Neural Network</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To protect the copyright of the 3D scene represented by the neural radiation field, the embedding and extraction of the neural radiation field watermark are considered as a pair of inverse problems of image transformations. A scheme for protecting the copyright of the neural radiation field is proposed using invertible neural network watermarking, which utilizes watermarking techniques for 2D images to achieve the protection of the 3D scene. The scheme embeds the watermark in the training image of the neural radiation field through the forward process in the invertible network and extracts the watermark from the image rendered by the neural radiation field using the inverse process to realize the copyright protection of both the neural radiation field and the 3D scene. Since the rendering process of the neural radiation field can cause the loss of watermark information, the scheme incorporates an image quality enhancement module, which utilizes a neural network to recover the rendered image and then extracts the watermark. The scheme embeds a watermark in each training image to train the neural radiation field and enables the extraction of watermark information from multiple viewpoints. Simulation experimental results demonstrate the effectiveness of the method.
<div id='section'>Paperid: <span id='pid'>756, <a href='https://arxiv.org/pdf/2311.17394.pdf' target='_blank'>https://arxiv.org/pdf/2311.17394.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mohamed R. Shoaib, Zefan Wang, Milad Taleby Ahvanooey, Jun Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.17394">Deepfakes, Misinformation, and Disinformation in the Era of Frontier AI, Generative AI, and Large AI Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the advent of sophisticated artificial intelligence (AI) technologies, the proliferation of deepfakes and the spread of m/disinformation have emerged as formidable threats to the integrity of information ecosystems worldwide. This paper provides an overview of the current literature. Within the frontier AI's crucial application in developing defense mechanisms for detecting deepfakes, we highlight the mechanisms through which generative AI based on large models (LM-based GenAI) craft seemingly convincing yet fabricated contents. We explore the multifaceted implications of LM-based GenAI on society, politics, and individual privacy violations, underscoring the urgent need for robust defense strategies. To address these challenges, in this study, we introduce an integrated framework that combines advanced detection algorithms, cross-platform collaboration, and policy-driven initiatives to mitigate the risks associated with AI-Generated Content (AIGC). By leveraging multi-modal analysis, digital watermarking, and machine learning-based authentication techniques, we propose a defense mechanism adaptable to AI capabilities of ever-evolving nature. Furthermore, the paper advocates for a global consensus on the ethical usage of GenAI and implementing cyber-wellness educational programs to enhance public awareness and resilience against m/disinformation. Our findings suggest that a proactive and collaborative approach involving technological innovation and regulatory oversight is essential for safeguarding netizens while interacting with cyberspace against the insidious effects of deepfakes and GenAI-enabled m/disinformation campaigns.
<div id='section'>Paperid: <span id='pid'>757, <a href='https://arxiv.org/pdf/2311.12059.pdf' target='_blank'>https://arxiv.org/pdf/2311.12059.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Zhu, Guanhui Ye, Chengdong Dong, Xiapu Luo, Shiyao Zhang, Xuetao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.12059">Mesh Watermark Removal Attack and Mitigation: A Novel Perspective of Function Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Mesh watermark embeds secret messages in 3D meshes and decodes the message from watermarked meshes for ownership verification. Current watermarking methods directly hide secret messages in vertex and face sets of meshes. However, mesh is a discrete representation that uses vertex and face sets to describe a continuous signal, which can be discretized in other discrete representations with different vertex and face sets. This raises the question of whether the watermark can still be verified on the different discrete representations of the watermarked mesh. We conduct this research in an attack-then-defense manner by proposing a novel function space mesh watermark removal attack FuncEvade and then mitigating it through function space mesh watermarking FuncMark. In detail, FuncEvade generates a different discrete representation of a watermarked mesh by extracting it from the signed distance function of the watermarked mesh. We observe that the generated mesh can evade ALL previous watermarking methods. FuncMark mitigates FuncEvade by watermarking signed distance function through message-guided deformation. Such deformation can survive isosurfacing and thus be inherited by the extracted meshes for further watermark decoding. Extensive experiments demonstrate that FuncEvade achieves 100% evasion rate among all previous watermarking methods while achieving only 0.3% evasion rate on FuncMark. Besides, our FuncMark performs similarly on other metrics compared to state-of-the-art mesh watermarking methods.
<div id='section'>Paperid: <span id='pid'>758, <a href='https://arxiv.org/pdf/2310.05054.pdf' target='_blank'>https://arxiv.org/pdf/2310.05054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yao Xiao, Sitian Chen, Amelie Chi Zhou, Shuhao Zhang, Yi Wang, Rui Mao, Xuan Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05054">Low-Latency Video Conferencing via Optimized Packet Routing and Reordering</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the face of rising global demand for video meetings, managing traffic across geographically distributed (geo-distributed) data centers presents a significant challenge due to the dynamic and limited nature of inter-DC network performance. Facing these issues, this paper introduces two novel techniques, VCRoute and WMJitter, to optimize the performance of geo-distributed video conferencing systems. VCRoute is a routing method designed for audio data packets of video conferences. It treats the routing problem as a Multi-Armed Bandit issue, and utilizes a tailored Thompson Sampling algorithm for resolution. Unlike traditional approaches, VCRoute considers transmitting latency and its variance simultaneously by using Thompson Sampling algorithm, which leads to effective end-to-end latency optimization. In conjunction with VCRoute, we present WMJitter, a watermark-based mechanism for managing network jitter, which can further reduce the end-to-end delay and keep an improved balance between latency and loss rate. Evaluations based on real geo-distributed network performance demonstrate the effectiveness and scalability of VCRoute and WMJitter, offering robust solutions for optimizing video conferencing systems in geo-distributed settings.
<div id='section'>Paperid: <span id='pid'>759, <a href='https://arxiv.org/pdf/2310.00568.pdf' target='_blank'>https://arxiv.org/pdf/2310.00568.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen-Hsiu Huang, Ja-Ling Wu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.00568">Image Data Hiding in Neural Compressed Latent Representations</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We propose an end-to-end learned image data hiding framework that embeds and extracts secrets in the latent representations of a generic neural compressor. By leveraging a perceptual loss function in conjunction with our proposed message encoder and decoder, our approach simultaneously achieves high image quality and high bit accuracy. Compared to existing techniques, our framework offers superior image secrecy and competitive watermarking robustness in the compressed domain while accelerating the embedding speed by over 50 times. These results demonstrate the potential of combining data hiding techniques and neural compression and offer new insights into developing neural compression techniques and their applications.
<div id='section'>Paperid: <span id='pid'>760, <a href='https://arxiv.org/pdf/2309.02385.pdf' target='_blank'>https://arxiv.org/pdf/2309.02385.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiaxuan Zhang, Alexander J. Gallo, Riccardo M. G. Ferrari
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.02385">Hybrid Design of Multiplicative Watermarking for Defense Against Malicious Parameter Identification</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is a promising active diagnosis technique for detection of highly sophisticated attacks, but is vulnerable to malicious agents that use eavesdropped data to identify and then remove or replicate the watermark. In this work, we propose a hybrid multiplicative watermarking (HMWM) scheme, where the watermark parameters are periodically updated, following the dynamics of the unobservable states of specifically designed piecewise affine (PWA) hybrid systems. We provide a theoretical analysis of the effects of this scheme on the closed-loop performance, and prove that stability properties are preserved. Additionally, we show that the proposed approach makes it difficult for an eavesdropper to reconstruct the watermarking parameters, both in terms of the associated computational complexity and from a systems theoretic perspective.
<div id='section'>Paperid: <span id='pid'>761, <a href='https://arxiv.org/pdf/2307.11628.pdf' target='_blank'>https://arxiv.org/pdf/2307.11628.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingyu Zhu, Guanhui Ye, Xiapu Luo, Xuetao Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.11628">Rethinking Mesh Watermark: Towards Highly Robust and Adaptable Deep 3D Mesh Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The goal of 3D mesh watermarking is to embed the message in 3D meshes that can withstand various attacks imperceptibly and reconstruct the message accurately from watermarked meshes. The watermarking algorithm is supposed to withstand multiple attacks, and the complexity should not grow significantly with the mesh size. Unfortunately, previous methods are less robust against attacks and lack of adaptability. In this paper, we propose a robust and adaptable deep 3D mesh watermarking Deep3DMark that leverages attention-based convolutions in watermarking tasks to embed binary messages in vertex distributions without texture assistance. Furthermore, our Deep3DMark exploits the property that simplified meshes inherit similar relations from the original ones, where the relation is the offset vector directed from one vertex to its neighbor. By doing so, our method can be trained on simplified meshes but remains effective on large size meshes (size adaptable) and unseen categories of meshes (geometry adaptable). Extensive experiments demonstrate our method remains efficient and effective even if the mesh size is 190x increased. Under mesh attacks, Deep3DMark achieves 10%~50% higher accuracy than traditional methods, and 2x higher SNR and 8% higher accuracy than previous DNN-based methods.
<div id='section'>Paperid: <span id='pid'>762, <a href='https://arxiv.org/pdf/2210.07543.pdf' target='_blank'>https://arxiv.org/pdf/2210.07543.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-Wei Chang, Cho-Jui Hsieh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2210.07543">Watermarking Pre-trained Language Models with Backdooring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large pre-trained language models (PLMs) have proven to be a crucial component of modern natural language processing systems. PLMs typically need to be fine-tuned on task-specific downstream datasets, which makes it hard to claim the ownership of PLMs and protect the developer's intellectual property due to the catastrophic forgetting phenomenon. We show that PLMs can be watermarked with a multi-task learning framework by embedding backdoors triggered by specific inputs defined by the owners, and those watermarks are hard to remove even though the watermarked PLMs are fine-tuned on multiple downstream tasks. In addition to using some rare words as triggers, we also show that the combination of common words can be used as backdoor triggers to avoid them being easily detected. Extensive experiments on multiple datasets demonstrate that the embedded watermarks can be robustly extracted with a high success rate and less influenced by the follow-up fine-tuning.
<div id='section'>Paperid: <span id='pid'>763, <a href='https://arxiv.org/pdf/2512.10185.pdf' target='_blank'>https://arxiv.org/pdf/2512.10185.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yangkun Wang, Jingbo Shang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10185">Watermarks for Language Models via Probabilistic Automata</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A recent watermarking scheme for language models achieves distortion-free embedding and robustness to edit-distance attacks. However, it suffers from limited generation diversity and high detection overhead. In parallel, recent research has focused on undetectability, a property ensuring that watermarks remain difficult for adversaries to detect and spoof. In this work, we introduce a new class of watermarking schemes constructed through probabilistic automata. We present two instantiations: (i) a practical scheme with exponential generation diversity and computational efficiency, and (ii) a theoretical construction with formal undetectability guarantees under cryptographic assumptions. Extensive experiments on LLaMA-3B and Mistral-7B validate the superior performance of our scheme in terms of robustness and efficiency.
<div id='section'>Paperid: <span id='pid'>764, <a href='https://arxiv.org/pdf/2511.12663.pdf' target='_blank'>https://arxiv.org/pdf/2511.12663.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Chen Gu, Yingying Sun, Yifan She, Donghui Hu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.12663">FLClear: Visually Verifiable Multi-Client Watermarking for Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) enables multiple clients to collaboratively train a shared global model while preserving the privacy of their local data. Within this paradigm, the intellectual property rights (IPR) of client models are critical assets that must be protected. In practice, the central server responsible for maintaining the global model may maliciously manipulate the global model to erase client contributions or falsely claim sole ownership, thereby infringing on clients' IPR. Watermarking has emerged as a promising technique for asserting model ownership and protecting intellectual property. However, existing FL watermarking approaches remain limited, suffering from potential watermark collisions among clients, insufficient watermark security, and non-intuitive verification mechanisms. In this paper, we propose FLClear, a novel framework that simultaneously achieves collision-free watermark aggregation, enhanced watermark security, and visually interpretable ownership verification. Specifically, FLClear introduces a transposed model jointly optimized with contrastive learning to integrate the watermarking and main task objectives. During verification, the watermark is reconstructed from the transposed model and evaluated through both visual inspection and structural similarity metrics, enabling intuitive and quantitative ownership verification. Comprehensive experiments conducted over various datasets, aggregation schemes, and attack scenarios demonstrate the effectiveness of FLClear and confirm that it consistently outperforms state-of-the-art FL watermarking methods.
<div id='section'>Paperid: <span id='pid'>765, <a href='https://arxiv.org/pdf/2510.17512.pdf' target='_blank'>https://arxiv.org/pdf/2510.17512.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kosta Pavlović, Lazar Stanarević, Petar Nedić, Slavko Kovačević, Igor Djurović
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17512">AWARE: Audio Watermarking with Adversarial Resistance to Edits</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Prevailing practice in learning-based audio watermarking is to pursue robustness by expanding the set of simulated distortions during training. However, such surrogates are narrow and prone to overfitting. This paper presents AWARE (Audio Watermarking with Adversarial Resistance to Edits), an alternative approach that avoids reliance on attack-simulation stacks and handcrafted differentiable distortions. Embedding is obtained via adversarial optimization in the time-frequency domain under a level-proportional perceptual budget. Detection employs a time-order-agnostic detector with a Bitwise Readout Head (BRH) that aggregates temporal evidence into one score per watermark bit, enabling reliable watermark decoding even under desynchronization and temporal cuts. Empirically, AWARE attains high audio quality and speech intelligibility (PESQ/STOI) and consistently low BER across various audio edits, often surpassing representative state-of-the-art learning-based audio watermarking systems.
<div id='section'>Paperid: <span id='pid'>766, <a href='https://arxiv.org/pdf/2510.17033.pdf' target='_blank'>https://arxiv.org/pdf/2510.17033.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leixu Huang, Zedian Shao, Teodora Baluta
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.17033">Watermark Robustness and Radioactivity May Be at Odds in Federated Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning (FL) enables fine-tuning large language models (LLMs) across distributed data sources. As these sources increasingly include LLM-generated text, provenance tracking becomes essential for accountability and transparency. We adapt LLM watermarking for data provenance in FL where a subset of clients compute local updates on watermarked data, and the server averages all updates into the global LLM. In this setup, watermarks are radioactive: the watermark signal remains detectable after fine-tuning with high confidence. The $p$-value can reach $10^{-24}$ even when as little as $6.6\%$ of data is watermarked. However, the server can act as an active adversary that wants to preserve model utility while evading provenance tracking. Our observation is that updates induced by watermarked synthetic data appear as outliers relative to non-watermark updates. Our adversary thus applies strong robust aggregation that can filter these outliers, together with the watermark signal. All evaluated radioactive watermarks are not robust against such an active filtering server. Our work suggests fundamental trade-offs between radioactivity, robustness, and utility.
<div id='section'>Paperid: <span id='pid'>767, <a href='https://arxiv.org/pdf/2506.06389.pdf' target='_blank'>https://arxiv.org/pdf/2506.06389.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rifat Sadik, Tanvir Rahman, Arpan Bhattacharjee, Bikash Chandra Halder, Ismail Hossain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06389">Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning models have shown remarkable success in dermatological image analysis, offering potential for automated skin disease diagnosis. Previously, convolutional neural network(CNN) based architectures have achieved immense popularity and success in computer vision (CV) based task like skin image recognition, generation and video analysis. But with the emergence of transformer based models, CV tasks are now are nowadays carrying out using these models. Vision Transformers (ViTs) is such a transformer-based models that have shown success in computer vision. It uses self-attention mechanisms to achieve state-of-the-art performance across various tasks. However, their reliance on global attention mechanisms makes them susceptible to adversarial perturbations. This paper aims to investigate the susceptibility of ViTs for medical images to adversarial watermarking-a method that adds so-called imperceptible perturbations in order to fool models. By generating adversarial watermarks through Projected Gradient Descent (PGD), we examine the transferability of such attacks to CNNs and analyze the performance defense mechanism -- adversarial training. Results indicate that while performance is not compromised for clean images, ViTs certainly become much more vulnerable to adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless, adversarial training raises it up to 90.0%.
<div id='section'>Paperid: <span id='pid'>768, <a href='https://arxiv.org/pdf/2502.08332.pdf' target='_blank'>https://arxiv.org/pdf/2502.08332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuhang Cai, Yaofei Wang, Donghui Hu, Chen Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.08332">Modification and Generated-Text Detection: Achieving Dual Detection Capabilities for the Outputs of LLM by Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The development of large language models (LLMs) has raised concerns about potential misuse. One practical solution is to embed a watermark in the text, allowing ownership verification through watermark extraction. Existing methods primarily focus on defending against modification attacks, often neglecting other spoofing attacks. For example, attackers can alter the watermarked text to produce harmful content without compromising the presence of the watermark, which could lead to false attribution of this malicious content to the LLM. This situation poses a serious threat to the LLMs service providers and highlights the significance of achieving modification detection and generated-text detection simultaneously. Therefore, we propose a technique to detect modifications in text for unbiased watermark which is sensitive to modification. We introduce a new metric called ``discarded tokens", which measures the number of tokens not included in watermark detection. When a modification occurs, this metric changes and can serve as evidence of the modification. Additionally, we improve the watermark detection process and introduce a novel method for unbiased watermark. Our experiments demonstrate that we can achieve effective dual detection capabilities: modification detection and generated-text detection by watermark.
<div id='section'>Paperid: <span id='pid'>769, <a href='https://arxiv.org/pdf/2411.16598.pdf' target='_blank'>https://arxiv.org/pdf/2411.16598.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andre Kassis, Urs Hengartner, Yaoliang Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.16598">DiffBreak: Is Diffusion-Based Purification Robust?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion-based purification (DBP) has become a cornerstone defense against adversarial examples (AEs), regarded as robust due to its use of diffusion models (DMs) that project AEs onto the natural data manifold. We refute this core claim, theoretically proving that gradient-based attacks effectively target the DM rather than the classifier, causing DBP's outputs to align with adversarial distributions. This prompts a reassessment of DBP's robustness, attributing it to two critical flaws: incorrect gradients and inappropriate evaluation protocols that test only a single random purification of the AE. We show that with proper accounting for stochasticity and resubmission risk, DBP collapses. To support this, we introduce DiffBreak, the first reliable toolkit for differentiation through DBP, eliminating gradient flaws that previously further inflated robustness estimates. We also analyze the current defense scheme used for DBP where classification relies on a single purification, pinpointing its inherent invalidity. We provide a statistically grounded majority-vote (MV) alternative that aggregates predictions across multiple purified copies, showing partial but meaningful robustness gain. We then propose a novel adaptation of an optimization method against deepfake watermarking, crafting systemic perturbations that defeat DBP even under MV, challenging DBP's viability.
<div id='section'>Paperid: <span id='pid'>770, <a href='https://arxiv.org/pdf/2411.15367.pdf' target='_blank'>https://arxiv.org/pdf/2411.15367.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Soumil Datta, Shih-Chieh Dai, Leo Yu, Guanhong Tao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.15367">Exploiting Watermark-Based Defense Mechanisms in Text-to-Image Diffusion Models for Unauthorized Data Usage</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-image diffusion models, such as Stable Diffusion, have shown exceptional potential in generating high-quality images. However, recent studies highlight concerns over the use of unauthorized data in training these models, which may lead to intellectual property infringement or privacy violations. A promising approach to mitigate these issues is to apply a watermark to images and subsequently check if generative models reproduce similar watermark features. In this paper, we examine the robustness of various watermark-based protection methods applied to text-to-image models. We observe that common image transformations are ineffective at removing the watermark effect. Therefore, we propose RATTAN, that leverages the diffusion process to conduct controlled image generation on the protected input, preserving the high-level features of the input while ignoring the low-level details utilized by watermarks. A small number of generated images are then used to fine-tune protected models. Our experiments on three datasets and 140 text-to-image diffusion models reveal that existing state-of-the-art protections are not robust against RATTAN.
<div id='section'>Paperid: <span id='pid'>771, <a href='https://arxiv.org/pdf/2411.05091.pdf' target='_blank'>https://arxiv.org/pdf/2411.05091.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Agnibh Dasgupta, Abdullah Tanvir, Xin Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05091">Watermarking Language Models through Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking the outputs of large language models (LLMs) is critical for provenance tracing, content regulation, and model accountability. Existing approaches often rely on access to model internals or are constrained by static rules and token-level perturbations. Moreover, the idea of steering generative behavior via prompt-based instruction control remains largely underexplored. We introduce a prompt-guided watermarking framework that operates entirely at the input level and requires no access to model parameters or decoding logits. The framework comprises three cooperating components: a Prompting LM that synthesizes watermarking instructions from user prompts, a Marking LM that generates watermarked outputs conditioned on these instructions, and a Detecting LM trained to classify whether a response carries an embedded watermark. This modular design enables dynamic watermarking that adapts to individual prompts while remaining compatible with diverse LLM architectures, including both proprietary and open-weight models. We evaluate the framework over 25 combinations of Prompting and Marking LMs, such as GPT-4o, Mistral, LLaMA3, and DeepSeek. Experimental results show that watermark signals generalize across architectures and remain robust under fine-tuning, model distillation, and prompt-based adversarial attacks, demonstrating the effectiveness and robustness of the proposed approach.
<div id='section'>Paperid: <span id='pid'>772, <a href='https://arxiv.org/pdf/2409.13382.pdf' target='_blank'>https://arxiv.org/pdf/2409.13382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauri Juvela, Xin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.13382">Audio Codec Augmentation for Robust Collaborative Watermarking of Speech Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Automatic detection of synthetic speech is becoming increasingly important as current synthesis methods are both near indistinguishable from human speech and widely accessible to the public. Audio watermarking and other active disclosure methods of are attracting research activity, as they can complement traditional deepfake defenses based on passive detection. In both active and passive detection, robustness is of major interest. Traditional audio watermarks are particularly susceptible to removal attacks by audio codec application. Most generated speech and audio content released into the wild passes through an audio codec purely as a distribution method. We recently proposed collaborative watermarking as method for making generated speech more easily detectable over a noisy but differentiable transmission channel. This paper extends the channel augmentation to work with non-differentiable traditional audio codecs and neural audio codecs and evaluates transferability and effect of codec bitrate over various configurations. The results show that collaborative watermarking can be reliably augmented by black-box audio codecs using a waveform-domain straight-through-estimator for gradient approximation. Furthermore, that results show that channel augmentation with a neural audio codec transfers well to traditional codecs. Listening tests demonstrate collaborative watermarking incurs negligible perceptual degradation with high bitrate codecs or DAC at 8kbps.
<div id='section'>Paperid: <span id='pid'>773, <a href='https://arxiv.org/pdf/2409.09996.pdf' target='_blank'>https://arxiv.org/pdf/2409.09996.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yuzhang Chen, Jiangnan Zhu, Yujie Gu, Minoru Kuribayashi, Kouichi Sakurai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.09996">FreeMark: A Non-Invasive White-Box Watermarking for Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) have achieved significant success in real-world applications. However, safeguarding their intellectual property (IP) remains extremely challenging. Existing DNN watermarking for IP protection often require modifying DNN models, which reduces model performance and limits their practicality.
  This paper introduces FreeMark, a novel DNN watermarking framework that leverages cryptographic principles without altering the original host DNN model, thereby avoiding any reduction in model performance. Unlike traditional DNN watermarking methods, FreeMark innovatively generates secret keys from a pre-generated watermark vector and the host model using gradient descent. These secret keys, used to extract watermark from the model's activation values, are securely stored with a trusted third party, enabling reliable watermark extraction from suspect models. Extensive experiments demonstrate that FreeMark effectively resists various watermark removal attacks while maintaining high watermark capacity.
<div id='section'>Paperid: <span id='pid'>774, <a href='https://arxiv.org/pdf/2409.02915.pdf' target='_blank'>https://arxiv.org/pdf/2409.02915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Robin San Roman, Pierre Fernandez, Antoine Deleforge, Yossi Adi, Romain Serizel
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.02915">Latent Watermarking of Audio Generative Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advancements in audio generative models have opened up new challenges in their responsible disclosure and the detection of their misuse. In response, we introduce a method to watermark latent generative models by a specific watermarking of their training data. The resulting watermarked models produce latent representations whose decoded outputs are detected with high confidence, regardless of the decoding method used. This approach enables the detection of the generated content without the need for a post-hoc watermarking step. It provides a more secure solution for open-sourced models and facilitates the identification of derivative works that fine-tune or use these models without adhering to their license terms. Our results indicate for instance that generated outputs are detected with an accuracy of more than 75% at a false positive rate of $10^{-3}$, even after fine-tuning the latent generative model.
<div id='section'>Paperid: <span id='pid'>775, <a href='https://arxiv.org/pdf/2405.15161.pdf' target='_blank'>https://arxiv.org/pdf/2405.15161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Huali Ren, Anli Yan, Chong-zhi Gao, Hongyang Yan, Zhenxin Zhang, Jin Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.15161">Are You Copying My Prompt? Protecting the Copyright of Vision Prompt for VPaaS via Watermark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Visual Prompt Learning (VPL) differs from traditional fine-tuning methods in reducing significant resource consumption by avoiding updating pre-trained model parameters. Instead, it focuses on learning an input perturbation, a visual prompt, added to downstream task data for making predictions. Since learning generalizable prompts requires expert design and creation, which is technically demanding and time-consuming in the optimization process, developers of Visual Prompts as a Service (VPaaS) have emerged. These developers profit by providing well-crafted prompts to authorized customers. However, a significant drawback is that prompts can be easily copied and redistributed, threatening the intellectual property of VPaaS developers. Hence, there is an urgent need for technology to protect the rights of VPaaS developers. To this end, we present a method named \textbf{WVPrompt} that employs visual prompt watermarking in a black-box way. WVPrompt consists of two parts: prompt watermarking and prompt verification. Specifically, it utilizes a poison-only backdoor attack method to embed a watermark into the prompt and then employs a hypothesis-testing approach for remote verification of prompt ownership. Extensive experiments have been conducted on three well-known benchmark datasets using three popular pre-trained models: RN50, BIT-M, and Instagram. The experimental results demonstrate that WVPrompt is efficient, harmless, and robust to various adversarial operations.
<div id='section'>Paperid: <span id='pid'>776, <a href='https://arxiv.org/pdf/2405.08363.pdf' target='_blank'>https://arxiv.org/pdf/2405.08363.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Andre Kassis, Urs Hengartner
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.08363">UnMarker: A Universal Attack on Defensive Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Reports regarding the misuse of Generative AI (GenAI) to create deepfakes are frequent. Defensive watermarking enables GenAI providers to hide fingerprints in their images and use them later for deepfake detection. Yet, its potential has not been fully explored. We present UnMarker -- the first practical universal attack on defensive watermarking. Unlike existing attacks, UnMarker requires no detector feedback, no unrealistic knowledge of the watermarking scheme or similar models, and no advanced denoising pipelines that may not be available. Instead, being the product of an in-depth analysis of the watermarking paradigm revealing that robust schemes must construct their watermarks in the spectral amplitudes, UnMarker employs two novel adversarial optimizations to disrupt the spectra of watermarked images, erasing the watermarks. Evaluations against SOTA schemes prove UnMarker's effectiveness. It not only defeats traditional schemes while retaining superior quality compared to existing attacks but also breaks semantic watermarks that alter an image's structure, reducing the best detection rate to $43\%$ and rendering them useless. To our knowledge, UnMarker is the first practical attack on semantic watermarks, which have been deemed the future of defensive watermarking. Our findings show that defensive watermarking is not a viable defense against deepfakes, and we urge the community to explore alternatives.
<div id='section'>Paperid: <span id='pid'>777, <a href='https://arxiv.org/pdf/2404.09401.pdf' target='_blank'>https://arxiv.org/pdf/2404.09401.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peifei Zhu, Tsubasa Takahashi, Hirokatsu Kataoka
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.09401">Watermark-embedded Adversarial Examples for Copyright Protection against Diffusion Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Diffusion Models (DMs) have shown remarkable capabilities in various image-generation tasks. However, there are growing concerns that DMs could be used to imitate unauthorized creations and thus raise copyright issues. To address this issue, we propose a novel framework that embeds personal watermarks in the generation of adversarial examples. Such examples can force DMs to generate images with visible watermarks and prevent DMs from imitating unauthorized images. We construct a generator based on conditional adversarial networks and design three losses (adversarial loss, GAN loss, and perturbation loss) to generate adversarial examples that have subtle perturbation but can effectively attack DMs to prevent copyright violations. Training a generator for a personal watermark by our method only requires 5-10 samples within 2-3 minutes, and once the generator is trained, it can generate adversarial examples with that watermark significantly fast (0.2s per image). We conduct extensive experiments in various conditional image-generation scenarios. Compared to existing methods that generate images with chaotic textures, our method adds visible watermarks on the generated images, which is a more straightforward way to indicate copyright violations. We also observe that our adversarial examples exhibit good transferability across unknown generative models. Therefore, this work provides a simple yet powerful way to protect copyright from DM-based imitation.
<div id='section'>Paperid: <span id='pid'>778, <a href='https://arxiv.org/pdf/2404.00850.pdf' target='_blank'>https://arxiv.org/pdf/2404.00850.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christoforos Somarakis, Raman Goyal, Erfaun Noorani, Shantanu Rane
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.00850">Delay-Induced Watermarking for Detection of Replay Attacks in Linear Systems</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A state-feedback watermarking signal design for the detection of replay attacks in linear systems is proposed. The control input is augmented with a random time-delayed term of the system state estimate, in order to secure the system against attacks of replay type. We outline the basic analysis of the closed-loop response of the state-feedback watermarking in a LQG controlled system. Our theoretical results are applied on a temperature process control example. While the proposed secure control scheme requires very involved analysis, it, nevertheless, holds promise of being superior to conventional, feed-forward, watermarking schemes, in both its ability to detect attacks as well as the secured system performance.
<div id='section'>Paperid: <span id='pid'>779, <a href='https://arxiv.org/pdf/2403.14719.pdf' target='_blank'>https://arxiv.org/pdf/2403.14719.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qilong Wu, Varun Chandrasekaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.14719">Bypassing LLM Watermarks with Color-Aware Substitutions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking approaches are proposed to identify if text being circulated is human or large language model (LLM) generated. The state-of-the-art watermarking strategy of Kirchenbauer et al. (2023a) biases the LLM to generate specific (``green'') tokens. However, determining the robustness of this watermarking method is an open problem. Existing attack methods fail to evade detection for longer text segments. We overcome this limitation, and propose {\em Self Color Testing-based Substitution (SCTS)}, the first ``color-aware'' attack. SCTS obtains color information by strategically prompting the watermarked LLM and comparing output tokens frequencies. It uses this information to determine token colors, and substitutes green tokens with non-green ones. In our experiments, SCTS successfully evades watermark detection using fewer number of edits than related work. Additionally, we show both theoretically and empirically that SCTS can remove the watermark for arbitrarily long watermarked text.
<div id='section'>Paperid: <span id='pid'>780, <a href='https://arxiv.org/pdf/2402.03760.pdf' target='_blank'>https://arxiv.org/pdf/2402.03760.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yali Yuan, Jian Ge, Guang Cheng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.03760">DeMarking: A Defense for Network Flow Watermarking in Real-Time</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The network flow watermarking technique associates the two communicating parties by actively modifying certain characteristics of the stream generated by the sender so that it covertly carries some special marking information. Some curious users communicating with the hidden server as a Tor client may attempt de-anonymization attacks to uncover the real identity of the hidden server by using this technique. This compromises the privacy of the anonymized communication system. Therefore, we propose a defense scheme against flow watermarking. The scheme is based on deep neural networks and utilizes generative adversarial networks to convert the original Inter-Packet Delays (IPD) into new IPDs generated by the model. We also adopt the concept of adversarial attacks to ensure that the detector will produce an incorrect classification when detecting these new IPDs. This approach ensures that these IPDs are considered "clean", effectively covering the potential watermarks. This scheme is effective against time-based flow watermarking techniques.
<div id='section'>Paperid: <span id='pid'>781, <a href='https://arxiv.org/pdf/2312.14260.pdf' target='_blank'>https://arxiv.org/pdf/2312.14260.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Janvi Thakkar, Giulio Zizzo, Sergio Maffeis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.14260">Elevating Defenses: Bridging Adversarial Training and Watermarking for Model Resilience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Machine learning models are being used in an increasing number of critical applications; thus, securing their integrity and ownership is critical. Recent studies observed that adversarial training and watermarking have a conflicting interaction. This work introduces a novel framework to integrate adversarial training with watermarking techniques to fortify against evasion attacks and provide confident model verification in case of intellectual property theft. We use adversarial training together with adversarial watermarks to train a robust watermarked model. The key intuition is to use a higher perturbation budget to generate adversarial watermarks compared to the budget used for adversarial training, thus avoiding conflict. We use the MNIST and Fashion-MNIST datasets to evaluate our proposed technique on various model stealing attacks. The results obtained consistently outperform the existing baseline in terms of robustness performance and further prove the resilience of this defense against pruning and fine-tuning removal attacks.
<div id='section'>Paperid: <span id='pid'>782, <a href='https://arxiv.org/pdf/2312.09125.pdf' target='_blank'>https://arxiv.org/pdf/2312.09125.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>DevriÅ Ä°Åler, Seoyeon Hwang, Yoshimichi Nakatsuka, Nikolaos Laoutaris, Gene Tsudik
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.09125">Puppy: A Publicly Verifiable Watermarking Protocol</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper, we propose Puppy, the first formally defined framework for converting any symmetric watermarking into a publicly verifiable one. Puppy allows anyone to verify a watermark any number of times with the help of an untrusted third party, without requiring owner presence during detection. We formally define and prove security of Puppy using the ideal/real-world simulation paradigm and construct two practical and secure instances: (1) Puppy-TEE that uses Trusted Execution Environments (TEEs), and (2) Puppy-2PC that relies on two-party computation (2PC) based on garbled circuits. We then convert four current symmetric watermarking schemes into publicly verifiable ones and run extensive experiments using Puppy-TEE and Puppy-2PC. Evaluation results show that, while Puppy-TEE incurs some overhead, its total latency is on the order of milliseconds for three out of four watermarking schemes. Although the overhead of Puppy-2PC is higher (on the order of seconds), it is viable for settings that lack a TEE or where strong trust assumptions about a TEE need to be avoided. We further optimize the solution to increase its scalability and resilience to denial of service attacks via memoization.
<div id='section'>Paperid: <span id='pid'>783, <a href='https://arxiv.org/pdf/2309.15224.pdf' target='_blank'>https://arxiv.org/pdf/2309.15224.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lauri Juvela, Xin Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2309.15224">Collaborative Watermarking for Adversarial Speech Synthesis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in neural speech synthesis have brought us technology that is not only close to human naturalness, but is also capable of instant voice cloning with little data, and is highly accessible with pre-trained models available. Naturally, the potential flood of generated content raises the need for synthetic speech detection and watermarking. Recently, considerable research effort in synthetic speech detection has been related to the Automatic Speaker Verification and Spoofing Countermeasure Challenge (ASVspoof), which focuses on passive countermeasures. This paper takes a complementary view to generated speech detection: a synthesis system should make an active effort to watermark the generated speech in a way that aids detection by another machine, but remains transparent to a human listener. We propose a collaborative training scheme for synthetic speech watermarking and show that a HiFi-GAN neural vocoder collaborating with the ASVspoof 2021 baseline countermeasure models consistently improves detection performance over conventional classifier training. Furthermore, we demonstrate how collaborative training can be paired with augmentation strategies for added robustness against noise and time-stretching. Finally, listening tests demonstrate that collaborative training has little adverse effect on perceptual quality of vocoded speech.
<div id='section'>Paperid: <span id='pid'>784, <a href='https://arxiv.org/pdf/2308.04603.pdf' target='_blank'>https://arxiv.org/pdf/2308.04603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xin Zhong, Arjon Das, Fahad Alrasheedi, Abdullah Tanvir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.04603">A Brief Yet In-Depth Survey of Deep Learning-Based Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a comprehensive survey on deep learning-based image watermarking, a technique that entails the invisible embedding and extraction of watermarks within a cover image, aiming to offer a seamless blend of robustness and adaptability. We navigate the complex landscape of this interdisciplinary domain, linking historical foundations, current innovations, and prospective developments. Unlike existing literature, our study concentrates exclusively on image watermarking with deep learning, delivering an in-depth, yet brief analysis enriched by three fundamental contributions. First, we introduce a refined categorization, segmenting the field into Embedder-Extractor, Deep Networks as a Feature Transformation, and Hybrid Methods. This taxonomy, inspired by the varied roles of deep learning across studies, is designed to infuse clarity, offering readers technical insights and directional guidance. Second, our exploration dives into representative methodologies, encapsulating the diverse research directions and inherent challenges within each category to provide a consolidated perspective. Lastly, we venture beyond established boundaries to outline emerging frontiers, offering a detailed insight into prospective research avenues.
<div id='section'>Paperid: <span id='pid'>785, <a href='https://arxiv.org/pdf/2305.05773.pdf' target='_blank'>https://arxiv.org/pdf/2305.05773.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Travis Munyer, Abdullah Tanvir, Arjon Das, Xin Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.05773">DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of text generators. With the potential for misuse escalating, the importance of discerning whether texts are human-authored or generated by LLMs has become paramount. Several preceding studies have ventured to address this challenge by employing binary classifiers to differentiate between human-written and LLM-generated text. Nevertheless, the reliability of these classifiers has been subject to question. Given that consequential decisions may hinge on the outcome of such classification, it is imperative that text source detection is of high caliber. In light of this, the present paper introduces DeepTextMark, a deep learning-driven text watermarking methodology devised for text source identification. By leveraging Word2Vec and Sentence Encoding for watermark insertion, alongside a transformer-based classifier for watermark detection, DeepTextMark epitomizes a blend of blindness, robustness, imperceptibility, and reliability. As elaborated within the paper, these attributes are crucial for universal text source detection, with a particular emphasis in this paper on text produced by LLMs. DeepTextMark offers a viable "add-on" solution to prevailing text generation frameworks, requiring no direct access or alterations to the underlying text generation mechanism. Experimental evaluations underscore the high imperceptibility, elevated detection accuracy, augmented robustness, reliability, and swift execution of DeepTextMark.
<div id='section'>Paperid: <span id='pid'>786, <a href='https://arxiv.org/pdf/2304.11215.pdf' target='_blank'>https://arxiv.org/pdf/2304.11215.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alejo Jose G. Sison, Marco Tulio Daza, Roberto Gozalo-Brizuela, Eduardo C. Garrido-MerchÃ¡n
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.11215">ChatGPT: More than a Weapon of Mass Deception, Ethical challenges and responses from the Human-Centered Artificial Intelligence (HCAI) perspective</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This article explores the ethical problems arising from the use of ChatGPT as a kind of generative AI and suggests responses based on the Human-Centered Artificial Intelligence (HCAI) framework. The HCAI framework is appropriate because it understands technology above all as a tool to empower, augment, and enhance human agency while referring to human wellbeing as a grand challenge, thus perfectly aligning itself with ethics, the science of human flourishing. Further, HCAI provides objectives, principles, procedures, and structures for reliable, safe, and trustworthy AI which we apply to our ChatGPT assessments. The main danger ChatGPT presents is the propensity to be used as a weapon of mass deception (WMD) and an enabler of criminal activities involving deceit. We review technical specifications to better comprehend its potentials and limitations. We then suggest both technical (watermarking, styleme, detectors, and fact-checkers) and non-technical measures (terms of use, transparency, educator considerations, HITL) to mitigate ChatGPT misuse or abuse and recommend best uses (creative writing, non-creative writing, teaching and learning). We conclude with considerations regarding the role of humans in ensuring the proper use of ChatGPT for individual and social wellbeing.
<div id='section'>Paperid: <span id='pid'>787, <a href='https://arxiv.org/pdf/2302.11361.pdf' target='_blank'>https://arxiv.org/pdf/2302.11361.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Khan, Minoru Kuribayashi, KokSheik Wong, Vishnu Monn Baskaran
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11361">HDR image watermarking using saliency detection and quantization index modulation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>High-dynamic range (HDR) images are circulated rapidly over the internet with risks of being exploited for unauthorized usage. To protect these images, some HDR image based watermarking (HDR-IW) methods were put forward. However, they inherited the same problem faced by conventional IW methods for standard dynamic range (SDR) images, where only trade-offs among conflicting requirements are managed instead of simultaneous improvement. In this paper, a novel saliency (eye-catching object) detection based trade-off independent HDR-IW is proposed, to simultaneously improve robustness, imperceptibility and payload. First, the host image goes through our proposed salient object detection model to produce a saliency map, which is, in turn, exploited to segment the foreground and background of the host image. Next, the binary watermark is partitioned into the foregrounds and backgrounds using the same mask and scrambled using a random permutation algorithm. Finally, the watermark segments are embedded into selected bit-plane of the corresponding host segments using quantized indexed modulation. Experimental results suggest that the proposed work outperforms state-of-the-art methods in terms of improving the conflicting requirements.
<div id='section'>Paperid: <span id='pid'>788, <a href='https://arxiv.org/pdf/2511.06645.pdf' target='_blank'>https://arxiv.org/pdf/2511.06645.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xingchi Li, Xiaochi Liu, Guanxun Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.06645">Adaptive Testing for Segmenting Watermarked Texts From Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid adoption of large language models (LLMs), such as GPT-4 and Claude 3.5, underscores the need to distinguish LLM-generated text from human-written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM-generated text to enable reliable identification. In this paper, we first generalize the likelihood-based LLM detection method of a previous study by introducing a flexible weighted formulation, and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non-watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next-token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non-watermarked content.
<div id='section'>Paperid: <span id='pid'>789, <a href='https://arxiv.org/pdf/2510.27533.pdf' target='_blank'>https://arxiv.org/pdf/2510.27533.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Khandoker Ashik Uz Zaman, Mohammad Zahangir Alam, Mohammed N. M. Ali, Mahdi H. Miraz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27533">Deep Neural Watermarking for Robust Copyright Protection in 3D Point Clouds</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The protection of intellectual property has become critical due to the rapid growth of three-dimensional content in digital media. Unlike traditional images or videos, 3D point clouds present unique challenges for copyright enforcement, as they are especially vulnerable to a range of geometric and non-geometric attacks that can easily degrade or remove conventional watermark signals. In this paper, we address these challenges by proposing a robust deep neural watermarking framework for 3D point cloud copyright protection and ownership verification. Our approach embeds binary watermarks into the singular values of 3D point cloud blocks using spectral decomposition, i.e. Singular Value Decomposition (SVD), and leverages the extraction capabilities of Deep Learning using PointNet++ neural network architecture. The network is trained to reliably extract watermarks even after the data undergoes various attacks such as rotation, scaling, noise, cropping and signal distortions. We validated our method using the publicly available ModelNet40 dataset, demonstrating that deep learning-based extraction significantly outperforms traditional SVD-based techniques under challenging conditions. Our experimental evaluation demonstrates that the deep learning-based extraction approach significantly outperforms existing SVD-based methods with deep learning achieving bitwise accuracy up to 0.83 and Intersection over Union (IoU) of 0.80, compared to SVD achieving a bitwise accuracy of 0.58 and IoU of 0.26 for the Crop (70%) attack, which is the most severe geometric distortion in our experiment. This demonstrates our method's ability to achieve superior watermark recovery and maintain high fidelity even under severe distortions.
<div id='section'>Paperid: <span id='pid'>790, <a href='https://arxiv.org/pdf/2510.25411.pdf' target='_blank'>https://arxiv.org/pdf/2510.25411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sana Hafeez, Ghulam E Mustafa Abro, Hifza Mustafa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25411">Quantum-Resilient Threat Modelling for Secure RIS-Assisted ISAC in 6G UAV Corridors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid deployment of unmanned aerial vehicle (UAV) corridors in sixth-generation (6G) networks requires safe, intelligence-driven integrated sensing and communications (ISAC). Reconfigurable intelligent surfaces (RIS) enhance spectrum efficiency, localisation accuracy, and situational awareness, while introducing new vulnerabilities. The rise of quantum computing increases the risks associated with harvest-now-decrypt-later strategies and quantum-enhanced spoofing. We propose a Quantum-Resilient Threat Modelling (QRTM) framework for RIS-assisted ISAC in UAV corridors to address these challenges. QRTM integrates classical, quantum-ready, and quantum-aided adversaries, countered using post-quantum cryptographic (PQC) primitives: ML-KEM for key establishment and Falcon for authentication, both embedded within RIS control signalling and UAV coordination. To strengthen security sensing, the framework introduces RIS-coded scene watermarking validated through a generalised likelihood ratio test (GLRT), with its detection probability characterised by the Marcum Q function. Furthermore, a Secure ISAC Utility (SIU) jointly optimises secrecy rate, spoofing detection, and throughput under RIS constraints, enabled by a scheduler with computational complexity of O(n^2). Monte Carlo evaluations using 3GPP Release 19 mid-band urban-canyon models (7-15 GHz) demonstrate a spoof-detection probability approaching 0.99 at a false-alarm rate of 1e-3, secrecy-rate retention exceeding 90 percent against quantum-capable adversaries, and signal-interference utilisation improvements of about 25 percent compared with baselines. These results show a standards-compliant path towards reliable, quantum-resilient ISAC for UAV corridors in smart cities and non-terrestrial networks.
<div id='section'>Paperid: <span id='pid'>791, <a href='https://arxiv.org/pdf/2510.18019.pdf' target='_blank'>https://arxiv.org/pdf/2510.18019.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Asim Mohamed, Martin Gubri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.18019">Is Multilingual LLM Watermarking Truly Multilingual? A Simple Back-Translation Solution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multilingual watermarking aims to make large language model (LLM) outputs traceable across languages, yet current methods still fall short. Despite claims of cross-lingual robustness, they are evaluated only on high-resource languages. We show that existing multilingual watermarking methods are not truly multilingual: they fail to remain robust under translation attacks in medium- and low-resource languages. We trace this failure to semantic clustering, which fails when the tokenizer vocabulary contains too few full-word tokens for a given language. To address this, we introduce STEAM, a back-translation-based detection method that restores watermark strength lost through translation. STEAM is compatible with any watermarking method, robust across different tokenizers and languages, non-invasive, and easily extendable to new languages. With average gains of +0.19 AUC and +40%p TPR@1% on 17 languages, STEAM provides a simple and robust path toward fairer watermarking across diverse languages.
<div id='section'>Paperid: <span id='pid'>792, <a href='https://arxiv.org/pdf/2507.20650.pdf' target='_blank'>https://arxiv.org/pdf/2507.20650.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhicheng Zhang, Peizhuo Lv, Mengke Wan, Jiang Fang, Diandian Guo, Yezeng Chen, Yinlong Liu, Wei Ma, Jiyan Sun, Liru Geng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.20650">Hot-Swap MarkBoard: An Efficient Black-box Watermarking Approach for Large-scale Model Distribution</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, Deep Learning (DL) models have been increasingly deployed on end-user devices as On-Device AI, offering improved efficiency and privacy. However, this deployment trend poses more serious Intellectual Property (IP) risks, as models are distributed on numerous local devices, making them vulnerable to theft and redistribution. Most existing ownership protection solutions (e.g., backdoor-based watermarking) are designed for cloud-based AI-as-a-Service (AIaaS) and are not directly applicable to large-scale distribution scenarios, where each user-specific model instance must carry a unique watermark. These methods typically embed a fixed watermark, and modifying the embedded watermark requires retraining the model. To address these challenges, we propose Hot-Swap MarkBoard, an efficient watermarking method. It encodes user-specific $n$-bit binary signatures by independently embedding multiple watermarks into a multi-branch Low-Rank Adaptation (LoRA) module, enabling efficient watermark customization without retraining through branch swapping. A parameter obfuscation mechanism further entangles the watermark weights with those of the base model, preventing removal without degrading model performance. The method supports black-box verification and is compatible with various model architectures and DL tasks, including classification, image generation, and text generation. Extensive experiments across three types of tasks and six backbone models demonstrate our method's superior efficiency and adaptability compared to existing approaches, achieving 100\% verification accuracy.
<div id='section'>Paperid: <span id='pid'>793, <a href='https://arxiv.org/pdf/2506.21393.pdf' target='_blank'>https://arxiv.org/pdf/2506.21393.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junwen Zhang, Pu Chen, Yin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.21393">TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multimodal understanding of tables in real-world contexts is challenging due to the complexity of structure, symbolic density, and visual degradation (blur, skew, watermarking, incomplete structures or fonts, multi-span or hierarchically nested layouts). Existing multimodal large language models (MLLMs) struggle with such WildStruct conditions, resulting in limited performance and poor generalization. To address these challenges, we propose TableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture specifically designed for robust, structured reasoning over multimodal table data. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which predicts latent semantic token roles (e.g., header, data cell, axis, formula) and dynamically routes table elements to specialized experts (Table-to-HTML, Table-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed by symbolic reasoning graphs. To facilitate effective alignment-driven pretraining, we introduce the large-scale TableMoE-Align dataset, consisting of 1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and industry, utilized exclusively for model pretraining. For evaluation, we curate and release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA, WMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models under real-world multimodal degradation and structural complexity. Experimental results demonstrate that TableMoE significantly surpasses existing state-of-the-art models. Extensive ablation studies validate each core component, emphasizing the critical role of Neuro-Symbolic Routing and structured expert alignment. Through qualitative analyses, we further showcase TableMoE's interpretability and enhanced robustness, underscoring the effectiveness of integrating neuro-symbolic reasoning for multimodal table understanding.
<div id='section'>Paperid: <span id='pid'>794, <a href='https://arxiv.org/pdf/2506.17134.pdf' target='_blank'>https://arxiv.org/pdf/2506.17134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Md Sakibur Sajal, Marc Dandin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.17134">Dynamic Watermark Generation for Digital Images using Perimeter Gated SPAD Imager PUFs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital image watermarks as a security feature can be derived from the imager's physically unclonable functions (PUFs) by utilizing the manufacturing variations, i.e., the dark signal non-uniformity (DSNU). While a few demonstrations focused on the CMOS image sensors (CIS) and active pixel sensors (APS), single photon avalanche diode (SPAD) imagers have never been investigated for this purpose. In this work, we have proposed a novel watermarking technique using perimeter gated SPAD (pgSPAD) imagers. We utilized the DSNU of three 64 x 64 pgSPAD imager chips, fabricated in a 0.35 Î¼m standard CMOS process and analyzed the simulated watermarks for standard test images from publicly available database. Our observation shows that both source identification and tamper detection can be achieved using the proposed source-scene-specific dynamic watermarks with a controllable sensitivity-robustness trade-off.
<div id='section'>Paperid: <span id='pid'>795, <a href='https://arxiv.org/pdf/2506.08602.pdf' target='_blank'>https://arxiv.org/pdf/2506.08602.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tingzhi Li, Xuefeng Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.08602">WGLE:Backdoor-free and Multi-bit Black-box Watermarking for Graph Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) are increasingly deployed in graph-related applications, making ownership verification critical to protect their intellectual property against model theft. Fingerprinting and black-box watermarking are two main methods. However, the former relies on determining model similarity, which is computationally expensive and prone to ownership collisions after model post-processing such as model pruning or fine-tuning. The latter embeds backdoors, exposing watermarked models to the risk of backdoor attacks. Moreover, both methods enable ownership verification but do not convey additional information. As a result, each distributed model requires a unique trigger graph, and all trigger graphs must be used to query the suspect model during verification. Multiple queries increase the financial cost and the risk of detection.
  To address these challenges, this paper proposes WGLE, a novel black-box watermarking paradigm for GNNs that enables embedding the multi-bit string as the ownership information without using backdoors. WGLE builds on a key insight we term Layer-wise Distance Difference on an Edge (LDDE), which quantifies the difference between the feature distance and the prediction distance of two connected nodes. By predefining positive or negative LDDE values for multiple selected edges, WGLE embeds the watermark encoding the intended information without introducing incorrect mappings that compromise the primary task. WGLE is evaluated on six public datasets and six mainstream GNN architectures along with state-of-the-art methods. The results show that WGLE achieves 100% ownership verification accuracy, an average fidelity degradation of 0.85%, comparable robustness against potential attacks, and low embedding overhead. The code is available in the repository.
<div id='section'>Paperid: <span id='pid'>796, <a href='https://arxiv.org/pdf/2506.05502.pdf' target='_blank'>https://arxiv.org/pdf/2506.05502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ya Jiang, Chuxiong Wu, Massieh Kordi Boroujeny, Brian Mark, Kai Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.05502">StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking for large language models (LLMs) offers a promising approach to identifying AI-generated text. Existing approaches, however, either compromise the distribution of original generated text by LLMs or are limited to embedding zero-bit information that only allows for watermark detection but ignores identification. We present StealthInk, a stealthy multi-bit watermarking scheme that preserves the original text distribution while enabling the embedding of provenance data, such as userID, TimeStamp, and modelID, within LLM-generated text. This enhances fast traceability without requiring access to the language model's API or prompts. We derive a lower bound on the number of tokens necessary for watermark detection at a fixed equal error rate, which provides insights on how to enhance the capacity. Comprehensive empirical evaluations across diverse tasks highlight the stealthiness, detectability, and resilience of StealthInk, establishing it as an effective solution for LLM watermarking applications.
<div id='section'>Paperid: <span id='pid'>797, <a href='https://arxiv.org/pdf/2505.14673.pdf' target='_blank'>https://arxiv.org/pdf/2505.14673.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yu Tong, Zihao Pan, Shuai Yang, Kaiyang Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.14673">Training-Free Watermarking for Autoregressive Image Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invisible image watermarking can protect image ownership and prevent malicious misuse of visual generative models. However, existing generative watermarking methods are mainly designed for diffusion models while watermarking for autoregressive image generation models remains largely underexplored. We propose IndexMark, a training-free watermarking framework for autoregressive image generation models. IndexMark is inspired by the redundancy property of the codebook: replacing autoregressively generated indices with similar indices produces negligible visual differences. The core component in IndexMark is a simple yet effective match-then-replace method, which carefully selects watermark tokens from the codebook based on token similarity, and promotes the use of watermark tokens through token replacement, thereby embedding the watermark without affecting the image quality. Watermark verification is achieved by calculating the proportion of watermark tokens in generated images, with precision further improved by an Index Encoder. Furthermore, we introduce an auxiliary validation scheme to enhance robustness against cropping attacks. Experiments demonstrate that IndexMark achieves state-of-the-art performance in terms of image quality and verification accuracy, and exhibits robustness against various perturbations, including cropping, noises, Gaussian blur, random erasing, color jittering, and JPEG compression.
<div id='section'>Paperid: <span id='pid'>798, <a href='https://arxiv.org/pdf/2505.05190.pdf' target='_blank'>https://arxiv.org/pdf/2505.05190.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yixin Cheng, Hongcheng Guo, Yangming Li, Leonid Sigal
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05190">Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text watermarking aims to subtly embed statistical signals into text by controlling the Large Language Model (LLM)'s sampling process, enabling watermark detectors to verify that the output was generated by the specified model. The robustness of these watermarking algorithms has become a key factor in evaluating their effectiveness. Current text watermarking algorithms embed watermarks in high-entropy tokens to ensure text quality. In this paper, we reveal that this seemingly benign design can be exploited by attackers, posing a significant risk to the robustness of the watermark. We introduce a generic efficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA), which leverages the vulnerability by calculating the self-information of each token to identify potential pattern tokens and perform targeted attack. Our work exposes a widely prevalent vulnerability in current watermarking algorithms. The experimental results show SIRA achieves nearly 100% attack success rates on seven recent watermarking methods with only 0.88 USD per million tokens cost. Our approach does not require any access to the watermark algorithms or the watermarked LLM and can seamlessly transfer to any LLM as the attack model, even mobile-level models. Our findings highlight the urgent need for more robust watermarking.
<div id='section'>Paperid: <span id='pid'>799, <a href='https://arxiv.org/pdf/2504.10411.pdf' target='_blank'>https://arxiv.org/pdf/2504.10411.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hong Ding, Chia Chao Kang, SuYang Xi, Zehang Liu, Xuan Zhang, Yi Ding
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.10411">FPGA-Optimized Hardware Accelerator for Fast Fourier Transform and Singular Value Decomposition in AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This research introduces an FPGA-based hardware accelerator to optimize the Singular Value Decomposition (SVD) and Fast Fourier transform (FFT) operations in AI models. The proposed design aims to improve processing speed and reduce computational latency. Through experiments, we validate the performance benefits of the hardware accelerator and show how well it handles FFT and SVD operations. With its strong security and durability, the accelerator design achieves significant speedups over software implementations, thanks to its modules for data flow control, watermark embedding, FFT, and SVD.
<div id='section'>Paperid: <span id='pid'>800, <a href='https://arxiv.org/pdf/2503.23332.pdf' target='_blank'>https://arxiv.org/pdf/2503.23332.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wenhao Luo, Zhangyi Shen, Ye Yao, Feng Ding, Guopu Zhu, Weizhi Meng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.23332">TraceMark-LDM: Authenticatable Watermarking for Latent Diffusion Models via Binary-Guided Rearrangement</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image generation algorithms are increasingly integral to diverse aspects of human society, driven by their practical applications. However, insufficient oversight in artificial Intelligence generated content (AIGC) can facilitate the spread of malicious content and increase the risk of copyright infringement. Among the diverse range of image generation models, the Latent Diffusion Model (LDM) is currently the most widely used, dominating the majority of the Text-to-Image model market. Currently, most attribution methods for LDMs rely on directly embedding watermarks into the generated images or their intermediate noise, a practice that compromises both the quality and the robustness of the generated content. To address these limitations, we introduce TraceMark-LDM, an novel algorithm that integrates watermarking to attribute generated images while guaranteeing non-destructive performance. Unlike current methods, TraceMark-LDM leverages watermarks as guidance to rearrange random variables sampled from a Gaussian distribution. To mitigate potential deviations caused by inversion errors, the small absolute elements are grouped and rearranged. Additionally, we fine-tune the LDM encoder to enhance the robustness of the watermark. Experimental results show that images synthesized using TraceMark-LDM exhibit superior quality and attribution accuracy compared to state-of-the-art (SOTA) techniques. Notably, TraceMark-LDM demonstrates exceptional robustness against various common attack methods, consistently outperforming SOTA methods.
<div id='section'>Paperid: <span id='pid'>801, <a href='https://arxiv.org/pdf/2503.11195.pdf' target='_blank'>https://arxiv.org/pdf/2503.11195.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shree Singhi, Aayan Yadav, Aayush Gupta, Shariar Ebrahimi, Parisa Hassanizadeh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.11195">Provenance Detection for AI-Generated Images: Combining Perceptual Hashing, Homomorphic Encryption, and AI Detection Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As AI-generated sensitive images become more prevalent, identifying their source is crucial for distinguishing them from real images. Conventional image watermarking methods are vulnerable to common transformations like filters, lossy compression, and screenshots, often applied during social media sharing. Watermarks can also be faked or removed if models are open-sourced or leaked since images can be rewatermarked. We have developed a three-part framework for secure, transformation-resilient AI content provenance detection, to address these limitations. We develop an adversarially robust state-of-the-art perceptual hashing model, DinoHash, derived from DINOV2, which is robust to common transformations like filters, compression, and crops. Additionally, we integrate a Multi-Party Fully Homomorphic Encryption~(MP-FHE) scheme into our proposed framework to ensure the protection of both user queries and registry privacy. Furthermore, we improve previous work on AI-generated media detection. This approach is useful in cases where the content is absent from our registry. DinoHash significantly improves average bit accuracy by 12% over state-of-the-art watermarking and perceptual hashing methods while maintaining superior true positive rate (TPR) and false positive rate (FPR) tradeoffs across various transformations. Our AI-generated media detection results show a 25% improvement in classification accuracy on commonly used real-world AI image generators over existing algorithms. By combining perceptual hashing, MP-FHE, and an AI content detection model, our proposed framework provides better robustness and privacy compared to previous work.
<div id='section'>Paperid: <span id='pid'>802, <a href='https://arxiv.org/pdf/2502.10329.pdf' target='_blank'>https://arxiv.org/pdf/2502.10329.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingyuan Fei, Wenjie Hou, Xuan Hai, Xin Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.10329">VocalCrypt: Novel Active Defense Against Deepfake Voice Based on Masking Effect</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancements in AI voice cloning, fueled by machine learning, have significantly impacted text-to-speech (TTS) and voice conversion (VC) fields. While these developments have led to notable progress, they have also raised concerns about the misuse of AI VC technology, causing economic losses and negative public perceptions. To address this challenge, this study focuses on creating active defense mechanisms against AI VC systems.
  We propose a novel active defense method, VocalCrypt, which embeds pseudo-timbre (jamming information) based on SFS into audio segments that are imperceptible to the human ear, thereby forming systematic fragments to prevent voice cloning. This approach protects the voice without compromising its quality. In comparison to existing methods, such as adversarial noise incorporation, VocalCrypt significantly enhances robustness and real-time performance, achieving a 500\% increase in generation speed while maintaining interference effectiveness.
  Unlike audio watermarking techniques, which focus on post-detection, our method offers preemptive defense, reducing implementation costs and enhancing feasibility. Extensive experiments using the Zhvoice and VCTK Corpus datasets show that our AI-cloned speech defense system performs excellently in automatic speaker verification (ASV) tests while preserving the integrity of the protected audio.
<div id='section'>Paperid: <span id='pid'>803, <a href='https://arxiv.org/pdf/2501.12174.pdf' target='_blank'>https://arxiv.org/pdf/2501.12174.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuang Li, Qiuping Yi, Zongcheng Ji, Yijian Lu, Yanqi Li, Keyang Xiao, Hongliang Liang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.12174">BiMarker: Enhancing Text Watermark Detection for Large Language Models with Bipolar Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid growth of Large Language Models (LLMs) raises concerns about distinguishing AI-generated text from human content. Existing watermarking techniques, like \kgw, struggle with low watermark strength and stringent false-positive requirements. Our analysis reveals that current methods rely on coarse estimates of non-watermarked text, limiting watermark detectability. To address this, we propose Bipolar Watermark (\tool), which splits generated text into positive and negative poles, enhancing detection without requiring additional computational resources or knowledge of the prompt. Theoretical analysis and experimental results demonstrate \tool's effectiveness and compatibility with existing optimization techniques, providing a new optimization dimension for watermarking in LLM-generated content.
<div id='section'>Paperid: <span id='pid'>804, <a href='https://arxiv.org/pdf/2412.19834.pdf' target='_blank'>https://arxiv.org/pdf/2412.19834.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aryaman Shaan, Garvit Banga, Raghav Mantri
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19834">RoboSignature: Robust Signature and Watermarking on Network Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have enabled easy creation and generation of images of all kinds given a single prompt. However, this has also raised ethical concerns about what is an actual piece of content created by humans or cameras compared to model-generated content like images or videos. Watermarking data generated by modern generative models is a popular method to provide information on the source of the content. The goal is for all generated images to conceal an invisible watermark, allowing for future detection or identification. The Stable Signature finetunes the decoder of Latent Diffusion Models such that a unique watermark is rooted in any image produced by the decoder. In this paper, we present a novel adversarial fine-tuning attack that disrupts the model's ability to embed the intended watermark, exposing a significant vulnerability in existing watermarking methods. To address this, we further propose a tamper-resistant fine-tuning algorithm inspired by methods developed for large language models, tailored to the specific requirements of watermarking in LDMs. Our findings emphasize the importance of anticipating and defending against potential vulnerabilities in generative systems.
<div id='section'>Paperid: <span id='pid'>805, <a href='https://arxiv.org/pdf/2411.14798.pdf' target='_blank'>https://arxiv.org/pdf/2411.14798.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Shulin Lan, Kanlin Liu, Yazhou Zhao, Chen Yang, Yingchao Wang, Xingshan Yao, Liehuang Zhu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.14798">Facial Features Matter: a Dynamic Watermark based Proactive Deepfake Detection Approach</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Current passive deepfake face-swapping detection methods encounter significance bottlenecks in model generalization capabilities. Meanwhile, proactive detection methods often use fixed watermarks which lack a close relationship with the content they protect and are vulnerable to security risks. Dynamic watermarks based on facial features offer a promising solution, as these features provide unique identifiers. Therefore, this paper proposes a Facial Feature-based Proactive deepfake detection method (FaceProtect), which utilizes changes in facial characteristics during deepfake manipulation as a novel detection mechanism. We introduce a GAN-based One-way Dynamic Watermark Generating Mechanism (GODWGM) that uses 128-dimensional facial feature vectors as inputs. This method creates irreversible mappings from facial features to watermarks, enhancing protection against various reverse inference attacks. Additionally, we propose a Watermark-based Verification Strategy (WVS) that combines steganography with GODWGM, allowing simultaneous transmission of the benchmark watermark representing facial features within the image. Experimental results demonstrate that our proposed method maintains exceptional detection performance and exhibits high practicality on images altered by various deepfake techniques.
<div id='section'>Paperid: <span id='pid'>806, <a href='https://arxiv.org/pdf/2411.13047.pdf' target='_blank'>https://arxiv.org/pdf/2411.13047.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Satoru Koda, Ikuya Morikawa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.13047">Bounding-box Watermarking: Defense against Model Extraction Attacks on Object Detectors</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) deployed in a cloud often allow users to query models via the APIs. However, these APIs expose the models to model extraction attacks (MEAs). In this attack, the attacker attempts to duplicate the target model by abusing the responses from the API. Backdoor-based DNN watermarking is known as a promising defense against MEAs, wherein the defender injects a backdoor into extracted models via API responses. The backdoor is used as a watermark of the model; if a suspicious model has the watermark (i.e., backdoor), it is verified as an extracted model. This work focuses on object detection (OD) models. Existing backdoor attacks on OD models are not applicable for model watermarking as the defense against MEAs on a realistic threat model. Our proposed approach involves inserting a backdoor into extracted models via APIs by stealthily modifying the bounding-boxes (BBs) of objects detected in queries while keeping the OD capability. In our experiments on three OD datasets, the proposed approach succeeded in identifying the extracted models with 100% accuracy in a wide variety of experimental scenarios.
<div id='section'>Paperid: <span id='pid'>807, <a href='https://arxiv.org/pdf/2411.05277.pdf' target='_blank'>https://arxiv.org/pdf/2411.05277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Saksham Rastogi, Danish Pruthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.05277">Revisiting the Robustness of Watermarking to Paraphrasing Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Amidst rising concerns about the internet being proliferated with content generated from language models (LMs), watermarking is seen as a principled way to certify whether text was generated from a model. Many recent watermarking techniques slightly modify the output probabilities of LMs to embed a signal in the generated output that can later be detected. Since early proposals for text watermarking, questions about their robustness to paraphrasing have been prominently discussed. Lately, some techniques are deliberately designed and claimed to be robust to paraphrasing. However, such watermarking schemes do not adequately account for the ease with which they can be reverse-engineered. We show that with access to only a limited number of generations from a black-box watermarked model, we can drastically increase the effectiveness of paraphrasing attacks to evade watermark detection, thereby rendering the watermark ineffective.
<div id='section'>Paperid: <span id='pid'>808, <a href='https://arxiv.org/pdf/2410.20824.pdf' target='_blank'>https://arxiv.org/pdf/2410.20824.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiyang Guo, Ruizhe Li, Mude Hui, Hanzhong Guo, Chen Zhang, Chuangjian Cai, Le Wan, Shangfei Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20824">FreqMark: Invisible Image Watermarking via Frequency Based Optimization in Latent Space</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Invisible watermarking is essential for safeguarding digital content, enabling copyright protection and content authentication. However, existing watermarking methods fall short in robustness against regeneration attacks. In this paper, we propose a novel method called FreqMark that involves unconstrained optimization of the image latent frequency space obtained after VAE encoding. Specifically, FreqMark embeds the watermark by optimizing the latent frequency space of the images and then extracts the watermark through a pre-trained image encoder. This optimization allows a flexible trade-off between image quality with watermark robustness and effectively resists regeneration attacks. Experimental results demonstrate that FreqMark offers significant advantages in image quality and robustness, permits flexible selection of the encoding bit number, and achieves a bit accuracy exceeding 90% when encoding a 48-bit hidden message under various attack scenarios.
<div id='section'>Paperid: <span id='pid'>809, <a href='https://arxiv.org/pdf/2410.20247.pdf' target='_blank'>https://arxiv.org/pdf/2410.20247.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Irena Gao, Percy Liang, Carlos Guestrin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20247">Model Equality Testing: Which Model Is This API Serving?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Users often interact with large language models through black-box inference APIs, both for closed- and open-weight models (e.g., Llama models are popularly accessed via Amazon Bedrock and Azure AI Studio). In order to cut costs or add functionality, API providers may quantize, watermark, or finetune the underlying model, changing the output distribution -- possibly without notifying users. We formalize detecting such distortions as Model Equality Testing, a two-sample testing problem, where the user collects samples from the API and a reference distribution and conducts a statistical test to see if the two distributions are the same. We find that tests based on the Maximum Mean Discrepancy between distributions are powerful for this task: a test built on a simple string kernel achieves a median of 77.4% power against a range of distortions, using an average of just 10 samples per prompt. We then apply this test to commercial inference APIs from Summer 2024 for four Llama models, finding that 11 out of 31 endpoints serve different distributions than reference weights released by Meta.
<div id='section'>Paperid: <span id='pid'>810, <a href='https://arxiv.org/pdf/2410.10712.pdf' target='_blank'>https://arxiv.org/pdf/2410.10712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jiahui Liu, Mark Zhandry
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10712">Composability in Watermarking Schemes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Software watermarking allows for embedding a mark into a piece of code, such that any attempt to remove the mark will render the code useless. Provably secure watermarking schemes currently seems limited to programs computing various cryptographic operations, such as evaluating pseudorandom functions (PRFs), signing messages, or decrypting ciphertexts (the latter often going by the name ``traitor tracing''). Moreover, each of these watermarking schemes has an ad-hoc construction of its own.
  We observe, however, that many cryptographic objects are used as building blocks in larger protocols. We ask: just as we can compose building blocks to obtain larger protocols, can we compose watermarking schemes for the building blocks to obtain watermarking schemes for the larger protocols? We give an affirmative answer to this question, by precisely formulating a set of requirements that allow for composing watermarking schemes. We use our formulation to derive a number of applications.
<div id='section'>Paperid: <span id='pid'>811, <a href='https://arxiv.org/pdf/2410.01906.pdf' target='_blank'>https://arxiv.org/pdf/2410.01906.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aakash Varma Nadimpalli, Ajita Rattani
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.01906">Social Media Authentication and Combating Deepfakes using Semi-fragile Invisible Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the significant advances in deep generative models for image and video synthesis, Deepfakes and manipulated media have raised severe societal concerns. Conventional machine learning classifiers for deepfake detection often fail to cope with evolving deepfake generation technology and are susceptible to adversarial attacks. Alternatively, invisible image watermarking is being researched as a proactive defense technique that allows media authentication by verifying an invisible secret message embedded in the image pixels. A handful of invisible image watermarking techniques introduced for media authentication have proven vulnerable to basic image processing operations and watermark removal attacks. In response, we have proposed a semi-fragile image watermarking technique that embeds an invisible secret message into real images for media authentication. Our proposed watermarking framework is designed to be fragile to facial manipulations or tampering while being robust to benign image-processing operations and watermark removal attacks. This is facilitated through a unique architecture of our proposed technique consisting of critic and adversarial networks that enforce high image quality and resiliency to watermark removal efforts, respectively, along with the backbone encoder-decoder and the discriminator networks. Thorough experimental investigations on SOTA facial Deepfake datasets demonstrate that our proposed model can embed a $64$-bit secret as an imperceptible image watermark that can be recovered with a high-bit recovery accuracy when benign image processing operations are applied while being non-recoverable when unseen Deepfake manipulations are applied. In addition, our proposed watermarking technique demonstrates high resilience to several white-box and black-box watermark removal attacks. Thus, obtaining state-of-the-art performance.
<div id='section'>Paperid: <span id='pid'>812, <a href='https://arxiv.org/pdf/2409.04459.pdf' target='_blank'>https://arxiv.org/pdf/2409.04459.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anudeex Shetty, Qiongkai Xu, Jey Han Lau
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.04459">WET: Overcoming Paraphrasing Vulnerabilities in Embeddings-as-a-Service with Linear Transformation Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embeddings-as-a-Service (EaaS) is a service offered by large language model (LLM) developers to supply embeddings generated by LLMs. Previous research suggests that EaaS is prone to imitation attacks -- attacks that clone the underlying EaaS model by training another model on the queried embeddings. As a result, EaaS watermarks are introduced to protect the intellectual property of EaaS providers. In this paper, we first show that existing EaaS watermarks can be removed by paraphrasing when attackers clone the model. Subsequently, we propose a novel watermarking technique that involves linearly transforming the embeddings, and show that it is empirically and theoretically robust against paraphrasing.
<div id='section'>Paperid: <span id='pid'>813, <a href='https://arxiv.org/pdf/2407.02111.pdf' target='_blank'>https://arxiv.org/pdf/2407.02111.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Rodriguez-Lois, Fernando Perez-Gonzalez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2407.02111">Exploring Federated Learning Dynamics for Black-and-White-Box DNN Traitor Tracing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As deep learning applications become more prevalent, the need for extensive training examples raises concerns for sensitive, personal, or proprietary data. To overcome this, Federated Learning (FL) enables collaborative model training across distributed data-owners, but it introduces challenges in safeguarding model ownership and identifying the origin in case of a leak. Building upon prior work, this paper explores the adaptation of black-and-white traitor tracing watermarking to FL classifiers, addressing the threat of collusion attacks from different data-owners. This study reveals that leak-resistant white-box fingerprints can be directly implemented without a significant impact from FL dynamics, while the black-box fingerprints are drastically affected, losing their traitor tracing capabilities. To mitigate this effect, we propose increasing the number of black-box salient neurons through dropout regularization. Though there are still some open problems to be explored, such as analyzing non-i.i.d. datasets and over-parameterized models, results show that collusion-resistant traitor tracing, identifying all data-owners involved in a suspected leak, is feasible in an FL framework, even in early stages of training.
<div id='section'>Paperid: <span id='pid'>814, <a href='https://arxiv.org/pdf/2406.15494.pdf' target='_blank'>https://arxiv.org/pdf/2406.15494.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mehmet Yildirim, Nasir Kenarangui, Robert Balog, Laszlo B. Kish, Chanan Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.15494">Simple Cracking of (Noise-Based) Dynamic Watermarking in Smart Grids</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Previous research employing a conceptual approach with a digital twin has demonstrated that (noise-based) dynamic watermarking is incapable of providing unconditional security in smart electrical grid systems. However, the implementation of digital twins can be prohibitively costly or infeasible due to limited available data on critical infrastructure. In this study, we first analyze the spectral properties of dynamic watermarking and its associated protocol. Subsequently, we present a straightforward attack inspired by the digital twin method, which extracts and utilizes the grid noises and completely breaches the security of dynamic watermarking without requiring knowledge of the private watermarking signal. The attacker can fully expose the grid while evading detection by the controller. Our findings indicate that in the absence of secure and authenticated communications, dynamic watermarking offers neither conditional nor unconditional security. Conversely, when communication lines, sensors, and communicators are equipped with tamper-resistant and secure/authenticated links, dynamic watermarking becomes redundant for grid security.
<div id='section'>Paperid: <span id='pid'>815, <a href='https://arxiv.org/pdf/2404.16849.pdf' target='_blank'>https://arxiv.org/pdf/2404.16849.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kate Davis, Laszlo B. Kish, Chanan Singh
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.16849">Smart Grids Secured By Dynamic Watermarking: How Secure?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Unconditional security for smart grids is defined. Cryptanalyses of the watermarked security of smart grids indicate that watermarking cannot guarantee unconditional security unless the communication within the grid system is unconditionally secure. The successful attack against the dynamically watermarked smart grid remains valid even with the presence of internal noise from the grid. An open question arises: if unconditionally authenticated secure communications within the grid, together with tamper resistance of the critical elements, are satisfactory conditions to provide unconditional security for the grid operation.
<div id='section'>Paperid: <span id='pid'>816, <a href='https://arxiv.org/pdf/2404.13134.pdf' target='_blank'>https://arxiv.org/pdf/2404.13134.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bishwa Karki, Chun-Hua Tsai, Pei-Chi Huang, Xin Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.13134">Deep Learning-based Text-in-Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this work, we introduce a novel deep learning-based approach to text-in-image watermarking, a method that embeds and extracts textual information within images to enhance data security and integrity. Leveraging the capabilities of deep learning, specifically through the use of Transformer-based architectures for text processing and Vision Transformers for image feature extraction, our method sets new benchmarks in the domain. The proposed method represents the first application of deep learning in text-in-image watermarking that improves adaptivity, allowing the model to intelligently adjust to specific image characteristics and emerging threats. Through testing and evaluation, our method has demonstrated superior robustness compared to traditional watermarking techniques, achieving enhanced imperceptibility that ensures the watermark remains undetectable across various image contents.
<div id='section'>Paperid: <span id='pid'>817, <a href='https://arxiv.org/pdf/2404.07572.pdf' target='_blank'>https://arxiv.org/pdf/2404.07572.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>ZhenZhe Gao, Zhenjun Tang, Zhaoxia Yin, Baoyuan Wu, Yue Lu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.07572">Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Neural networks have increasingly influenced people's lives. Ensuring the faithful deployment of neural networks as designed by their model owners is crucial, as they may be susceptible to various malicious or unintentional modifications, such as backdooring and poisoning attacks. Fragile model watermarks aim to prevent unexpected tampering that could lead DNN models to make incorrect decisions. They ensure the detection of any tampering with the model as sensitively as possible.However, prior watermarking methods suffered from inefficient sample generation and insufficient sensitivity, limiting their practical applicability. Our approach employs a sample-pairing technique, placing the model boundaries between pairs of samples, while simultaneously maximizing logits. This ensures that the model's decision results of sensitive samples change as much as possible and the Top-1 labels easily alter regardless of the direction it moves.
<div id='section'>Paperid: <span id='pid'>818, <a href='https://arxiv.org/pdf/2403.01472.pdf' target='_blank'>https://arxiv.org/pdf/2403.01472.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anudeex Shetty, Yue Teng, Ke He, Qiongkai Xu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.01472">WARDEN: Multi-Directional Backdoor Watermarks for Embedding-as-a-Service Copyright Protection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Embedding as a Service (EaaS) has become a widely adopted solution, which offers feature extraction capabilities for addressing various downstream tasks in Natural Language Processing (NLP). Prior studies have shown that EaaS can be prone to model extraction attacks; nevertheless, this concern could be mitigated by adding backdoor watermarks to the text embeddings and subsequently verifying the attack models post-publication. Through the analysis of the recent watermarking strategy for EaaS, EmbMarker, we design a novel CSE (Clustering, Selection, Elimination) attack that removes the backdoor watermark while maintaining the high utility of embeddings, indicating that the previous watermarking approach can be breached. In response to this new threat, we propose a new protocol to make the removal of watermarks more challenging by incorporating multiple possible watermark directions. Our defense approach, WARDEN, notably increases the stealthiness of watermarks and has been empirically shown to be effective against CSE attack.
<div id='section'>Paperid: <span id='pid'>819, <a href='https://arxiv.org/pdf/2402.16578.pdf' target='_blank'>https://arxiv.org/pdf/2402.16578.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Massieh Kordi Boroujeny, Ya Jiang, Kai Zeng, Brian Mark
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.16578">Multi-Bit Distortion-Free Watermarking for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Methods for watermarking large language models have been proposed that distinguish AI-generated text from human-generated text by slightly altering the model output distribution, but they also distort the quality of the text, exposing the watermark to adversarial detection. More recently, distortion-free watermarking methods were proposed that require a secret key to detect the watermark. The prior methods generally embed zero-bit watermarks that do not provide additional information beyond tagging a text as being AI-generated. We extend an existing zero-bit distortion-free watermarking method by embedding multiple bits of meta-information as part of the watermark. We also develop a computationally efficient decoder that extracts the embedded information from the watermark with low bit error rate.
<div id='section'>Paperid: <span id='pid'>820, <a href='https://arxiv.org/pdf/2401.05167.pdf' target='_blank'>https://arxiv.org/pdf/2401.05167.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mateusz KrubiÅski, Stefan Matcovici, Diana Grigore, Daniel Voinea, Alin-Ionut Popa
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.05167">Watermark Text Pattern Spotting in Document Images</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermark text spotting in document images can offer access to an often unexplored source of information, providing crucial evidence about a record's scope, audience and sometimes even authenticity. Stemming from the problem of text spotting, detecting and understanding watermarks in documents inherits the same hardships - in the wild, writing can come in various fonts, sizes and forms, making generic recognition a very difficult problem. To address the lack of resources in this field and propel further research, we propose a novel benchmark (K-Watermark) containing 65,447 data samples generated using Wrender, a watermark text patterns rendering procedure. A validity study using humans raters yields an authenticity score of 0.51 against pre-generated watermarked documents. To prove the usefulness of the dataset and rendering technique, we developed an end-to-end solution (Wextract) for detecting the bounding box instances of watermark text, while predicting the depicted text. To deal with this specific task, we introduce a variance minimization loss and a hierarchical self-attention mechanism. To the best of our knowledge, we are the first to propose an evaluation benchmark and a complete solution for retrieving watermarks from documents surpassing baselines by 5 AP points in detection and 4 points in character accuracy.
<div id='section'>Paperid: <span id='pid'>821, <a href='https://arxiv.org/pdf/2312.16547.pdf' target='_blank'>https://arxiv.org/pdf/2312.16547.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>DevriÅ Ä°Åler, Elisa Cabana, Alvaro Garcia-Recuero, Georgia Koutrika, Nikolaos Laoutaris
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.16547">FreqyWM: Frequency Watermarking for the New Data Economy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel technique for modulating the appearance frequency of a few tokens within a dataset for encoding an invisible watermark that can be used to protect ownership rights upon data. We develop optimal as well as fast heuristic algorithms for creating and verifying such watermarks. We also demonstrate the robustness of our technique against various attacks and derive analytical bounds for the false positive probability of erroneously detecting a watermark on a dataset that does not carry it. Our technique is applicable to both single dimensional and multidimensional datasets, is independent of token type, allows for a fine control of the introduced distortion, and can be used in a variety of use cases that involve buying and selling data in contemporary data marketplaces.
<div id='section'>Paperid: <span id='pid'>822, <a href='https://arxiv.org/pdf/2311.07700.pdf' target='_blank'>https://arxiv.org/pdf/2311.07700.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhen Guo, Shangdi Yu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.07700">AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have opened up enormous opportunities while simultaneously posing ethical dilemmas. One of the major concerns is their ability to create text that closely mimics human writing, which can lead to potential misuse, such as academic misconduct, disinformation, and fraud. To address this problem, we present AuthentiGPT, an efficient classifier that distinguishes between machine-generated and human-written texts. Under the assumption that human-written text resides outside the distribution of machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input text with artificially added noise, and then semantically compares the denoised text with the original to determine if the content is machine-generated. With only one trainable parameter, AuthentiGPT eliminates the need for a large training dataset, watermarking the LLM's output, or computing the log-likelihood. Importantly, the detection capability of AuthentiGPT can be easily adapted to any generative language model. With a 0.918 AUROC score on a domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other commercial algorithms, highlighting its potential for detecting machine-generated text in academic settings.
<div id='section'>Paperid: <span id='pid'>823, <a href='https://arxiv.org/pdf/2310.05395.pdf' target='_blank'>https://arxiv.org/pdf/2310.05395.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Agnibh Dasgupta, Xin Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2310.05395">Robust Image Watermarking based on Cross-Attention and Invariant Domain Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image watermarking involves embedding and extracting watermarks within a cover image, with deep learning approaches emerging to bolster generalization and robustness. Predominantly, current methods employ convolution and concatenation for watermark embedding, while also integrating conceivable augmentation in the training process. This paper explores a robust image watermarking methodology by harnessing cross-attention and invariant domain learning, marking two novel, significant advancements. First, we design a watermark embedding technique utilizing a multi-head cross attention mechanism, enabling information exchange between the cover image and watermark to identify semantically suitable embedding locations. Second, we advocate for learning an invariant domain representation that encapsulates both semantic and noise-invariant information concerning the watermark, shedding light on promising avenues for enhancing image watermarking techniques.
<div id='section'>Paperid: <span id='pid'>824, <a href='https://arxiv.org/pdf/2308.11123.pdf' target='_blank'>https://arxiv.org/pdf/2308.11123.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Luke Ditria, Tom Drummond
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.11123">Hey That's Mine Imperceptible Watermarks are Preserved in Diffusion Generated Outputs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Generative models have seen an explosion in popularity with the release of huge generative Diffusion models like Midjourney and Stable Diffusion to the public. Because of this new ease of access, questions surrounding the automated collection of data and issues regarding content ownership have started to build. In this paper we present new work which aims to provide ways of protecting content when shared to the public. We show that a generative Diffusion model trained on data that has been imperceptibly watermarked will generate new images with these watermarks present. We further show that if a given watermark is correlated with a certain feature of the training data, the generated images will also have this correlation. Using statistical tests we show that we are able to determine whether a model has been trained on marked data, and what data was marked. As a result our system offers a solution to protect intellectual property when sharing content online.
<div id='section'>Paperid: <span id='pid'>825, <a href='https://arxiv.org/pdf/2307.06695.pdf' target='_blank'>https://arxiv.org/pdf/2307.06695.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Elena Rodriguez-Lois, Fernando Perez-Gonzalez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2307.06695">Towards Traitor Tracing in Black-and-White-Box DNN Watermarking with Tardos-based Codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing popularity of Deep Neural Networks, which often require computationally expensive training and access to a vast amount of data, calls for accurate authorship verification methods to deter unlawful dissemination of the models and identify the source of the leak. In DNN watermarking the owner may have access to the full network (white-box) or only be able to extract information from its output to queries (black-box), but a watermarked model may include both approaches in order to gather sufficient evidence to then gain access to the network. Although there has been limited research in white-box watermarking that considers traitor tracing, this problem is yet to be explored in the black-box scenario. In this paper, we propose a black-and-white-box watermarking method for DNN classifiers that opens the door to collusion-resistant traitor tracing in black-box, exploiting the properties of Tardos codes, and making it possible to identify the source of the leak before access to the model is granted. While experimental results show that the method can successfully identify traitors, even when further attacks have been performed, we also discuss its limitations and open problems for traitor tracing in black-box.
<div id='section'>Paperid: <span id='pid'>826, <a href='https://arxiv.org/pdf/2306.15896.pdf' target='_blank'>https://arxiv.org/pdf/2306.15896.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junlong Mao, Huiyi Tang, Shanxiang Lyu, Zhengchun Zhou, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.15896">Content-Aware Quantization Index Modulation:Leveraging Data Statistics for Enhanced Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Image watermarking techniques have continuously evolved to address new challenges and incorporate advanced features. The advent of data-driven approaches has enabled the processing and analysis of large volumes of data, extracting valuable insights and patterns. In this paper, we propose two content-aware quantization index modulation (QIM) algorithms: Content-Aware QIM (CA-QIM) and Content-Aware Minimum Distortion QIM (CAMD-QIM). These algorithms aim to improve the embedding distortion of QIM-based watermarking schemes by considering the statistics of the cover signal vectors and messages. CA-QIM introduces a canonical labeling approach, where the closest coset to each cover vector is determined during the embedding process. An adjacency matrix is constructed to capture the relationships between the cover vectors and messages. CAMD-QIM extends the concept of minimum distortion (MD) principle to content-aware QIM. Instead of quantizing the carriers to lattice points, CAMD-QIM quantizes them to close points in the correct decoding region. Canonical labeling is also employed in CAMD-QIM to enhance its performance. Simulation results demonstrate the effectiveness of CA-QIM and CAMD-QIM in reducing embedding distortion compared to traditional QIM. The combination of canonical labeling and the minimum distortion principle proves to be powerful, minimizing the need for changes to most cover vectors/carriers. These content-aware QIM algorithms provide improved performance and robustness for watermarking applications.
<div id='section'>Paperid: <span id='pid'>827, <a href='https://arxiv.org/pdf/2306.01356.pdf' target='_blank'>https://arxiv.org/pdf/2306.01356.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junchuan Liang, Rong Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.01356">FedCIP: Federated Client Intellectual Property Protection with Traitor Tracking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Federated learning is an emerging privacy-preserving distributed machine learning that enables multiple parties to collaboratively learn a shared model while keeping each party's data private. However, federated learning faces two main problems: semi-honest server privacy inference attacks and malicious client-side model theft. To address privacy inference attacks, parameter-based encrypted federated learning secure aggregation can be used. To address model theft, a watermark-based intellectual property protection scheme can verify model ownership. Although watermark-based intellectual property protection schemes can help verify model ownership, they are not sufficient to address the issue of continuous model theft by uncaught malicious clients in federated learning. Existing IP protection schemes that have the ability to track traitors are also not compatible with federated learning security aggregation. Thus, in this paper, we propose a Federated Client-side Intellectual Property Protection (FedCIP), which is compatible with federated learning security aggregation and has the ability to track traitors. To the best of our knowledge, this is the first IP protection scheme in federated learning that is compatible with secure aggregation and tracking capabilities.
<div id='section'>Paperid: <span id='pid'>828, <a href='https://arxiv.org/pdf/2305.18456.pdf' target='_blank'>https://arxiv.org/pdf/2305.18456.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Leonard Tang, Gavin Uberti, Tom Shlomi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.18456">Baselines for Identifying Watermarked Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We consider the emerging problem of identifying the presence and use of watermarking schemes in widely used, publicly hosted, closed source large language models (LLMs). We introduce a suite of baseline algorithms for identifying watermarks in LLMs that rely on analyzing distributions of output tokens and logits generated by watermarked and unmarked LLMs. Notably, watermarked LLMs tend to produce distributions that diverge qualitatively and identifiably from standard models. Furthermore, we investigate the identifiability of watermarks at varying strengths and consider the tradeoffs of each of our identification mechanisms with respect to watermarking scenario. Along the way, we formalize the specific problem of identifying watermarks in LLMs, as well as LLM watermarks and watermark detection in general, providing a framework and foundations for studying them.
<div id='section'>Paperid: <span id='pid'>829, <a href='https://arxiv.org/pdf/2305.17879.pdf' target='_blank'>https://arxiv.org/pdf/2305.17879.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junren Qin, Shanxiang Lyu, Fan Yang, Jiarui Deng, Zhihua Xia, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.17879">Reversible Quantization Index Modulation for Static Deep Neural Network Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Static deep neural network (DNN) watermarking techniques typically employ irreversible methods to embed watermarks into the DNN model weights. However, this approach causes permanent damage to the watermarked model and fails to meet the requirements of integrity authentication. Reversible data hiding (RDH) methods offer a potential solution, but existing approaches suffer from weaknesses in terms of usability, capacity, and fidelity, hindering their practical adoption. In this paper, we propose a novel RDH-based static DNN watermarking scheme using quantization index modulation (QIM). Our scheme incorporates a novel approach based on a one-dimensional quantizer for watermark embedding. Furthermore, we design two schemes to address the challenges of integrity protection and legitimate authentication for DNNs. Through simulation results on training loss and classification accuracy, we demonstrate the feasibility and effectiveness of our proposed schemes, highlighting their superior adaptability compared to existing methods.
<div id='section'>Paperid: <span id='pid'>830, <a href='https://arxiv.org/pdf/2305.00266.pdf' target='_blank'>https://arxiv.org/pdf/2305.00266.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Omair Faraj, David MegÃ­as, Joaquin Garcia-Alfaro
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.00266">ZIRCON: Zero-watermarking-based approach for data integrity and secure provenance in IoT networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The Internet of Things (IoT) is integrating the Internet and smart devices in almost every domain such as home automation, e-healthcare systems, vehicular networks, industrial control and military applications. In these sectors, sensory data, which is collected from multiple sources and managed through intermediate processing by multiple nodes, is used for decision-making processes. Ensuring data integrity and keeping track of data provenance is a core requirement in such a highly dynamic context, since data provenance is an important tool for the assurance of data trustworthiness. Dealing with such requirements is challenging due to the limited computational and energy resources in IoT networks. This requires addressing several challenges such as processing overhead, secure provenance, bandwidth consumption and storage efficiency. In this paper, we propose ZIRCON, a novel zero-watermarking approach to establish end-to-end data trustworthiness in an IoT network. In ZIRCON, provenance information is stored in a tamper-proof centralized network database through watermarks, generated at source node before transmission. We provide an extensive security analysis showing the resilience of our scheme against passive and active attacks. We also compare our scheme with existing works based on performance metrics such as computational time, energy utilization and cost analysis. The results show that ZIRCON is robust against several attacks, lightweight, storage efficient, and better in energy utilization and bandwidth consumption, compared to prior art.
<div id='section'>Paperid: <span id='pid'>831, <a href='https://arxiv.org/pdf/2303.13733.pdf' target='_blank'>https://arxiv.org/pdf/2303.13733.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Taeyoung Kim, Yunhee Jang, Chanjong Lee, Hyungjoon Koo, Hyoungshick Kim
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.13733">SmartMark: Software Watermarking Scheme for Smart Contracts</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Smart contracts are self-executing programs on a blockchain to ensure immutable and transparent agreements without the involvement of intermediaries. Despite the growing popularity of smart contracts for many blockchain platforms like Ethereum, smart contract developers cannot prevent copying their smart contracts from competitors due to the absence of technical means available. However, applying existing software watermarking techniques is challenging because of the unique properties of smart contracts, such as a code size constraint, non-free execution cost, and no support for dynamic allocation under a virtual machine environment. This paper introduces a novel software watermarking scheme, dubbed SmartMark, aiming to protect the piracy of smart contracts. SmartMark builds the control flow graph of a target contract runtime bytecode and locates a series of bytes randomly selected from a collection of opcodes to represent a watermark. We implement a full-fledged prototype for Ethereum, applying SmartMark to 27,824 unique smart contract bytecodes. Our empirical results demonstrate that SmartMark can effectively embed a watermark into smart contracts and verify its presence, meeting the requirements of credibility and imperceptibility while incurring a slight performance degradation. Furthermore, our security analysis shows that SmartMark is resilient against foreseeable watermarking corruption attacks; e.g., a large number of dummy opcodes are needed to disable a watermark effectively, resulting in producing illegitimate smart contract clones that are not economical.
<div id='section'>Paperid: <span id='pid'>832, <a href='https://arxiv.org/pdf/2208.03948.pdf' target='_blank'>https://arxiv.org/pdf/2208.03948.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tianxing Zhang, Hanzhou Wu, Xiaofeng Lu, Guangling Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2208.03948">AWEncoder: Adversarial Watermarking Pre-trained Encoders in Contrastive Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As a self-supervised learning paradigm, contrastive learning has been widely used to pre-train a powerful encoder as an effective feature extractor for various downstream tasks. This process requires numerous unlabeled training data and computational resources, which makes the pre-trained encoder become valuable intellectual property of the owner. However, the lack of a priori knowledge of downstream tasks makes it non-trivial to protect the intellectual property of the pre-trained encoder by applying conventional watermarking methods. To deal with this problem, in this paper, we introduce AWEncoder, an adversarial method for watermarking the pre-trained encoder in contrastive learning. First, as an adversarial perturbation, the watermark is generated by enforcing the training samples to be marked to deviate respective location and surround a randomly selected key image in the embedding space. Then, the watermark is embedded into the pre-trained encoder by further optimizing a joint loss function. As a result, the watermarked encoder not only performs very well for downstream tasks, but also enables us to verify its ownership by analyzing the discrepancy of output provided using the encoder as the backbone under both white-box and black-box conditions. Extensive experiments demonstrate that the proposed work enjoys pretty good effectiveness and robustness on different contrastive learning algorithms and downstream tasks, which has verified the superiority and applicability of the proposed work.
<div id='section'>Paperid: <span id='pid'>833, <a href='https://arxiv.org/pdf/2206.02541.pdf' target='_blank'>https://arxiv.org/pdf/2206.02541.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xuefeng Fan, Dahao Fu, Hangyu Gui, Xinpeng Zhang, Xiaoyi Zhou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2206.02541">PCPT and ACPT: Copyright Protection and Traceability Scheme for DNN Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep neural networks (DNNs) have achieved tremendous success in artificial intelligence (AI) fields. However, DNN models can be easily illegally copied, redistributed, or abused by criminals, seriously damaging the interests of model inventors. The copyright protection of DNN models by neural network watermarking has been studied, but the establishment of a traceability mechanism for determining the authorized users of a leaked model is a new problem driven by the demand for AI services. Because the existing traceability mechanisms are used for models without watermarks, a small number of false-positives are generated. Existing black-box active protection schemes have loose authorization control and are vulnerable to forgery attacks. Therefore, based on the idea of black-box neural network watermarking with the video framing and image perceptual hash algorithm, a passive copyright protection and traceability framework PCPT is proposed that uses an additional class of DNN models, improving the existing traceability mechanism that yields a small number of false-positives. Based on an authorization control strategy and image perceptual hash algorithm, a DNN model active copyright protection and traceability framework ACPT is proposed. This framework uses the authorization control center constructed by the detector and verifier. This approach realizes stricter authorization control, which establishes a strong connection between users and model owners, improves the framework security, and supports traceability verification.
<div id='section'>Paperid: <span id='pid'>834, <a href='https://arxiv.org/pdf/2006.03903.pdf' target='_blank'>https://arxiv.org/pdf/2006.03903.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Flavio Bertini, Rajesh Sharma, Danilo Montesi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2006.03903">Are Social Networks Watermarking Us or Are We (Unawarely) Watermarking Ourself?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the last decade, Social Networks (SNs) have deeply changed many aspects of society, and one of the most widespread behaviours is the sharing of pictures. However, malicious users often exploit shared pictures to create fake profiles leading to the growth of cybercrime. Thus, keeping in mind this scenario, authorship attribution and verification through image watermarking techniques are becoming more and more important. In this paper, firstly, we investigate how 13 most popular SNs treat the uploaded pictures, in order to identify a possible implementation of image watermarking techniques by respective SNs. Secondly, on these 13 SNs, we test the robustness of several image watermarking algorithms. Finally, we verify whether a method based on the Photo-Response Non-Uniformity (PRNU) technique can be successfully used as a watermarking approach for authorship attribution and verification of pictures on SNs. The proposed method is robust enough in spite of the fact that the pictures get downgraded during the uploading process by SNs. The results of our analysis on a real dataset of 8,400 pictures show that the proposed method is more effective than other watermarking techniques and can help to address serious questions about privacy and security on SNs.
<div id='section'>Paperid: <span id='pid'>835, <a href='https://arxiv.org/pdf/2512.19378.pdf' target='_blank'>https://arxiv.org/pdf/2512.19378.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhiqing Hu, Chenxu Zhao, Jiazhong Lu, Xiaolei Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.19378">HATS: High-Accuracy Triple-Set Watermarking for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.
<div id='section'>Paperid: <span id='pid'>836, <a href='https://arxiv.org/pdf/2512.18791.pdf' target='_blank'>https://arxiv.org/pdf/2512.18791.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yichuan Zhang, Chengxin Li, Yujie Gu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.18791">Smark: A Watermark for Text-to-Speech Diffusion Models via Discrete Wavelet Transform</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text-to-Speech (TTS) diffusion models generate high-quality speech, which raises challenges for the model intellectual property protection and speech tracing for legal use. Audio watermarking is a promising solution. However, due to the structural differences among various TTS diffusion models, existing watermarking methods are often designed for a specific model and degrade audio quality, which limits their practical applicability. To address this dilemma, this paper proposes a universal watermarking scheme for TTS diffusion models, termed Smark. This is achieved by designing a lightweight watermark embedding framework that operates in the common reverse diffusion paradigm shared by all TTS diffusion models. To mitigate the impact on audio quality, Smark utilizes the discrete wavelet transform (DWT) to embed watermarks into the relatively stable low-frequency regions of the audio, which ensures seamless watermark-audio integration and is resistant to removal during the reverse diffusion process. Extensive experiments are conducted to evaluate the audio quality and watermark performance in various simulated real-world attack scenarios. The experimental results show that Smark achieves superior performance in both audio quality and watermark extraction accuracy.
<div id='section'>Paperid: <span id='pid'>837, <a href='https://arxiv.org/pdf/2512.14753.pdf' target='_blank'>https://arxiv.org/pdf/2512.14753.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Li Lin, Siyuan Xin, Yang Cao, Xiaochun Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.14753">CODE ACROSTIC: Robust Watermarking for Code Generation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking large language models (LLMs) is vital for preventing their misuse, including the fabrication of fake news, plagiarism, and spam. It is especially important to watermark LLM-generated code, as it often contains intellectual property.However, we found that existing methods for watermarking LLM-generated code fail to address comment removal attack.In such cases, an attacker can simply remove the comments from the generated code without affecting its functionality, significantly reducing the effectiveness of current code-watermarking techniques.On the other hand, injecting a watermark into code is challenging because, as previous works have noted, most code represents a low-entropy scenario compared to natural language. Our approach to addressing this issue involves leveraging prior knowledge to distinguish between low-entropy and high-entropy parts of the code, as indicated by a Cue List of words.We then inject the watermark guided by this Cue List, achieving higher detectability and usability than existing methods.We evaluated our proposed method on HumanEvaland compared our method with three state-of-the-art code watermarking techniques. The results demonstrate the effectiveness of our approach.
<div id='section'>Paperid: <span id='pid'>838, <a href='https://arxiv.org/pdf/2512.10248.pdf' target='_blank'>https://arxiv.org/pdf/2512.10248.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zhuo Wang, Xiliang Liu, Ligang Sun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.10248">RobustSora: De-Watermarked Benchmark for Robust AI-Generated Video Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of AI-generated video technologies poses challenges to information integrity. While recent benchmarks advance AIGC video detection, they overlook a critical factor: many state-of-the-art generative models embed digital watermarks in outputs, and detectors may partially rely on these patterns. To evaluate this influence, we present RobustSora, the benchmark designed to assess watermark robustness in AIGC video detection. We systematically construct a dataset of 6,500 videos comprising four types: Authentic-Clean (A-C), Authentic-Spoofed with fake watermarks (A-S), Generated-Watermarked (G-W), and Generated-DeWatermarked (G-DeW). Our benchmark introduces two evaluation tasks: Task-I tests performance on watermark-removed AI videos, while Task-II assesses false alarm rates on authentic videos with fake watermarks. Experiments with ten models spanning specialized AIGC detectors, transformer architectures, and MLLM approaches reveal performance variations of 2-8pp under watermark manipulation. Transformer-based models show consistent moderate dependency (6-8pp), while MLLMs exhibit diverse patterns (2-8pp). These findings indicate partial watermark dependency and highlight the need for watermark-aware training strategies. RobustSora provides essential tools to advance robust AIGC detection research.
<div id='section'>Paperid: <span id='pid'>839, <a href='https://arxiv.org/pdf/2512.07038.pdf' target='_blank'>https://arxiv.org/pdf/2512.07038.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Min Jae Song, Kameron Shahabi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.07038">Ideal Attribution and Faithful Watermarks for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.
<div id='section'>Paperid: <span id='pid'>840, <a href='https://arxiv.org/pdf/2512.04044.pdf' target='_blank'>https://arxiv.org/pdf/2512.04044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yizhou Zhao, Zhiwei Steven Wu, Adam Block
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.04044">MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.
<div id='section'>Paperid: <span id='pid'>841, <a href='https://arxiv.org/pdf/2511.13722.pdf' target='_blank'>https://arxiv.org/pdf/2511.13722.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>William Guo, Adaku Uchendu, Ana Smith
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.13722">Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\to$ another language $\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.
<div id='section'>Paperid: <span id='pid'>842, <a href='https://arxiv.org/pdf/2510.27307.pdf' target='_blank'>https://arxiv.org/pdf/2510.27307.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mingcui Zhang, Zhigang Jia
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.27307">A fragile zero-watermarking method based on dual quaternion matrix decomposition</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical images play a crucial role in assisting diagnosis, remote consultation, and academic research. However, during the transmission and sharing process, they face serious risks of copyright ownership and content tampering. Therefore, protecting medical images is of great importance. As an effective means of image copyright protection, zero-watermarking technology focuses on constructing watermarks without modifying the original carrier by extracting its stable features, which provides an ideal approach for protecting medical images. This paper aims to propose a fragile zero-watermarking model based on dual quaternion matrix decomposition, which utilizes the operational relationship between the standard part and the dual part of dual quaternions to correlate the original carrier image with the watermark image, and generates zero-watermarking information based on the characteristics of dual quaternion matrix decomposition, ultimately achieving copyright protection and content tampering detection for medical images.
<div id='section'>Paperid: <span id='pid'>843, <a href='https://arxiv.org/pdf/2510.26274.pdf' target='_blank'>https://arxiv.org/pdf/2510.26274.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Haohua Duan, Liyao Xiang, Xin Zhang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.26274">PVMark: Enabling Public Verifiability for LLM Watermarking Schemes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking schemes for large language models (LLMs) have been proposed to identify the source of the generated text, mitigating the potential threats emerged from model theft. However, current watermarking solutions hardly resolve the trust issue: the non-public watermark detection cannot prove itself faithfully conducting the detection. We observe that it is attributed to the secret key mostly used in the watermark detection -- it cannot be public, or the adversary may launch removal attacks provided the key; nor can it be private, or the watermarking detection is opaque to the public. To resolve the dilemma, we propose PVMark, a plugin based on zero-knowledge proof (ZKP), enabling the watermark detection process to be publicly verifiable by third parties without disclosing any secret key. PVMark hinges upon the proof of `correct execution' of watermark detection on which a set of ZKP constraints are built, including mapping, random number generation, comparison, and summation. We implement multiple variants of PVMark in Python, Rust and Circom, covering combinations of three watermarking schemes, three hash functions, and four ZKP protocols, to show our approach effectively works under a variety of circumstances. By experimental results, PVMark efficiently enables public verifiability on the state-of-the-art LLM watermarking schemes yet without compromising the watermarking performance, promising to be deployed in practice.
<div id='section'>Paperid: <span id='pid'>844, <a href='https://arxiv.org/pdf/2510.25934.pdf' target='_blank'>https://arxiv.org/pdf/2510.25934.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jipeng Li, Yannning Shen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.25934">Robust GNN Watermarking via Implicit Perception of Topological Invariants</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Graph Neural Networks (GNNs) are valuable intellectual property, yet many watermarks rely on backdoor triggers that break under common model edits and create ownership ambiguity. We present InvGNN-WM, which ties ownership to a model's implicit perception of a graph invariant, enabling trigger-free, black-box verification with negligible task impact. A lightweight head predicts normalized algebraic connectivity on an owner-private carrier set; a sign-sensitive decoder outputs bits, and a calibrated threshold controls the false-positive rate. Across diverse node and graph classification datasets and backbones, InvGNN-WM matches clean accuracy while yielding higher watermark accuracy than trigger- and compression-based baselines. It remains strong under unstructured pruning, fine-tuning, and post-training quantization; plain knowledge distillation (KD) weakens the mark, while KD with a watermark loss (KD+WM) restores it. We provide guarantees for imperceptibility and robustness, and we prove that exact removal is NP-complete.
<div id='section'>Paperid: <span id='pid'>845, <a href='https://arxiv.org/pdf/2508.20718.pdf' target='_blank'>https://arxiv.org/pdf/2508.20718.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ruiyi Yan, Yugo Murawaki
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.20718">Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models have significantly enhanced the capacities and efficiency of text generation. On the one hand, they have improved the quality of text-based steganography. On the other hand, they have also underscored the importance of watermarking as a safeguard against malicious misuse. In this study, we focus on tokenization inconsistency (TI) between Alice and Bob in steganography and watermarking, where TI can undermine robustness. Our investigation reveals that the problematic tokens responsible for TI exhibit two key characteristics: infrequency and temporariness. Based on these findings, we propose two tailored solutions for TI elimination: a stepwise verification method for steganography and a post-hoc rollback method for watermarking. Experiments show that (1) compared to traditional disambiguation methods in steganography, directly addressing TI leads to improvements in fluency, imperceptibility, and anti-steganalysis capacity; (2) for watermarking, addressing TI enhances detectability and robustness against attacks.
<div id='section'>Paperid: <span id='pid'>846, <a href='https://arxiv.org/pdf/2508.07044.pdf' target='_blank'>https://arxiv.org/pdf/2508.07044.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>William Zerong Wang, Dongfang Zhao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2508.07044">Balancing Privacy and Efficiency: Music Information Retrieval via Additive Homomorphic Encryption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the era of generative AI, ensuring the privacy of music data presents unique challenges: unlike static artworks such as images, music data is inherently temporal and multimodal, and it is sampled, transformed, and remixed at an unprecedented scale. These characteristics make its core vector embeddings, i.e, the numerical representations of the music, highly susceptible to being learned, misused, or even stolen by models without accessing the original audio files. Traditional methods like copyright licensing and digital watermarking offer limited protection for these abstract mathematical representations, thus necessitating a stronger, e.g., cryptographic, approach to safeguarding the embeddings themselves. Standard encryption schemes, such as AES, render data unintelligible for computation, making such searches impossible. While Fully Homomorphic Encryption (FHE) provides a plausible solution by allowing arbitrary computations on ciphertexts, its substantial performance overhead remains impractical for large-scale vector similarity searches. Given this trade-off, we propose a more practical approach using Additive Homomorphic Encryption (AHE) for vector similarity search. The primary contributions of this paper are threefold: we analyze threat models unique to music information retrieval systems; we provide a theoretical analysis and propose an efficient AHE-based solution through inner products of music embeddings to deliver privacy-preserving similarity search; and finally, we demonstrate the efficiency and practicality of the proposed approach through empirical evaluation and comparison to FHE schemes on real-world MP3 files.
<div id='section'>Paperid: <span id='pid'>847, <a href='https://arxiv.org/pdf/2507.23002.pdf' target='_blank'>https://arxiv.org/pdf/2507.23002.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peter F. Michael, Zekun Hao, Serge Belongie, Abe Davis
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.23002">Noise-Coded Illumination for Forensic and Photometric Video Analysis</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of advanced tools for manipulating video has led to an arms race, pitting those who wish to sow disinformation against those who want to detect and expose it. Unfortunately, time favors the ill-intentioned in this race, with fake videos growing increasingly difficult to distinguish from real ones. At the root of this trend is a fundamental advantage held by those manipulating media: equal access to a distribution of what we consider authentic (i.e., "natural") video. In this paper, we show how coding very subtle, noise-like modulations into the illumination of a scene can help combat this advantage by creating an information asymmetry that favors verification. Our approach effectively adds a temporal watermark to any video recorded under coded illumination. However, rather than encoding a specific message, this watermark encodes an image of the unmanipulated scene as it would appear lit only by the coded illumination. We show that even when an adversary knows that our technique is being used, creating a plausible coded fake video amounts to solving a second, more difficult version of the original adversarial content creation problem at an information disadvantage. This is a promising avenue for protecting high-stakes settings like public events and interviews, where the content on display is a likely target for manipulation, and while the illumination can be controlled, the cameras capturing video cannot.
<div id='section'>Paperid: <span id='pid'>848, <a href='https://arxiv.org/pdf/2507.08288.pdf' target='_blank'>https://arxiv.org/pdf/2507.08288.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qingxiao Guo, Xinjie Zhu, Yilong Ma, Hui Jin, Yunhao Wang, Weifeng Zhang, Xiaobing Guo
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.08288">Invariant-based Robust Weights Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking technology has gained significant attention due to the increasing importance of intellectual property (IP) rights, particularly with the growing deployment of large language models (LLMs) on billions resource-constrained edge devices. To counter the potential threats of IP theft by malicious users, this paper introduces a robust watermarking scheme without retraining or fine-tuning for transformer models. The scheme generates a unique key for each user and derives a stable watermark value by solving linear constraints constructed from model invariants. Moreover, this technology utilizes noise mechanism to hide watermark locations in multi-user scenarios against collusion attack. This paper evaluates the approach on three popular models (Llama3, Phi3, Gemma), and the experimental results confirm the strong robustness across a range of attack methods (fine-tuning, pruning, quantization, permutation, scaling, reversible matrix and collusion attacks).
<div id='section'>Paperid: <span id='pid'>849, <a href='https://arxiv.org/pdf/2507.07250.pdf' target='_blank'>https://arxiv.org/pdf/2507.07250.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jordi Serra-Ruiz, David MegÃ­as
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.07250">Semi-fragile watermarking of remote sensing images using DWT, vector quantization and automatic tiling</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A semi-fragile watermarking scheme for multiple band images is presented in this article. We propose to embed a mark into remote sensing images applying a tree-structured vector quantization approach to the pixel signatures instead of processing each band separately. The signature of the multispectral or hyperspectral image is used to embed the mark in it order to detect any significant modification of the original image. The image is segmented into three-dimensional blocks, and a tree-structured vector quantizer is built for each block. These trees are manipulated using an iterative algorithm until the resulting block satisfies a required criterion, which establishes the embedded mark. The method is shown to be able to preserve the mark under lossy compression (above a given threshold) but, at the same time, it detects possibly forged blocks and their position in the whole image.
<div id='section'>Paperid: <span id='pid'>850, <a href='https://arxiv.org/pdf/2507.04495.pdf' target='_blank'>https://arxiv.org/pdf/2507.04495.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hyunwook Choi, Sangyun Won, Daeyeon Hwang, Junhyeok Choi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.04495">README: Robust Error-Aware Digital Signature Framework via Deep Watermarking Model</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Deep learning-based watermarking has emerged as a promising solution for robust image authentication and protection. However, existing models are limited by low embedding capacity and vulnerability to bit-level errors, making them unsuitable for cryptographic applications such as digital signatures, which require over 2048 bits of error-free data. In this paper, we propose README (Robust Error-Aware Digital Signature via Deep WaterMarking ModEl), a novel framework that enables robust, verifiable, and error-tolerant digital signatures within images. Our method combines a simple yet effective cropping-based capacity scaling mechanism with ERPA (ERror PAinting Module), a lightweight error correction module designed to localize and correct bit errors using Distinct Circular Subsum Sequences (DCSS). Without requiring any fine-tuning of existing pretrained watermarking models, README significantly boosts the zero-bit-error image rate (Z.B.I.R) from 1.2% to 86.3% when embedding 2048-bit digital signatures into a single image, even under real-world distortions. Moreover, our use of perceptual hash-based signature verification ensures public verifiability and robustness against tampering. The proposed framework unlocks a new class of high-assurance applications for deep watermarking, bridging the gap between signal-level watermarking and cryptographic security.
<div id='section'>Paperid: <span id='pid'>851, <a href='https://arxiv.org/pdf/2507.03014.pdf' target='_blank'>https://arxiv.org/pdf/2507.03014.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Do-hyeon Yoon, Minsoo Chun, Thomas Allen, Hans MÃ¼ller, Min Wang, Rajesh Sharma
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.03014">Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal A Model!</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) face significant copyright and intellectual property challenges as the cost of training increases and model reuse becomes prevalent. While watermarking techniques have been proposed to protect model ownership, they may not be robust to continue training and development, posing serious threats to model attribution and copyright protection. This work introduces a simple yet effective approach for robust LLM fingerprinting based on intrinsic model characteristics. We discover that the standard deviation distributions of attention parameter matrices across different layers exhibit distinctive patterns that remain stable even after extensive continued training. These parameter distribution signatures serve as robust fingerprints that can reliably identify model lineage and detect potential copyright infringement. Our experimental validation across multiple model families demonstrates the effectiveness of our method for model authentication. Notably, our investigation uncovers evidence that a recently Pangu Pro MoE model released by Huawei is derived from Qwen-2.5 14B model through upcycling techniques rather than training from scratch, highlighting potential cases of model plagiarism, copyright violation, and information fabrication. These findings underscore the critical importance of developing robust fingerprinting methods for protecting intellectual property in large-scale model development and emphasize that deliberate continued training alone is insufficient to completely obscure model origins.
<div id='section'>Paperid: <span id='pid'>852, <a href='https://arxiv.org/pdf/2507.00827.pdf' target='_blank'>https://arxiv.org/pdf/2507.00827.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gabriel Grobler, Sheunesu Makura, Hein Venter
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.00827">A Technique for the Detection of PDF Tampering or Forgery</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Tampering or forgery of digital documents has become widespread, most commonly through altering images without any malicious intent such as enhancing the overall appearance of the image. However, there are occasions when tampering of digital documents can have negative consequences, such as financial fraud and reputational damage. Tampering can occur through altering a digital document's text or editing an image's pixels. Many techniques have been developed to detect whether changes have been made to a document. Most of these techniques rely on generating hashes or watermarking the document. These techniques, however, have limitations in that they cannot detect alterations to portable document format (PDF) signatures or other non-visual aspects, such as metadata. This paper presents a new technique that can be used to detect tampering within a PDF document by utilizing the PDF document's file page objects. The technique employs a prototype that can detect changes to a PDF document, such as changes made to the text, images, or metadata of the said file.
<div id='section'>Paperid: <span id='pid'>853, <a href='https://arxiv.org/pdf/2506.22623.pdf' target='_blank'>https://arxiv.org/pdf/2506.22623.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Badr Youbi Idrissi, Monica Millunzi, Amelia Sorrenti, Lorenzo Baraldi, Daryna Dementieva
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.22623">Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the present-day scenario, Large Language Models (LLMs) are establishing their presence as powerful instruments permeating various sectors of society. While their utility offers valuable support to individuals, there are multiple concerns over potential misuse. Consequently, some academic endeavors have sought to introduce watermarking techniques, characterized by the inclusion of markers within machine-generated text, to facilitate algorithmic identification. This research project is focused on the development of a novel methodology for the detection of synthetic text, with the overarching goal of ensuring the ethical application of LLMs in AI-driven text generation. The investigation commences with replicating findings from a previous baseline study, thereby underscoring its susceptibility to variations in the underlying generation model. Subsequently, we propose an innovative watermarking approach and subject it to rigorous evaluation, employing paraphrased generated text to asses its robustness. Experimental results highlight the robustness of our proposal compared to the~\cite{aarson} watermarking method.
<div id='section'>Paperid: <span id='pid'>854, <a href='https://arxiv.org/pdf/2506.10502.pdf' target='_blank'>https://arxiv.org/pdf/2506.10502.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Junhua Lin, Marc Juarez
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.10502">A Crack in the Bark: Leveraging Public Knowledge to Remove Tree-Ring Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a novel attack specifically designed against Tree-Ring, a watermarking technique for diffusion models known for its high imperceptibility and robustness against removal attacks. Unlike previous removal attacks, which rely on strong assumptions about attacker capabilities, our attack only requires access to the variational autoencoder that was used to train the target diffusion model, a component that is often publicly available. By leveraging this variational autoencoder, the attacker can approximate the model's intermediate latent space, enabling more effective surrogate-based attacks. Our evaluation shows that this approach leads to a dramatic reduction in the AUC of Tree-Ring detector's ROC and PR curves, decreasing from 0.993 to 0.153 and from 0.994 to 0.385, respectively, while maintaining high image quality. Notably, our attacks outperform existing methods that assume full access to the diffusion model. These findings highlight the risk of reusing public autoencoders to train diffusion models -- a threat not considered by current industry practices. Furthermore, the results suggest that the Tree-Ring detector's precision, a metric that has been overlooked by previous evaluations, falls short of the requirements for real-world deployment.
<div id='section'>Paperid: <span id='pid'>855, <a href='https://arxiv.org/pdf/2506.06691.pdf' target='_blank'>https://arxiv.org/pdf/2506.06691.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaushik Talathi, Aparna Santra Biswas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.06691">An Efficient Digital Watermarking Technique for Small Scale devices</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the age of IoT and mobile platforms, ensuring that content stay authentic whilst avoiding overburdening limited hardware is a key problem. This study introduces hybrid Fast Wavelet Transform & Additive Quantization index Modulation (FWT-AQIM) scheme, a lightweight watermarking approach that secures digital pictures on low-power, memory-constrained small scale devices to achieve a balanced trade-off among robustness, imperceptibility, and computational efficiency. The method embeds watermark in the luminance component of YCbCr color space using low-frequency FWT sub-bands, minimizing perceptual distortion, using additive QIM for simplicity. Both the extraction and embedding processes run in less than 40 ms and require minimum RAM when tested on a Raspberry Pi 5. Quality assessments on standard and high-resolution images yield PSNR greater than equal to 34 dB and SSIM greater than equal to 0.97, while robustness verification includes various geometric and signal-processing attacks demonstrating near-zero bit error rates and NCC greater than equal to 0.998. Using a mosaic-based watermark, redundancy added enhancing robustness without reducing throughput, which peaks at 11 MP/s. These findings show that FWT-AQIM provides an efficient, scalable solution for real-time, secure watermarking in bandwidth- and power-constrained contexts, opening the way for dependable content protection in developing IoT and multimedia applications.
<div id='section'>Paperid: <span id='pid'>856, <a href='https://arxiv.org/pdf/2505.05712.pdf' target='_blank'>https://arxiv.org/pdf/2505.05712.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>JarosÅaw Janas, PaweÅ Morawiecki, Josef Pieprzyk
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.05712">LLM-Text Watermarking based on Lagrange Interpolation</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of LLMs (Large Language Models) has established them as a foundational technology for many AI and ML-powered human computer interactions. A critical challenge in this context is the attribution of LLM-generated text -- either to the specific language model that produced it or to the individual user who embedded their identity via a so-called multi-bit watermark. This capability is essential for combating misinformation, fake news, misinterpretation, and plagiarism. One of the key techniques for addressing this challenge is digital watermarking.
  This work presents a watermarking scheme for LLM-generated text based on Lagrange interpolation, enabling the recovery of a multi-bit author identity even when the text has been heavily redacted by an adversary. The core idea is to embed a continuous sequence of points $(x, f(x))$ that lie on a single straight line. The $x$-coordinates are computed pseudorandomly using a cryptographic hash function $H$ applied to the concatenation of the previous token's identity and a secret key $s_k$. Crucially, the $x$-coordinates do not need to be embedded into the text -- only the corresponding $f(x)$ values are embedded. During extraction, the algorithm recovers the original points along with many spurious ones, forming an instance of the Maximum Collinear Points (MCP) problem, which can be solved efficiently. Experimental results demonstrate that the proposed method is highly effective, allowing the recovery of the author identity even when as few as three genuine points remain after adversarial manipulation.
<div id='section'>Paperid: <span id='pid'>857, <a href='https://arxiv.org/pdf/2505.01484.pdf' target='_blank'>https://arxiv.org/pdf/2505.01484.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Pedro Abdalla, Roman Vershynin
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01484">LLM Watermarking Using Mixtures and Statistical-to-Computational Gaps</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Given a text, can we determine whether it was generated by a large language model (LLM) or by a human? A widely studied approach to this problem is watermarking. We propose an undetectable and elementary watermarking scheme in the closed setting. Also, in the harder open setting, where the adversary has access to most of the model, we propose an unremovable watermarking scheme.
<div id='section'>Paperid: <span id='pid'>858, <a href='https://arxiv.org/pdf/2505.01474.pdf' target='_blank'>https://arxiv.org/pdf/2505.01474.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>I. F. Serzhenko, L. A. Khaertdinova, M. A. Pautov, A. V. Antsiferova
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.01474">Watermark Overwriting Attack on StegaStamp algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents an attack method on the StegaStamp watermarking algorithm that completely removes watermarks from an image with minimal quality loss, developed as part of the NeurIPS "Erasing the invisible" competition.
<div id='section'>Paperid: <span id='pid'>859, <a href='https://arxiv.org/pdf/2503.13805.pdf' target='_blank'>https://arxiv.org/pdf/2503.13805.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Muhammad Ahtesham, Xin Zhong
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.13805">Text-Guided Image Invariant Feature Learning for Robust Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring robustness in image watermarking is crucial for and maintaining content integrity under diverse transformations. Recent self-supervised learning (SSL) approaches, such as DINO, have been leveraged for watermarking but primarily focus on general feature representation rather than explicitly learning invariant features. In this work, we propose a novel text-guided invariant feature learning framework for robust image watermarking. Our approach leverages CLIP's multimodal capabilities, using text embeddings as stable semantic anchors to enforce feature invariance under distortions. We evaluate the proposed method across multiple datasets, demonstrating superior robustness against various image transformations. Compared to state-of-the-art SSL methods, our model achieves higher cosine similarity in feature consistency tests and outperforms existing watermarking schemes in extraction accuracy under severe distortions. These results highlight the efficacy of our method in learning invariant representations tailored for robust deep learning-based watermarking.
<div id='section'>Paperid: <span id='pid'>860, <a href='https://arxiv.org/pdf/2503.07661.pdf' target='_blank'>https://arxiv.org/pdf/2503.07661.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Wei Junhao, Yu Zhe, Sakuma Jun
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2503.07661">Disrupting Model Merging: A Parameter-Level Defense Without Sacrificing Accuracy</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Model merging is a technique that combines multiple finetuned models into a single model without additional training, allowing a free-rider to cheaply inherit specialized capabilities. This study investigates methodologies to suppress unwanted model merging by free-riders. Existing methods such as model watermarking or fingerprinting can only detect merging in hindsight. In contrast, we propose a first proactive defense against model merging. Specifically, our defense method modifies the model parameters so that the model is disrupted if the model is merged with any other model, while its functionality is kept unchanged if not merged with others. Our approach consists of two modules, rearranging MLP parameters and scaling attention heads, which push the model out of the shared basin in parameter space, causing the merging performance with other models to degrade significantly. We conduct extensive experiments on image classification, image generation, and text classification to demonstrate that our defense severely disrupts merging while retaining the functionality of the post-protect model. Moreover, we analyze potential adaptive attacks and further propose a dropout-based pruning to improve our proposal's robustness.
<div id='section'>Paperid: <span id='pid'>861, <a href='https://arxiv.org/pdf/2502.17089.pdf' target='_blank'>https://arxiv.org/pdf/2502.17089.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Feick, Xuxin Tang, Raul Garcia-Martin, Alexandru Luchianov, Roderick Wei Xiao Huang, Chang Xiao, Alexa Siu, Mustafa Doga Dogan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.17089">Imprinto: Enhancing Infrared Inkjet Watermarking for Human and Machine Perception</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Hybrid paper interfaces leverage augmented reality to combine the desired tangibility of paper documents with the affordances of interactive digital media. Typically, virtual content can be embedded through direct links (e.g., QR codes); however, this impacts the aesthetics of the paper print and limits the available visual content space. To address this problem, we present Imprinto, an infrared inkjet watermarking technique that allows for invisible content embeddings only by using off-the-shelf IR inks and a camera. Imprinto was established through a psychophysical experiment, studying how much IR ink can be used while remaining invisible to users regardless of background color. We demonstrate that we can detect invisible IR content through our machine learning pipeline, and we developed an authoring tool that optimizes the amount of IR ink on the color regions of an input document for machine and human detectability. Finally, we demonstrate several applications, including augmenting paper documents and objects.
<div id='section'>Paperid: <span id='pid'>862, <a href='https://arxiv.org/pdf/2502.12710.pdf' target='_blank'>https://arxiv.org/pdf/2502.12710.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malte Hellmeier, Hendrik Norkowski, Ernst-Christoph Schrewe, Haydar Qarawlus, Falk Howar
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.12710">Innamark: A Whitespace Replacement Information-Hiding Method</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) have gained significant popularity in recent years. Differentiating between a text written by a human and one generated by an LLM has become almost impossible. Information-hiding techniques such as digital watermarking or steganography can help by embedding information inside text in a form that is unlikely to be noticed. However, existing techniques, such as linguistic-based or format-based methods, change the semantics or cannot be applied to pure, unformatted text. In this paper, we introduce a novel method for information hiding called Innamark, which can conceal any byte-encoded sequence within a sufficiently long cover text. This method is implemented as a multi-platform library using the Kotlin programming language, which is accompanied by a command-line tool and a web interface. By substituting conventional whitespace characters with visually similar Unicode whitespace characters, our proposed scheme preserves the semantics of the cover text without changing the number of characters. Furthermore, we propose a specified structure for secret messages that enables configurable compression, encryption, hashing, and error correction. An experimental benchmark comparison on a dataset of 1 000 000 Wikipedia articles compares ten algorithms. The results demonstrate the robustness of our proposed Innamark method in various applications and the imperceptibility of its watermarks to humans. We discuss the limits to the embedding capacity and robustness of the algorithm and how these could be addressed in future work.
<div id='section'>Paperid: <span id='pid'>863, <a href='https://arxiv.org/pdf/2502.05931.pdf' target='_blank'>https://arxiv.org/pdf/2502.05931.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ahmed Abdelaziz, Ahmed Fathi, Ahmed Fares
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05931">Protecting Intellectual Property of EEG-based Neural Networks with Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>EEG-based neural networks, pivotal in medical diagnosis and brain-computer interfaces, face significant intellectual property (IP) risks due to their reliance on sensitive neurophysiological data and resource-intensive development. Current watermarking methods, particularly those using abstract trigger sets, lack robust authentication and fail to address the unique challenges of EEG models. This paper introduces a cryptographic wonder filter-based watermarking framework tailored for EEG-based neural networks. Leveraging collision-resistant hashing and public-key encryption, the wonder filter embeds the watermark during training, ensuring minimal distortion ($\leq 5\%$ drop in EEG task accuracy) and high reliability (100\% watermark detection). The framework is rigorously evaluated against adversarial attacks, including fine-tuning, transfer learning, and neuron pruning. Results demonstrate persistent watermark retention, with classification accuracy for watermarked states remaining above 90\% even after aggressive pruning, while primary task performance degrades faster, deterring removal attempts. Piracy resistance is validated by the inability to embed secondary watermarks without severe accuracy loss ( $>10\%$ in EEGNet and CCNN models). Cryptographic hashing ensures authentication, reducing brute-force attack success probabilities. Evaluated on the DEAP dataset across models (CCNN, EEGNet, TSception), the method achieves $>99.4\%$ null-embedding accuracy, effectively eliminating false positives. By integrating wonder filters with EEG-specific adaptations, this work bridges a critical gap in IP protection for neurophysiological models, offering a secure, tamper-proof solution for healthcare and biometric applications. The framework's robustness against adversarial modifications underscores its potential to safeguard sensitive EEG models while maintaining diagnostic utility.
<div id='section'>Paperid: <span id='pid'>864, <a href='https://arxiv.org/pdf/2502.05213.pdf' target='_blank'>https://arxiv.org/pdf/2502.05213.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Qihao Lin, Chen Tang, Lan zhang, Junyang zhang, Xiangyang Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2502.05213">DERMARK: A Dynamic, Efficient and Robust Multi-bit Watermark for Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) grow more powerful, concerns over copyright infringement of LLM-generated texts have intensified. LLM watermarking has been proposed to trace unauthorized redistribution or resale of generated content by embedding identifiers within the text. Existing approaches primarily rely on one-bit watermarking, which only verifies whether a text was generated by a specific LLM. In contrast, multi-bit watermarking encodes richer information, enabling the identification of the specific LLM and user involved in generated or distributed content. However, current multi-bit methods directly embed the watermark into the text without considering its watermark capacity, which can result in failures, especially in low-entropy texts. In this paper, we analyze that the watermark embedding follows a normal distribution. We then derive a formal inequality to optimally segment the text for watermark embedding. Building upon this, we propose DERMARK, a dynamic, efficient, and robust multi-bit watermarking method that divides the text into variable-length segments for each watermark bit during the inference. Moreover, DERMARK incurs negligible overhead since no additional intermediate matrices are generated and achieves robustness against text editing by minimizing watermark extraction loss. Experiments demonstrate that, compared to SOTA, on average, our method reduces the number of tokens required per embedded bit by 25\%, reduces watermark embedding time by 50\%, and maintains high robustness against text modifications and watermark erasure attacks.
<div id='section'>Paperid: <span id='pid'>865, <a href='https://arxiv.org/pdf/2501.06161.pdf' target='_blank'>https://arxiv.org/pdf/2501.06161.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Farzana Kabir, David Megias, Krzysztof Cabaj
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.06161">RIOT-based smart metering system for privacy-preserving data aggregation using watermarking and encryption</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The remarkable advancement of smart grid technology in the IoT sector has raised concerns over the privacy and security of the data collected and transferred in real-time. Smart meters generate detailed information about consumers' energy consumption patterns, increasing the risks of data breaches, identity theft, and other forms of cyber attacks. This study proposes a privacy-preserving data aggregation protocol that uses reversible watermarking and AES cryptography to ensure the security and privacy of the data. There are two versions of the protocol: one for low-frequency smart meters that uses LSB-shifting-based reversible watermarking (RLS) and another for high-frequency smart meters that uses difference expansion-based reversible watermarking (RDE). This enables the aggregation of smart meter data, maintaining confidentiality, integrity, and authenticity. The proposed protocol significantly enhances privacy-preserving measures for smart metering systems, conducting an experimental evaluation with real hardware implementation using Nucleo microcontroller boards and the RIOT operating system and comparing the results to existing security schemes.
<div id='section'>Paperid: <span id='pid'>866, <a href='https://arxiv.org/pdf/2501.00463.pdf' target='_blank'>https://arxiv.org/pdf/2501.00463.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lu Zhang, Liang Zeng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.00463">SAT-LDM: Provably Generalizable Image Watermarking for Latent Diffusion Models with Self-Augmented Training</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid proliferation of AI-generated images necessitates effective watermarking techniques to protect intellectual property and detect fraudulent content. While existing training-based watermarking methods show promise, they often struggle with generalizing across diverse prompts and tend to introduce visible artifacts. To this end, we propose a novel, provably generalizable image watermarking approach for Latent Diffusion Models, termed Self-Augmented Training (SAT-LDM). Our method aligns the training and testing phases through a free generation distribution, thereby enhancing the watermarking module's generalization capabilities. We theoretically consolidate SAT-LDM by proving that the free generation distribution contributes to its tight generalization bound, without the need for additional data collection. Extensive experiments show that SAT-LDM not only achieves robust watermarking but also significantly improves the quality of watermarked images across a wide range of prompts. Moreover, our experimental analyses confirm the strong generalization abilities of SAT-LDM. We hope that our method provides a practical and efficient solution for securing high-fidelity AI-generated content.
<div id='section'>Paperid: <span id='pid'>867, <a href='https://arxiv.org/pdf/2412.16789.pdf' target='_blank'>https://arxiv.org/pdf/2412.16789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Imants Svalbe, Rob Tijdeman
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.16789">Inflation of 2D boundary ghosts and digital watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Projection ghosts are discrete arrays of signed values positioned so that their discrete projections vanish for some chosen set of n projection angles. Minimal ghosts are designed to be compact, with no internal pixels having value zero. Here we control the shape, number of boundary pixels and area that each minimal ghost encloses. Binary minimal ghosts and their boundaries can themselves be inflated by tiling copies of themselves to make ghosts with larger sizes and different shapes, whilst still retaining the same set of n zero projection angles. The intricate perimeters of minimal ghosts are formed by three strings of connected pixels that are defined by the minimal projection angles. We show that large changes to the ghost areas can be made whilst keeping the length of their segmented perimeters fixed. These inflated boundary ghosts may prove useful as secure watermarks to embed into digital image data. Boundary ghosts may also help guide the selection of angles used to reconstruct images where the object domain is confined to oval shaped arcs.
<div id='section'>Paperid: <span id='pid'>868, <a href='https://arxiv.org/pdf/2412.10649.pdf' target='_blank'>https://arxiv.org/pdf/2412.10649.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Christopher J. Tralie, Matt Amery, Benjamin Douglas, Ian Utz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.10649">Hidden Echoes Survive Training in Audio To Audio Generative Instrument Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As generative techniques pervade the audio domain, there has been increasing interest in tracing back through these complicated models to understand how they draw on their training data to synthesize new examples, both to ensure that they use properly licensed data and also to elucidate their black box behavior. In this paper, we show that if imperceptible echoes are hidden in the training data, a wide variety of audio to audio architectures (differentiable digital signal processing (DDSP), Realtime Audio Variational autoEncoder (RAVE), and ``Dance Diffusion'') will reproduce these echoes in their outputs. Hiding a single echo is particularly robust across all architectures, but we also show promising results hiding longer time spread echo patterns for an increased information capacity. We conclude by showing that echoes make their way into fine tuned models, that they survive mixing/demixing, and that they survive pitch shift augmentation during training. Hence, this simple, classical idea in watermarking shows significant promise for tagging generative audio models.
<div id='section'>Paperid: <span id='pid'>869, <a href='https://arxiv.org/pdf/2411.17684.pdf' target='_blank'>https://arxiv.org/pdf/2411.17684.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bhaktipriya Radharapu, Harish Krishna
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.17684">RealSeal: Revolutionizing Media Authentication with Real-Time Realism Scoring</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The growing threat of deepfakes and manipulated media necessitates a radical rethinking of media authentication. Existing methods for watermarking synthetic data fall short, as they can be easily removed or altered, and current deepfake detection algorithms do not achieve perfect accuracy. Provenance techniques, which rely on metadata to verify content origin, fail to address the fundamental problem of staged or fake media.
  This paper introduces a groundbreaking paradigm shift in media authentication by advocating for the watermarking of real content at its source, as opposed to watermarking synthetic data. Our innovative approach employs multisensory inputs and machine learning to assess the realism of content in real-time and across different contexts. We propose embedding a robust realism score within the image metadata, fundamentally transforming how images are trusted and circulated. By combining established principles of human reasoning about reality, rooted in firmware and hardware security, with the sophisticated reasoning capabilities of contemporary machine learning systems, we develop a holistic approach that analyzes information from multiple perspectives.
  This ambitious, blue sky approach represents a significant leap forward in the field, pushing the boundaries of media authenticity and trust. By embracing cutting-edge advancements in technology and interdisciplinary research, we aim to establish a new standard for verifying the authenticity of digital media.
<div id='section'>Paperid: <span id='pid'>870, <a href='https://arxiv.org/pdf/2411.03806.pdf' target='_blank'>https://arxiv.org/pdf/2411.03806.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Hiu Ting Lau, Arkaitz Zubiaga
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03806">Understanding the Effects of Human-written Paraphrases in LLM-generated Text Detection</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Natural Language Generation has been rapidly developing with the advent of large language models (LLMs). While their usage has sparked significant attention from the general public, it is important for readers to be aware when a piece of text is LLM-generated. This has brought about the need for building models that enable automated LLM-generated text detection, with the aim of mitigating potential negative outcomes of such content. Existing LLM-generated detectors show competitive performances in telling apart LLM-generated and human-written text, but this performance is likely to deteriorate when paraphrased texts are considered. In this study, we devise a new data collection strategy to collect Human & LLM Paraphrase Collection (HLPC), a first-of-its-kind dataset that incorporates human-written texts and paraphrases, as well as LLM-generated texts and paraphrases. With the aim of understanding the effects of human-written paraphrases on the performance of state-of-the-art LLM-generated text detectors OpenAI RoBERTa and watermark detectors, we perform classification experiments that incorporate human-written paraphrases, watermarked and non-watermarked LLM-generated documents from GPT and OPT, and LLM-generated paraphrases from DIPPER and BART. The results show that the inclusion of human-written paraphrases has a significant impact of LLM-generated detector performance, promoting TPR@1%FPR with a possible trade-off of AUROC and accuracy.
<div id='section'>Paperid: <span id='pid'>871, <a href='https://arxiv.org/pdf/2409.14829.pdf' target='_blank'>https://arxiv.org/pdf/2409.14829.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weitong Chen, Yuheng Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.14829">RoWSFormer: A Robust Watermarking Framework with Swin Transformer for Enhanced Geometric Attack Resilience</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years, digital watermarking techniques based on deep learning have been widely studied. To achieve both imperceptibility and robustness of image watermarks, most current methods employ convolutional neural networks to build robust watermarking frameworks. However, despite the success of CNN-based watermarking models, they struggle to achieve robustness against geometric attacks due to the limitations of convolutional neural networks in capturing global and long-range relationships. To address this limitation, we propose a robust watermarking framework based on the Swin Transformer, named RoWSFormer. Specifically, we design the Locally-Channel Enhanced Swin Transformer Block as the core of both the encoder and decoder. This block utilizes the self-attention mechanism to capture global and long-range information, thereby significantly improving adaptation to geometric distortions. Additionally, we construct the Frequency-Enhanced Transformer Block to extract frequency domain information, which further strengthens the robustness of the watermarking framework. Experimental results demonstrate that our RoWSFormer surpasses existing state-of-the-art watermarking methods. For most non-geometric attacks, RoWSFormer improves the PSNR by 3 dB while maintaining the same extraction accuracy. In the case of geometric attacks (such as rotation, scaling, and affine transformations), RoWSFormer achieves over a 6 dB improvement in PSNR, with extraction accuracy exceeding 97\%.
<div id='section'>Paperid: <span id='pid'>872, <a href='https://arxiv.org/pdf/2409.07580.pdf' target='_blank'>https://arxiv.org/pdf/2409.07580.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Surendra Ghentiyala, Venkatesan Guruswami
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.07580">New constructions of pseudorandom codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Introduced in [CG24], pseudorandom error-correcting codes (PRCs) are a new cryptographic primitive with applications in watermarking generative AI models. These are codes where a collection of polynomially many codewords is computationally indistinguishable from random for an adversary that does not have the secret key, but anyone with the secret key is able to efficiently decode corrupted codewords. In this work, we examine the assumptions under which PRCs with robustness to a constant error rate exist.
  1. We show that if both the planted hyperloop assumption introduced in [BKR23] and security of a version of Goldreich's PRG hold, then there exist public-key PRCs for which no efficient adversary can distinguish a polynomial number of codewords from random with better than $o(1)$ advantage.
  2. We revisit the construction of [CG24] and show that it can be based on a wider range of assumptions than presented in [CG24]. To do this, we introduce a weakened version of the planted XOR assumption which we call the weak planted XOR assumption and which may be of independent interest.
  3. We initiate the study of PRCs which are secure against space-bounded adversaries. We show how to construct secret-key PRCs of length $O(n)$ which are $\textit{unconditionally}$ indistinguishable from random by $\text{poly}(n)$ time, $O(n^{1.5-\varepsilon})$ space adversaries.
<div id='section'>Paperid: <span id='pid'>873, <a href='https://arxiv.org/pdf/2408.15553.pdf' target='_blank'>https://arxiv.org/pdf/2408.15553.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Martin Moritz, Toni OlÃ¡n, Tuomas Virtanen
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.15553">Noise-to-mask Ratio Loss for Deep Neural Network based Audio Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Digital audio watermarking consists in inserting a message into audio signals in a transparent way and can be used to allow automatic recognition of audio material and management of the copyrights. We propose a perceptual loss function to be used in deep neural network based audio watermarking systems. The loss is based on the noise-to-mask ratio (NMR), which is a model of the psychoacoustic masking effect characteristic of the human ear. We use the NMR loss between marked and host signals to train the deep neural models and we evaluate the objective quality with PEAQ and the subjective quality with a MUSHRA test. Both objective and subjective tests show that models trained with NMR loss generate more transparent watermarks than models trained with the conventionally used MSE loss
<div id='section'>Paperid: <span id='pid'>874, <a href='https://arxiv.org/pdf/2406.11239.pdf' target='_blank'>https://arxiv.org/pdf/2406.11239.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aldan Creo, Shushanta Pudasaini
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2406.11239">SilverSpeak: Evading AI-Generated Text Detectors using Homoglyphs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The advent of Large Language Models (LLMs) has enabled the generation of text that increasingly exhibits human-like characteristics. As the detection of such content is of significant importance, substantial research has been conducted with the objective of developing reliable AI-generated text detectors. These detectors have demonstrated promising results on test data, but recent research has revealed that they can be circumvented by employing different techniques.
  In this paper, we present homoglyph-based attacks (A $\rightarrow$ Cyrillic A) as a means of circumventing existing detectors. We conduct a comprehensive evaluation to assess the effectiveness of these attacks on seven detectors, including ArguGPT, Binoculars, DetectGPT, Fast-DetectGPT, Ghostbuster, OpenAI's detector, and watermarking techniques, on five different datasets. Our findings demonstrate that homoglyph-based attacks can effectively circumvent state-of-the-art detectors, leading them to classify all texts as either AI-generated or human-written (decreasing the average Matthews Correlation Coefficient from 0.64 to -0.01). Through further examination, we extract the technical justification underlying the success of the attacks, which varies across detectors. Finally, we discuss the implications of these findings and potential defenses against such attacks.
<div id='section'>Paperid: <span id='pid'>875, <a href='https://arxiv.org/pdf/2405.12336.pdf' target='_blank'>https://arxiv.org/pdf/2405.12336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>John C. Simmons, Joseph M. Winograd
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.12336">Interoperable Provenance Authentication of Broadcast Media using Open Standards-based Metadata, Watermarking and Cryptography</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The spread of false and misleading information is receiving significant attention from legislative and regulatory bodies. Consumers place trust in specific sources of information, so a scalable, interoperable method for determining the provenance and authenticity of information is needed. In this paper we analyze the posting of broadcast news content to a social media platform, the role of open standards, the interplay of cryptographic metadata and watermarks when validating provenance, and likely success and failure scenarios. We conclude that the open standards for cryptographically authenticated metadata developed by the Coalition for Provenance and Authenticity (C2PA) and for audio and video watermarking developed by the Advanced Television Systems Committee (ATSC) are well suited to address broadcast provenance. We suggest methods for using these standards for optimal success.
<div id='section'>Paperid: <span id='pid'>876, <a href='https://arxiv.org/pdf/2405.11109.pdf' target='_blank'>https://arxiv.org/pdf/2405.11109.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aloni Cohen, Alexander Hoover, Gabe Schoenbach
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2405.11109">Watermarking Language Models for Many Adaptive Users</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We study watermarking schemes for language models with provable guarantees. As we show, prior works offer no robustness guarantees against adaptive prompting: when a user queries a language model more than once, as even benign users do. And with just a single exception (Christ and Gunn, 2024), prior works are restricted to zero-bit watermarking: machine-generated text can be detected as such, but no additional information can be extracted from the watermark. Unfortunately, merely detecting AI-generated text may not prevent future abuses.
  We introduce multi-user watermarks, which allow tracing model-generated text to individual users or to groups of colluding users, even in the face of adaptive prompting. We construct multi-user watermarking schemes from undetectable, adaptively robust, zero-bit watermarking schemes (and prove that the undetectable zero-bit scheme of Christ, Gunn, and Zamir (2024) is adaptively robust). Importantly, our scheme provides both zero-bit and multi-user assurances at the same time. It detects shorter snippets just as well as the original scheme, and traces longer excerpts to individuals.
  The main technical component is a construction of message-embedding watermarks from zero-bit watermarks. Ours is the first generic reduction between watermarking schemes for language models. A challenge for such reductions is the lack of a unified abstraction for robustness -- that marked text is detectable even after edits. We introduce a new unifying abstraction called AEB-robustness. AEB-robustness provides that the watermark is detectable whenever the edited text "approximates enough blocks" of model-generated output.
<div id='section'>Paperid: <span id='pid'>877, <a href='https://arxiv.org/pdf/2404.11408.pdf' target='_blank'>https://arxiv.org/pdf/2404.11408.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>James Weichert, Chinecherem Dimobi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2404.11408">DUPE: Detection Undermining via Prompt Engineering for Deepfake Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) become increasingly commonplace, concern about distinguishing between human and AI text increases as well. The growing power of these models is of particular concern to teachers, who may worry that students will use LLMs to write school assignments. Facing a technology with which they are unfamiliar, teachers may turn to publicly-available AI text detectors. Yet the accuracy of many of these detectors has not been thoroughly verified, posing potential harm to students who are falsely accused of academic dishonesty. In this paper, we evaluate three different AI text detectors-Kirchenbauer et al. watermarks, ZeroGPT, and GPTZero-against human and AI-generated essays. We find that watermarking results in a high false positive rate, and that ZeroGPT has both high false positive and false negative rates. Further, we are able to significantly increase the false negative rate of all detectors by using ChatGPT 3.5 to paraphrase the original AI-generated texts, thereby effectively bypassing the detectors.
<div id='section'>Paperid: <span id='pid'>878, <a href='https://arxiv.org/pdf/2403.15522.pdf' target='_blank'>https://arxiv.org/pdf/2403.15522.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Vijay Kumar, Kolin Paul
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.15522">Medical Image Data Provenance for Medical Cyber-Physical System</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Continuous advancements in medical technology have led to the creation of affordable mobile imaging devices suitable for telemedicine and remote monitoring. However, the rapid examination of large populations poses challenges, including the risk of fraudulent practices by healthcare professionals and social workers exchanging unverified images via mobile applications. To mitigate these risks, this study proposes using watermarking techniques to embed a device fingerprint (DFP) into captured images, ensuring data provenance. The DFP, representing the unique attributes of the capturing device and raw image, is embedded into raw images before storage, thus enabling verification of image authenticity and source. Moreover, a robust remote validation method is introduced to authenticate images, enhancing the integrity of medical image data in interconnected healthcare systems. Through a case study on mobile fundus imaging, the effectiveness of the proposed framework is evaluated in terms of computational efficiency, image quality, security, and trustworthiness. This approach is suitable for a range of applications, including telemedicine, the Internet of Medical Things (IoMT), eHealth, and Medical Cyber-Physical Systems (MCPS) applications, providing a reliable means to maintain data provenance in diagnostic settings utilizing medical images or videos.
<div id='section'>Paperid: <span id='pid'>879, <a href='https://arxiv.org/pdf/2403.06094.pdf' target='_blank'>https://arxiv.org/pdf/2403.06094.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tiroshan Madushanka, Dhammika S. Kumara, Atheesh A. Rathnaweera
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06094">SecureRights: A Blockchain-Powered Trusted DRM Framework for Robust Protection and Asserting Digital Rights</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the dynamic realm of digital content, safeguarding intellectual property rights poses critical challenges. This paper presents "SecureRights," an innovative Blockchain-based Trusted Digital Rights Management (DRM) framework. It strengthens the defence against unauthorized use and streamlines the claim of digital rights. Utilizing blockchain, digital watermarking, perceptual hashing, Quick Response (QR) codes, and the Interplanetary File System (IPFS), SecureRights securely stores watermark information on the blockchain with timestamp authentication. Incorporating perceptual hashing generates robust hash tokens based on image structure. The addition of QR codes enhances the watermarking, offering a comprehensive solution for resilient intellectual property rights protection. Rigorous evaluations affirm SecureRights' resilience against various attacks, establishing its efficacy in safeguarding digital content and simplifying rightful ownership assertion.
<div id='section'>Paperid: <span id='pid'>880, <a href='https://arxiv.org/pdf/2402.05508.pdf' target='_blank'>https://arxiv.org/pdf/2402.05508.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ryoto Kanegae, Masaki Kawamura
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.05508">Performance Evaluation of Associative Watermarking Using Statistical Neurodynamics</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We theoretically evaluated the performance of our proposed associative watermarking method in which the watermark is not embedded directly into the image. We previously proposed a watermarking method that extends the zero-watermarking model by applying associative memory models. In this model, the hetero-associative memory model is introduced to the mapping process between image features and watermarks, and the auto-associative memory model is applied to correct watermark errors. We herein show that the associative watermarking model outperforms the zero-watermarking model through computer simulations using actual images. In this paper, we describe how we derive the macroscopic state equation for the associative watermarking model using the Okada theory. The theoretical results obtained by the fourth-order theory were in good agreement with those obtained by computer simulations. Furthermore, the performance of the associative watermarking model was evaluated using the bit error rate of the watermark, both theoretically and using computer simulations.
<div id='section'>Paperid: <span id='pid'>881, <a href='https://arxiv.org/pdf/2402.01762.pdf' target='_blank'>https://arxiv.org/pdf/2402.01762.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Daniel Trusilo, David Danks
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2402.01762">Commercial AI, Conflict, and Moral Responsibility: A theoretical analysis and practical approach to the moral responsibilities associated with dual-use AI technology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents a theoretical analysis and practical approach to the moral responsibilities when developing AI systems for non-military applications that may nonetheless be used for conflict applications. We argue that AI represents a form of crossover technology that is different from previous historical examples of dual- or multi-use technology as it has a multiplicative effect across other technologies. As a result, existing analyses of ethical responsibilities around dual-use technologies do not necessarily work for AI systems. We instead argue that stakeholders involved in the AI system lifecycle are morally responsible for uses of their systems that are reasonably foreseeable. The core idea is that an agent's moral responsibility for some action is not necessarily determined by their intentions alone; we must also consider what the agent could reasonably have foreseen to be potential outcomes of their action, such as the potential use of a system in conflict even when it is not designed for that. In particular, we contend that it is reasonably foreseeable that: (1) civilian AI systems will be applied to active conflict, including conflict support activities, (2) the use of civilian AI systems in conflict will impact applications of the law of armed conflict, and (3) crossover AI technology will be applied to conflicts that fall short of armed conflict. Given these reasonably foreseeably outcomes, we present three technically feasible actions that developers of civilian AIs can take to potentially mitigate their moral responsibility: (a) establishing systematic approaches to multi-perspective capability testing, (b) integrating digital watermarking in model weight matrices, and (c) utilizing monitoring and reporting mechanisms for conflict-related AI applications.
<div id='section'>Paperid: <span id='pid'>882, <a href='https://arxiv.org/pdf/2312.02382.pdf' target='_blank'>https://arxiv.org/pdf/2312.02382.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Karanpartap Singh, James Zou
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.02382">New Evaluation Metrics Capture Quality Degradation due to LLM Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the increasing use of large-language models (LLMs) like ChatGPT, watermarking has emerged as a promising approach for tracing machine-generated content. However, research on LLM watermarking often relies on simple perplexity or diversity-based measures to assess the quality of watermarked text, which can mask important limitations in watermarking. Here we introduce two new easy-to-use methods for evaluating watermarking algorithms for LLMs: 1) evaluation by LLM-judger with specific guidelines; and 2) binary classification on text embeddings to distinguish between watermarked and unwatermarked text. We apply these methods to characterize the effectiveness of current watermarking techniques. Our experiments, conducted across various datasets, reveal that current watermarking methods are detectable by even simple classifiers, challenging the notion of watermarking subtlety. We also found, through the LLM judger, that watermarking impacts text quality, especially in degrading the coherence and depth of the response. Our findings underscore the trade-off between watermark robustness and text quality and highlight the importance of having more informative metrics to assess watermarking quality.
<div id='section'>Paperid: <span id='pid'>883, <a href='https://arxiv.org/pdf/2311.18054.pdf' target='_blank'>https://arxiv.org/pdf/2311.18054.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaan Efe KeleÅ, Ãmer Kaan GÃ¼rbÃ¼z, Mucahid Kutlu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.18054">I Know You Did Not Write That! A Sampling Based Watermarking Method for Identifying Machine Generated Text</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Potential harms of Large Language Models such as mass misinformation and plagiarism can be partially mitigated if there exists a reliable way to detect machine generated text. In this paper, we propose a new watermarking method to detect machine-generated texts. Our method embeds a unique pattern within the generated text, ensuring that while the content remains coherent and natural to human readers, it carries distinct markers that can be identified algorithmically. Specifically, we intervene with the token sampling process in a way which enables us to trace back our token choices during the detection phase. We show how watermarking affects textual quality and compare our proposed method with a state-of-the-art watermarking method in terms of robustness and detectability. Through extensive experiments, we demonstrate the effectiveness of our watermarking scheme in distinguishing between watermarked and non-watermarked text, achieving high detection rates while maintaining textual quality.
<div id='section'>Paperid: <span id='pid'>884, <a href='https://arxiv.org/pdf/2311.09816.pdf' target='_blank'>https://arxiv.org/pdf/2311.09816.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anirudh Ajith, Sameer Singh, Danish Pruthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2311.09816">Downstream Trade-offs of a Family of Text Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking involves implanting an imperceptible signal into generated text that can later be detected via statistical tests. A prominent family of watermarking strategies for LLMs embeds this signal by upsampling a (pseudorandomly-chosen) subset of tokens at every generation step. However, such signals alter the model's output distribution and can have unintended effects on its downstream performance. In this work, we evaluate the performance of LLMs watermarked using three different strategies over a diverse suite of tasks including those cast as k-class classification (CLS), multiple choice question answering (MCQ), short-form generation (e.g., open-ended question answering) and long-form generation (e.g., translation) tasks. We find that watermarks (under realistic hyperparameters) can cause significant drops in LLMs' effective utility across all tasks. We observe drops of 10 to 20% in CLS tasks in the average case, which shoot up to 100% in the worst case. We notice degradations of about 7% in MCQ tasks, 10-15% in short-form generation, and 5-15% in long-form generation tasks. Our findings highlight the trade-offs that users should be cognizant of when using watermarked models.
<div id='section'>Paperid: <span id='pid'>885, <a href='https://arxiv.org/pdf/2308.13178.pdf' target='_blank'>https://arxiv.org/pdf/2308.13178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yibo Wang, Yunhu Ye, Yuanpeng Mao, Yanwei Yu, Yuanping Song
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2308.13178">Self-supervised Scene Text Segmentation with Object-centric Layered Representations Augmented by Text Regions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Text segmentation tasks have a very wide range of application values, such as image editing, style transfer, watermark removal, etc.However, existing public datasets are of poor quality of pixel-level labels that have been shown to be notoriously costly to acquire, both in terms of money and time. At the same time, when pretraining is performed on synthetic datasets, the data distribution of the synthetic datasets is far from the data distribution in the real scene. These all pose a huge challenge to the current pixel-level text segmentation algorithms.To alleviate the above problems, we propose a self-supervised scene text segmentation algorithm with layered decoupling of representations derived from the object-centric manner to segment images into texts and background. In our method, we propose two novel designs which include Region Query Module and Representation Consistency Constraints adapting to the unique properties of text as complements to Auto Encoder, which improves the network's sensitivity to texts.For this unique design, we treat the polygon-level masks predicted by the text localization model as extra input information, and neither utilize any pixel-level mask annotations for training stage nor pretrain on synthetic datasets.Extensive experiments show the effectiveness of the method proposed. On several public scene text datasets, our method outperforms the state-of-the-art unsupervised segmentation algorithms.
<div id='section'>Paperid: <span id='pid'>886, <a href='https://arxiv.org/pdf/2305.10112.pdf' target='_blank'>https://arxiv.org/pdf/2305.10112.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Alicia MarÃ­a CenturiÃ³n-Fajardo, Alberto Lastra, Anier Soria-Lorente
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2305.10112">A dual watermaking scheme based on Sobolev type orthogonal moments for document authentication</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A dual watermarking scheme based on Sobolev type orthogonal moments, Charlier and Meixner, is proposed based on different discrete measures. The existing relation through the connection formulas allows to provide with structure and recurrence relations, together with two difference equations satisfied by such families. Weighted polynomials derived from them are being applied in an embedding and extraction watermarking algorithm, comparing the results obtained in imperceptibly and robustness tests with other families of polynomials.
<div id='section'>Paperid: <span id='pid'>887, <a href='https://arxiv.org/pdf/2304.12682.pdf' target='_blank'>https://arxiv.org/pdf/2304.12682.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aleksey Yakushev, Yury Markin, Dmitry Obydenkov, Alexander Frolov, Stas Fomin, Manuk Akopyan, Alexander Kozachok, Arthur Gaynov
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.12682">Docmarking: Real-Time Screen-Cam Robust Document Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper focuses on investigation of confidential documents leaks in the form of screen photographs. Proposed approach does not try to prevent leak in the first place but rather aims to determine source of the leak. Method works by applying on the screen a unique identifying watermark as semi-transparent image that is almost imperceptible for human eyes. Watermark image is static and stays on the screen all the time thus watermark present on every captured photograph of the screen. The key components of the approach are three neural networks. The first network generates an image with embedded message in a way that this image is almost invisible when displayed on the screen. The other two neural networks are used to retrieve embedded message with high accuracy. Developed method was comprehensively tested on different screen and cameras. Test results showed high efficiency of the proposed approach.
<div id='section'>Paperid: <span id='pid'>888, <a href='https://arxiv.org/pdf/2303.17853.pdf' target='_blank'>https://arxiv.org/pdf/2303.17853.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Samuel T. Spencer, Vikas Joshi, Alison M. W. Mitchell
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2303.17853">Can AI Put Gamma-Ray Astrophysicists Out of a Job?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In what will likely be a litany of generative-model-themed arXiv submissions celebrating April the 1st, we evaluate the capacity of state-of-the-art transformer models to create a paper detailing the detection of a Pulsar Wind Nebula with a non-existent Imaging Atmospheric Cherenkov Telescope (IACT) Array. We do this to evaluate the ability of such models to interpret astronomical observations and sources based on language information alone, and to assess potential means by which fraudulently generated scientific papers could be identified during peer review (given that reliable generative model watermarking has yet to be deployed for these tools). We conclude that our jobs as astronomers are safe for the time being. From this point on, prompts given to ChatGPT and Stable Diffusion are shown in orange, text generated by ChatGPT is shown in black, whereas analysis by the (human) authors is in blue.
<div id='section'>Paperid: <span id='pid'>889, <a href='https://arxiv.org/pdf/2302.03837.pdf' target='_blank'>https://arxiv.org/pdf/2302.03837.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Zi-yu Jiang, Chi-Man Pun, Xiao-Chen Yuan, Tong Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.03837">Robust Digital Watermarking Method Based on Adaptive Feature Area Extraction and Local Histogram Shifting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>A new local watermarking method based on histogram shifting has been proposed in this paper to deal with various signal processing attacks (e.g. median filtering, JPEG compression and Gaussian noise addition) and geometric attacks (e.g. rotation, scaling and cropping). A feature detector is used to select local areas for embedding. Then stationary wavelet transform (SWT) is applied on each local area for denoising by setting the corresponding diagonal coefficients to zero. With the implementation of histogram shifting, the watermark is embedded into denoised local areas. Meanwhile, a secret key is used in the embedding process which ensures the security that the watermark cannot be easily hacked. After the embedding process, the SWT diagonal coefficients are used to reconstruct the watermarked image. With the proposed watermarking method, we can achieve higher image quality and less bit error rate (BER) in the decoding process even after some attacks. Compared with global watermarking methods, the proposed watermarking scheme based on local histogram shifting has the advantages of higher security and larger capacity. The experimental results show the better image quality as well as lower BER compared with the state-of-art watermarking methods.
<div id='section'>Paperid: <span id='pid'>890, <a href='https://arxiv.org/pdf/2106.10147.pdf' target='_blank'>https://arxiv.org/pdf/2106.10147.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Suyoung Lee, Wonho Song, Suman Jana, Meeyoung Cha, Sooel Son
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2106.10147">Evaluating the Robustness of Trigger Set-Based Watermarks Embedded in Deep Neural Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Trigger set-based watermarking schemes have gained emerging attention as they provide a means to prove ownership for deep neural network model owners. In this paper, we argue that state-of-the-art trigger set-based watermarking algorithms do not achieve their designed goal of proving ownership. We posit that this impaired capability stems from two common experimental flaws that the existing research practice has committed when evaluating the robustness of watermarking algorithms: (1) incomplete adversarial evaluation and (2) overlooked adaptive attacks. We conduct a comprehensive adversarial evaluation of 11 representative watermarking schemes against six of the existing attacks and demonstrate that each of these watermarking schemes lacks robustness against at least two non-adaptive attacks. We also propose novel adaptive attacks that harness the adversary's knowledge of the underlying watermarking algorithm of a target model. We demonstrate that the proposed attacks effectively break all of the 11 watermarking schemes, consequently allowing adversaries to obscure the ownership of any watermarked model. We encourage follow-up studies to consider our guidelines when evaluating the robustness of their watermarking schemes via conducting comprehensive adversarial evaluation that includes our adaptive attacks to demonstrate a meaningful upper bound of watermark robustness.
<div id='section'>Paperid: <span id='pid'>891, <a href='https://arxiv.org/pdf/1104.5616.pdf' target='_blank'>https://arxiv.org/pdf/1104.5616.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peter Meerwald, Teddy Furon
</span></div><div id="title">Title: <a href="https://arxiv.org/html/1104.5616">Towards joint decoding of binary Tardos fingerprinting codes</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The class of joint decoder of probabilistic fingerprinting codes is of utmost importance in theoretical papers to establish the concept of fingerprint capacity. However, no implementation supporting a large user base is known to date. This article presents an iterative decoder which is, as far as we are aware of, the first practical attempt towards joint decoding. The discriminative feature of the scores benefits on one hand from the side-information of previously accused users, and on the other hand, from recently introduced universal linear decoders for compound channels. Neither the code construction nor the decoder make precise assumptions about the collusion (size or strategy). The extension to incorporate soft outputs from the watermarking layer is straightforward. An extensive experimental work benchmarks the very good performance and offers a clear comparison with previous state-of-the-art decoders.
<div id='section'>Paperid: <span id='pid'>892, <a href='https://arxiv.org/pdf/2512.22501.pdf' target='_blank'>https://arxiv.org/pdf/2512.22501.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Edwin Vargas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.22501">NOWA: Null-space Optical Watermark for Invisible Capture Fingerprinting and Tamper Localization</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring the authenticity and ownership of digital images is increasingly challenging as modern editing tools enable highly realistic forgeries. Existing image protection systems mainly rely on digital watermarking, which is susceptible to sophisticated digital attacks. To address this limitation, we propose a hybrid optical-digital framework that incorporates physical authentication cues during image formation and preserves them through a learned reconstruction process. At the optical level, a phase mask in the camera aperture produces a Null-space Optical Watermark (NOWA) that lies in the Null Space of the imaging operator and therefore remains invisible in the captured image. Then, a Null-Space Network (NSN) performs measurement-consistent reconstruction that delivers high-quality protected images while preserving the NOWA signature. The proposed design enables tamper localization by projecting the image onto the camera's null space and detecting pixel-level inconsistencies. Our design preserves perceptual quality, resists common degradations such as compression, and establishes a structural security asymmetry: without access to the optical or NSN parameters, adversaries cannot forge the NOWA signature. Experiments with simulations and a prototype camera demonstrate competitive performance in terms of image quality preservation, and tamper localization accuracy compared to state-of-the-art digital watermarking and learning-based authentication methods.
<div id='section'>Paperid: <span id='pid'>893, <a href='https://arxiv.org/pdf/2512.17251.pdf' target='_blank'>https://arxiv.org/pdf/2512.17251.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Madhava Gaikwad
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.17251">AlignDP: Hybrid Differential Privacy with Rarity-Aware Protection for LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models are exposed to risks of extraction, distillation, and unauthorized fine-tuning. Existing defenses use watermarking or monitoring, but these act after leakage. We design AlignDP, a hybrid privacy lock that blocks knowledge transfer at the data interface. The key idea is to separate rare and non-rare fields. Rare fields are shielded by PAC indistinguishability, giving effective zero-epsilon local DP. Non-rare fields are privatized with RAPPOR, giving unbiased frequency estimates under local DP. A global aggregator enforces composition and budget. This two-tier design hides rare events and adds controlled noise to frequent events. We prove limits of PAC extension to global aggregation, give bounds for RAPPOR estimates, and analyze utility trade-off. A toy simulation confirms feasibility: rare categories remain hidden, frequent categories are recovered with small error.
<div id='section'>Paperid: <span id='pid'>894, <a href='https://arxiv.org/pdf/2512.13325.pdf' target='_blank'>https://arxiv.org/pdf/2512.13325.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Malte Hellmeier
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.13325">Security and Detectability Analysis of Unicode Text Watermarking Methods Against Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Securing digital text is becoming increasingly relevant due to the widespread use of large language models. Individuals' fear of losing control over data when it is being used to train such machine learning models or when distinguishing model-generated output from text written by humans. Digital watermarking provides additional protection by embedding an invisible watermark within the data that requires protection. However, little work has been taken to analyze and verify if existing digital text watermarking methods are secure and undetectable by large language models. In this paper, we investigate the security-related area of watermarking and machine learning models for text data. In a controlled testbed of three experiments, ten existing Unicode text watermarking methods were implemented and analyzed across six large language models: GPT-5, GPT-4o, Teuken 7B, Llama 3.3, Claude Sonnet 4, and Gemini 2.5 Pro. The findings of our experiments indicate that, especially the latest reasoning models, can detect a watermarked text. Nevertheless, all models fail to extract the watermark unless implementation details in the form of source code are provided. We discuss the implications for security researchers and practitioners and outline future research opportunities to address security concerns.
<div id='section'>Paperid: <span id='pid'>895, <a href='https://arxiv.org/pdf/2512.06922.pdf' target='_blank'>https://arxiv.org/pdf/2512.06922.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>George Mikros
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.06922">Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.
<div id='section'>Paperid: <span id='pid'>896, <a href='https://arxiv.org/pdf/2512.03079.pdf' target='_blank'>https://arxiv.org/pdf/2512.03079.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Anudeex Shetty
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2512.03079">Watermarks for Embeddings-as-a-Service Large Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques. Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques. Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.
<div id='section'>Paperid: <span id='pid'>897, <a href='https://arxiv.org/pdf/2511.23026.pdf' target='_blank'>https://arxiv.org/pdf/2511.23026.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kassem Kallas
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.23026">A Game-Theoretic Approach for Adversarial Information Fusion in Distributed Sensor Networks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Every day we share our personal information through digital systems which are constantly exposed to threats. For this reason, security-oriented disciplines of signal processing have received increasing attention in the last decades: multimedia forensics, digital watermarking, biometrics, network monitoring, steganography and steganalysis are just a few examples. Even though each of these fields has its own peculiarities, they all have to deal with a common problem: the presence of one or more adversaries aiming at making the system fail. Adversarial Signal Processing lays the basis of a general theory that takes into account the impact that the presence of an adversary has on the design of effective signal processing tools. By focusing on the application side of Adversarial Signal Processing, namely adversarial information fusion in distributed sensor networks, and adopting a game-theoretic approach, this thesis contributes to the above mission by addressing four issues. First, we address decision fusion in distributed sensor networks by developing a novel soft isolation defense scheme that protect the network from adversaries, specifically, Byzantines. Second, we develop an optimum decision fusion strategy in the presence of Byzantines. In the next step, we propose a technique to reduce the complexity of the optimum fusion by relying on a novel near-optimum message passing algorithm based on factor graphs. Finally, we introduce a defense mechanism to protect decentralized networks running consensus algorithm against data falsification attacks.
<div id='section'>Paperid: <span id='pid'>898, <a href='https://arxiv.org/pdf/2511.10245.pdf' target='_blank'>https://arxiv.org/pdf/2511.10245.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rizal Khoirul Anam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.10245">Robustness and Imperceptibility Analysis of Hybrid Spatial-Frequency Domain Image Watermarking</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The proliferation of digital media necessitates robust methods for copyright protection and content authentication. This paper presents a comprehensive comparative study of digital image watermarking techniques implemented using the spatial domain (Least Significant Bit - LSB), the frequency domain (Discrete Fourier Transform - DFT), and a novel hybrid (LSB+DFT) approach. The core objective is to evaluate the trade-offs between imperceptibility (measured by Peak Signal-to-Noise Ratio - PSNR) and robustness (measured by Normalized Correlation - NC and Bit Error Rate - BER). We implemented these three techniques within a unified MATLAB-based experimental framework. The watermarked images were subjected to a battery of common image processing attacks, including JPEG compression, Gaussian noise, and salt-and-pepper noise, at varying intensities. Experimental results generated from standard image datasets (USC-SIPI) demonstrate that while LSB provides superior imperceptibility, it is extremely fragile. The DFT method offers significant robustness at the cost of visual quality. The proposed hybrid LSB+DFT technique, which leverages redundant embedding and a fallback extraction mechanism, is shown to provide the optimal balance, maintaining high visual fidelity while exhibiting superior resilience to all tested attacks.
<div id='section'>Paperid: <span id='pid'>899, <a href='https://arxiv.org/pdf/2511.03641.pdf' target='_blank'>https://arxiv.org/pdf/2511.03641.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Thomas Souverain
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2511.03641">Watermarking Large Language Models in Europe: Interpreting the AI Act in Light of Technology</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>To foster trustworthy Artificial Intelligence (AI) within the European Union, the AI Act requires providers to mark and detect the outputs of their general-purpose models. The Article 50 and Recital 133 call for marking methods that are ''sufficiently reliable, interoperable, effective and robust''. Yet, the rapidly evolving and heterogeneous landscape of watermarks for Large Language Models (LLMs) makes it difficult to determine how these four standards can be translated into concrete and measurable evaluations. Our paper addresses this challenge, anchoring the normativity of European requirements in the multiplicity of watermarking techniques. Introducing clear and distinct concepts on LLM watermarking, our contribution is threefold. (1) Watermarking Categorisation: We propose an accessible taxonomy of watermarking methods according to the stage of the LLM lifecycle at which they are applied - before, during, or after training, and during next-token distribution or sampling. (2) Watermarking Evaluation: We interpret the EU AI Act's requirements by mapping each criterion with state-of-the-art evaluations on robustness and detectability of the watermark, and of quality of the LLM. Since interoperability remains largely untheorised in LLM watermarking research, we propose three normative dimensions to frame its assessment. (3) Watermarking Comparison: We compare current watermarking methods for LLMs against the operationalised European criteria and show that no approach yet satisfies all four standards. Encouraged by emerging empirical tests, we recommend further research into watermarking directly embedded within the low-level architecture of LLMs.
<div id='section'>Paperid: <span id='pid'>900, <a href='https://arxiv.org/pdf/2510.24789.pdf' target='_blank'>https://arxiv.org/pdf/2510.24789.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Gokul Ganesan
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.24789">Cross-Lingual Summarization as a Black-Box Watermark Removal Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking has been proposed as a lightweight mechanism to identify AI-generated text, with schemes typically relying on perturbations to token distributions. While prior work shows that paraphrasing can weaken such signals, these attacks remain partially detectable or degrade text quality. We demonstrate that cross-lingual summarization attacks (CLSA) -- translation to a pivot language followed by summarization and optional back-translation -- constitute a qualitatively stronger attack vector. By forcing a semantic bottleneck across languages, CLSA systematically destroys token-level statistical biases while preserving semantic fidelity. In experiments across multiple watermarking schemes (KGW, SIR, XSIR, Unigram) and five languages (Amharic, Chinese, Hindi, Spanish, Swahili), we show that CLSA reduces watermark detection accuracy more effectively than monolingual paraphrase at similar quality levels. Our results highlight an underexplored vulnerability that challenges the practicality of watermarking for provenance or regulation. We argue that robust provenance solutions must move beyond distributional watermarking and incorporate cryptographic or model-attestation approaches. On 300 held-out samples per language, CLSA consistently drives detection toward chance while preserving task utility. Concretely, for XSIR (explicitly designed for cross-lingual robustness), AUROC with paraphrasing is $0.827$, with Cross-Lingual Watermark Removal Attacks (CWRA) [He et al., 2024] using Chinese as the pivot, it is $0.823$, whereas CLSA drives it down to $0.53$ (near chance). Results highlight a practical, low-cost removal pathway that crosses languages and compresses content without visible artifacts.
<div id='section'>Paperid: <span id='pid'>901, <a href='https://arxiv.org/pdf/2510.04303.pdf' target='_blank'>https://arxiv.org/pdf/2510.04303.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Om Tailor
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.04303">Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Multi-agent deployments of large language models (LLMs) are increasingly embedded in market, allocation, and governance workflows, yet covert coordination among agents can silently erode trust and social welfare. Existing audits are dominated by heuristics that lack theoretical guarantees, struggle to transfer across tasks, and seldom ship with the infrastructure needed for independent replication. We introduce \emph{Audit the Whisper}, a conference-grade research artifact that spans theory, benchmark design, detection, and reproducibility. Our contributions are: (i) a channel-capacity analysis showing how interventions such as paraphrase, rate limiting, and role permutation impose quantifiable capacity penalties -- operationalized via paired-run Kullback--Leibler diagnostics -- that tighten mutual-information thresholds with finite-sample guarantees; (ii) \textsc{ColludeBench}-v0, covering pricing, first-price auctions, and peer review with configurable covert schemes, deterministic manifests, and reward instrumentation; and (iii) a calibrated auditing pipeline that fuses cross-run mutual information, permutation invariance, watermark variance, and fairness-aware acceptance bias, each tuned to a \(10^{-3}\) false-positive budget. Across 600 audited runs spanning 12 intervention conditions, the union meta-test attains TPR~$=1$ with zero observed false alarms, while ablations surface the price-of-auditing trade-off and highlight fairness-driven colluders invisible to MI alone. We release regeneration scripts, seed-stamped manifests, and documentation so that external auditors can reproduce every figure and extend the framework with minimal effort.
<div id='section'>Paperid: <span id='pid'>902, <a href='https://arxiv.org/pdf/2510.03770.pdf' target='_blank'>https://arxiv.org/pdf/2510.03770.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>David Megias
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2510.03770">Complex Domain Approach for Reversible Data Hiding and Homomorphic Encryption: General Framework and Application to Dispersed Data</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Ensuring the trustworthiness of data from distributed and resource-constrained environments, such as Wireless Sensor Networks or IoT devices, is critical. Existing Reversible Data Hiding (RDH) methods for scalar data suffer from low embedding capacity and poor intrinsic mixing between host data and watermark. This paper introduces Hiding in the Imaginary Domain with Data Encryption (H[i]dden), a novel framework based on complex number arithmetic for simultaneous information embedding and encryption. The H[i]dden framework offers perfect reversibility, in-principle unlimited watermark size, and intrinsic data-watermark mixing. The paper further introduces two protocols: H[i]dden-EG, for joint reversible data hiding and encryption, and H[i]dden-AggP, for privacy-preserving aggregation of watermarked data, based on partially homomorphic encryption. These protocols provide efficient and resilient solutions for data integrity, provenance and confidentiality, serving as a foundation for new schemes based on the algebraic properties of the complex domain.
<div id='section'>Paperid: <span id='pid'>903, <a href='https://arxiv.org/pdf/2509.21601.pdf' target='_blank'>https://arxiv.org/pdf/2509.21601.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Jason Anderson
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.21601">World's First Authenticated Satellite Pseudorange from Orbit</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Cryptographic Ranging Authentication is here! We present initial results on the Pulsar authenticated ranging service broadcast from space with Pulsar-0 utilizing a recording taken at Xona headquarters in Burlingame, CA. No assumptions pertaining to the ownership or leakage of encryption keys are required. This work discusses the Pulsar watermark design and security analysis. We derive the Pulsar watermark's probabilities of missed detection and false alarm, and we discuss the required receiver processing needed to utilize the Pulsar watermark. We present validation results of the Pulsar watermark utilizing the transmissions from orbit. Lastly, we provide results that demonstrate the spoofing detection efficacy with a spoofing scenario that incorporates the authentic transmissions from orbit. Because we make no assumption about the leakage of symmetric encryption keys, this work provides mathematical justification of the watermark's security, and our July 2025 transmissions from orbit, we claim the world's first authenticated satellite pseudorange from orbit.
<div id='section'>Paperid: <span id='pid'>904, <a href='https://arxiv.org/pdf/2509.11915.pdf' target='_blank'>https://arxiv.org/pdf/2509.11915.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Aadil Gani Ganie
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2509.11915">Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As large language models (LLMs) become more advanced, it is increasingly difficult to distinguish between human-written and AI-generated text. This paper draws a conceptual parallel between quantum uncertainty and the limits of authorship detection in natural language. We argue that there is a fundamental trade-off: the more confidently one tries to identify whether a text was written by a human or an AI, the more one risks disrupting the text's natural flow and authenticity. This mirrors the tension between precision and disturbance found in quantum systems. We explore how current detection methods--such as stylometry, watermarking, and neural classifiers--face inherent limitations. Enhancing detection accuracy often leads to changes in the AI's output, making other features less reliable. In effect, the very act of trying to detect AI authorship introduces uncertainty elsewhere in the text. Our analysis shows that when AI-generated text closely mimics human writing, perfect detection becomes not just technologically difficult but theoretically impossible. We address counterarguments and discuss the broader implications for authorship, ethics, and policy. Ultimately, we suggest that the challenge of AI-text detection is not just a matter of better tools--it reflects a deeper, unavoidable tension in the nature of language itself.
<div id='section'>Paperid: <span id='pid'>905, <a href='https://arxiv.org/pdf/2507.21097.pdf' target='_blank'>https://arxiv.org/pdf/2507.21097.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Abraham Itzhak Weinberg
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2507.21097">Singularity Cipher: A Topology-Driven Cryptographic Scheme Based on Visual Paradox and Klein Bottle Illusions</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper presents the Singularity Cipher, a novel cryptographic-steganographic framework that integrates topological transformations and visual paradoxes to achieve multidimensional security. Inspired by the non-orientable properties of the Klein bottle -- constructed from two Mobius strips -- the cipher applies symbolic twist functions to simulate topological traversal, producing high confusion and diffusion in the ciphertext. The resulting binary data is then encoded using perceptual illusions, such as the missing square paradox, to visually obscure the presence of encrypted content. Unlike conventional ciphers that rely solely on algebraic complexity, the Singularity Cipher introduces a dual-layer approach: symbolic encryption rooted in topology and visual steganography designed for human cognitive ambiguity. This combination enhances both cryptographic strength and detection resistance, making it well-suited for secure communication, watermarking, and plausible deniability in adversarial environments. The paper formalizes the architecture, provides encryption and decryption algorithms, evaluates security properties, and compares the method against classical, post-quantum, and steganographic approaches. Potential applications and future research directions are also discussed.
<div id='section'>Paperid: <span id='pid'>906, <a href='https://arxiv.org/pdf/2506.12032.pdf' target='_blank'>https://arxiv.org/pdf/2506.12032.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krti Tallam
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2506.12032">Embedding Trust at Scale: Physics-Aware Neural Watermarking for Secure and Verifiable Data Pipelines</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We present a robust neural watermarking framework for scientific data integrity, targeting high-dimensional fields common in climate modeling and fluid simulations. Using a convolutional autoencoder, binary messages are invisibly embedded into structured data such as temperature, vorticity, and geopotential. Our method ensures watermark persistence under lossy transformations - including noise injection, cropping, and compression - while maintaining near-original fidelity (sub-1\% MSE). Compared to classical singular value decomposition (SVD)-based watermarking, our approach achieves $>$98\% bit accuracy and visually indistinguishable reconstructions across ERA5 and Navier-Stokes datasets. This system offers a scalable, model-compatible tool for data provenance, auditability, and traceability in high-performance scientific workflows, and contributes to the broader goal of securing AI systems through verifiable, physics-aware watermarking. We evaluate on physically grounded scientific datasets as a representative stress-test; the framework extends naturally to other structured domains such as satellite imagery and autonomous-vehicle perception streams.
<div id='section'>Paperid: <span id='pid'>907, <a href='https://arxiv.org/pdf/2505.19145.pdf' target='_blank'>https://arxiv.org/pdf/2505.19145.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Weijie Su
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2505.19145">Do Large Language Models (Really) Need Statistical Foundations?</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Large language models (LLMs) represent a new paradigm for processing unstructured data, with applications across an unprecedented range of domains. In this paper, we address, through two arguments, whether the development and application of LLMs would genuinely benefit from foundational contributions from the statistics discipline. First, we argue affirmatively, beginning with the observation that LLMs are inherently statistical models due to their profound data dependency and stochastic generation processes, where statistical insights are naturally essential for handling variability and uncertainty. Second, we argue that the persistent black-box nature of LLMs -- stemming from their immense scale, architectural complexity, and development practices often prioritizing empirical performance over theoretical interpretability -- renders closed-form or purely mechanistic analyses generally intractable, thereby necessitating statistical approaches due to their flexibility and often demonstrated effectiveness. To substantiate these arguments, the paper outlines several research areas -- including alignment, watermarking, uncertainty quantification, evaluation, and data mixture optimization -- where statistical methodologies are critically needed and are already beginning to make valuable contributions. We conclude with a discussion suggesting that statistical research concerning LLMs will likely form a diverse ``mosaic'' of specialized topics rather than deriving from a single unifying theory, and highlighting the importance of timely engagement by our statistics community in LLM research.
<div id='section'>Paperid: <span id='pid'>908, <a href='https://arxiv.org/pdf/2504.13205.pdf' target='_blank'>https://arxiv.org/pdf/2504.13205.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Houssam Kherraz
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.13205">On-Device Watermarking: A Socio-Technical Imperative For Authenticity In The Age of Generative AI</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>As generative AI models produce increasingly realistic output, both academia and industry are focusing on the ability to detect whether an output was generated by an AI model or not. Many of the research efforts and policy discourse are centered around robust watermarking of AI outputs. While plenty of progress has been made, all watermarking and AI detection techniques face severe limitations. In this position paper, we argue that we are adopting the wrong approach, and should instead focus on watermarking via cryptographic signatures trustworthy content rather than AI generated ones. For audio-visual content, in particular, all real content is grounded in the physical world and captured via hardware sensors. This presents a unique opportunity to watermark at the hardware layer, and we lay out a socio-technical framework and draw parallels with HTTPS certification and Blu-Ray verification protocols. While acknowledging implementation challenges, we contend that hardware-based authentication offers a more tractable path forward, particularly from a policy perspective. As generative models approach perceptual indistinguishability, the research community should be wary of being overly optimistic with AI watermarking, and we argue that AI watermarking research efforts are better spent in the text and LLM space, which are ultimately not traceable to a physical sensor.
<div id='section'>Paperid: <span id='pid'>909, <a href='https://arxiv.org/pdf/2504.03765.pdf' target='_blank'>https://arxiv.org/pdf/2504.03765.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lele Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.03765">Watermarking for AI Content Detection: A Review on Text, Visual, and Audio Modalities</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The rapid advancement of generative artificial intelligence (GenAI) has revolutionized content creation across text, visual, and audio domains, simultaneously introducing significant risks such as misinformation, identity fraud, and content manipulation. This paper presents a practical survey of watermarking techniques designed to proactively detect GenAI content. We develop a structured taxonomy categorizing watermarking methods for text, visual, and audio modalities and critically evaluate existing approaches based on their effectiveness, robustness, and practicality. Additionally, we identify key challenges, including resistance to adversarial attacks, lack of standardization across different content types, and ethical considerations related to privacy and content ownership. Finally, we discuss potential future research directions aimed at enhancing watermarking strategies to ensure content authenticity and trustworthiness. This survey serves as a foundational resource for researchers and practitioners seeking to understand and advance watermarking techniques for AI-generated content detection.
<div id='section'>Paperid: <span id='pid'>910, <a href='https://arxiv.org/pdf/2504.02898.pdf' target='_blank'>https://arxiv.org/pdf/2504.02898.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Lele Cao
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2504.02898">A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media.
<div id='section'>Paperid: <span id='pid'>911, <a href='https://arxiv.org/pdf/2501.08604.pdf' target='_blank'>https://arxiv.org/pdf/2501.08604.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Krishna Panthi
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2501.08604">Watermarking in Diffusion Model: Gaussian Shading with Exact Diffusion Inversion via Coupled Transformations (EDICT)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>This paper introduces a novel approach to enhance the performance of Gaussian Shading, a prevalent watermarking technique, by integrating the Exact Diffusion Inversion via Coupled Transformations (EDICT) framework. While Gaussian Shading traditionally embeds watermarks in a noise latent space, followed by iterative denoising for image generation and noise addition for watermark recovery, its inversion process is not exact, leading to potential watermark distortion. We propose to leverage EDICT's ability to derive exact inverse mappings to refine this process. Our method involves duplicating the watermark-infused noisy latent and employing a reciprocal, alternating denoising and noising scheme between the two latents, facilitated by EDICT. This allows for a more precise reconstruction of both the image and the embedded watermark. Empirical evaluation on standard datasets demonstrates that our integrated approach yields a slight, yet statistically significant improvement in watermark recovery fidelity. These results highlight the potential of EDICT to enhance existing diffusion-based watermarking techniques by providing a more accurate and robust inversion mechanism. To the best of our knowledge, this is the first work to explore the synergy between EDICT and Gaussian Shading for digital watermarking, opening new avenues for research in robust and high-fidelity watermark embedding and extraction.
<div id='section'>Paperid: <span id='pid'>912, <a href='https://arxiv.org/pdf/2412.19603.pdf' target='_blank'>https://arxiv.org/pdf/2412.19603.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Minhao Bai
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2412.19603">Let Watermarks Speak: A Robust and Unforgeable Watermark for Language Models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking is an effective way to trace model-generated content. Current watermark methods cannot resist forgery attacks, such as a deceptive claim that the model-generated content is a response to a fabricated prompt. None of them can be made unforgeable without degrading robustness.
  Unforgeability demands that the watermarked output is not only detectable but also verifiable for integrity, indicating whether it has been modified. This underscores the necessity and significance of a multi-bit watermarking scheme.
  Recent works try to build multi-bit scheme based on existing zero-bit watermarking scheme, but they either degrades the robustness or brings a significant computational burden. We aim to design a novel single-bit watermark scheme, which provides the ability to embed 2 different watermark signals.
  This paper's main contribution is that we are the first to propose an undetectable, robust, single-bit watermarking scheme. It has a comparable robustness to the most advanced zero-bit watermarking schemes. Then we construct a multi-bit watermarking scheme to use the hash value of prompt or the newest generated content as the watermark signals, and embed them into the following content, which guarantees the unforgeability.
  Additionally, we provide sufficient experiments on some popular language models, while the other advanced methods with provable guarantees do not often provide. The results show that our method is practically effective and robust.
<div id='section'>Paperid: <span id='pid'>913, <a href='https://arxiv.org/pdf/2411.03847.pdf' target='_blank'>https://arxiv.org/pdf/2411.03847.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Peihao Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2411.03847">A Novel Access Control and Privacy-Enhancing Approach for Models in Edge Computing</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the widespread adoption of edge computing technologies and the increasing prevalence of deep learning models in these environments, the security risks and privacy threats to models and data have grown more acute. Attackers can exploit various techniques to illegally obtain models or misuse data, leading to serious issues such as intellectual property infringement and privacy breaches. Existing model access control technologies primarily rely on traditional encryption and authentication methods; however, these approaches exhibit significant limitations in terms of flexibility and adaptability in dynamic environments. Although there have been advancements in model watermarking techniques for marking model ownership, they remain limited in their ability to proactively protect intellectual property and prevent unauthorized access. To address these challenges, we propose a novel model access control method tailored for edge computing environments. This method leverages image style as a licensing mechanism, embedding style recognition into the model's operational framework to enable intrinsic access control. Consequently, models deployed on edge platforms are designed to correctly infer only on license data with specific style, rendering them ineffective on any other data. By restricting the input data to the edge model, this approach not only prevents attackers from gaining unauthorized access to the model but also enhances the privacy of data on terminal devices. We conducted extensive experiments on benchmark datasets, including MNIST, CIFAR-10, and FACESCRUB, and the results demonstrate that our method effectively prevents unauthorized access to the model while maintaining accuracy. Additionally, the model shows strong resistance against attacks such as forged licenses and fine-tuning. These results underscore the method's usability, security, and robustness.
<div id='section'>Paperid: <span id='pid'>914, <a href='https://arxiv.org/pdf/2410.20519.pdf' target='_blank'>https://arxiv.org/pdf/2410.20519.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Yiquan Wang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.20519">Fractal Signatures: Securing AI-Generated Pollock-Style Art via Intrinsic Watermarking and Blockchain</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>The digital art market faces unprecedented challenges in authenticity verification and copyright protection. This study introduces an integrated framework to address these issues by combining neural style transfer, fractal analysis, and blockchain technology. We generate abstract artworks inspired by Jackson Pollock, using their inherent mathematical complexity to create robust, imperceptible watermarks. Our method embeds these watermarks, derived from fractal and turbulence features, directly into the artwork's structure. This approach is then secured by linking the watermark to NFT metadata, ensuring immutable proof of ownership. Rigorous testing shows our feature-based watermarking achieves a 76.2% average detection rate against common attacks, significantly outperforming traditional methods (27.8-44.0%). This work offers a practical solution for digital artists and collectors, enhancing security and trust in the digital art ecosystem.
<div id='section'>Paperid: <span id='pid'>915, <a href='https://arxiv.org/pdf/2410.10178.pdf' target='_blank'>https://arxiv.org/pdf/2410.10178.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Renyi Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.10178">GUISE: Graph GaUssIan Shading watErmark</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the expanding field of generative artificial intelligence, integrating robust watermarking technologies is essential to protect intellectual property and maintain content authenticity. Traditionally, watermarking techniques have been developed primarily for rich information media such as images and audio. However, these methods have not been adequately adapted for graph-based data, particularly molecular graphs. Latent 3D graph diffusion(LDM-3DG) is an ascendant approach in the molecular graph generation field. This model effectively manages the complexities of molecular structures, preserving essential symmetries and topological features. We adapt the Gaussian Shading, a proven performance lossless watermarking technique, to the latent graph diffusion domain to protect this sophisticated new technology. Our adaptation simplifies the watermark diffusion process through duplication and padding, making it adaptable and suitable for various message types. We conduct several experiments using the LDM-3DG model on publicly available datasets QM9 and Drugs, to assess the robustness and effectiveness of our technique. Our results demonstrate that the watermarked molecules maintain statistical parity in 9 out of 10 performance metrics compared to the original. Moreover, they exhibit a 100% detection rate and a 99% extraction rate in a 2D decoded pipeline, while also showing robustness against post-editing attacks.
<div id='section'>Paperid: <span id='pid'>916, <a href='https://arxiv.org/pdf/2410.09071.pdf' target='_blank'>https://arxiv.org/pdf/2410.09071.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Givon Zirkind
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2410.09071">Using Steganography and Watermarking For Medical Image Integrity</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Medical imaging has kept up with the digital age. Medical images such as x-rays are no longer keep on film or; even made with film. Rather, they are digital. In addition, they are transmitted for reasons of consultation and telehealth as well as archived. Transmission and retrieval of these images presents an integrity issue, with a high level of integrity being needed. Very small artifacts in a digital medical image can have significant importance, making or changing a diagnosis. It is imperative that the integrity of a medical image, especially in a Region of Interest be identifiable and preserved. Watermarking and steganography are used for the purposes of authenticating images, especially for copyright purposes. These techniques can be applied to medical images. However, these techniques can interfere with the integrity of the picture. While such distortion may be acceptable in other domains, in the medical domain this distortion is not acceptable. High accuracy is imperative for diagnosis. This paper discusses the techniques used, their advantages and shortcomings as well as methods of overcoming obstacles to integrity.
<div id='section'>Paperid: <span id='pid'>917, <a href='https://arxiv.org/pdf/2409.19506.pdf' target='_blank'>https://arxiv.org/pdf/2409.19506.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Kaixin Deng
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2409.19506">IWN: Image Watermarking Based on Idempotency</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In the expanding field of digital media, maintaining the strength and integrity of watermarking technology is becoming increasingly challenging. This paper, inspired by the Idempotent Generative Network (IGN), explores the prospects of introducing idempotency into image watermark processing and proposes an innovative neural network model - the Idempotent Watermarking Network (IWN). The proposed model, which focuses on enhancing the recovery quality of color image watermarks, leverages idempotency to ensure superior image reversibility. This feature ensures that, even if color image watermarks are attacked or damaged, they can be effectively projected and mapped back to their original state. Therefore, the extracted watermarks have unquestionably increased quality. The IWN model achieves a balance between embedding capacity and robustness, alleviating to some extent the inherent contradiction between these two factors in traditional watermarking techniques and steganography methods.
<div id='section'>Paperid: <span id='pid'>918, <a href='https://arxiv.org/pdf/2408.13277.pdf' target='_blank'>https://arxiv.org/pdf/2408.13277.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Guang Yang
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2408.13277">An Improved Phase Coding Audio Steganography Algorithm</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Advances in AI technology have made voice cloning increasingly accessible, leading to a rise in fraud involving AI-generated audio forgeries. This highlights the need to covertly embed information and verify the authenticity and integrity of audio. Digital Audio Watermarking plays a crucial role in this context. This study presents an improved Phase Coding audio steganography algorithm that segments the audio signal dynamically, embedding data into the mid-frequency phase components. This approach enhances resistance to steganalysis, simplifies computation, and ensures secure audio integrity.
<div id='section'>Paperid: <span id='pid'>919, <a href='https://arxiv.org/pdf/2403.06593.pdf' target='_blank'>https://arxiv.org/pdf/2403.06593.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Tim RÃ¤z
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2403.06593">Authorship and the Politics and Ethics of LLM Watermarks</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Recently, watermarking schemes for large language models (LLMs) have been proposed to distinguish text generated by machines and by humans. The present paper explores philosophical, political, and ethical ramifications of implementing and using watermarking schemes. A definition of authorship that includes both machines (LLMs) and humans is proposed to serve as a backdrop. It is argued that private watermarks may provide private companies with sweeping rights to determine authorship, which is incompatible with traditional standards of authorship determination. Then, possible ramifications of the so-called entropy dependence of watermarking mechanisms are explored. It is argued that entropy may vary for different, socially salient groups. This could lead to group dependent rates at which machine generated text is detected. Specifically, groups more interested in low entropy text may face the challenge that it is harder to detect machine generated text that is of interest to them.
<div id='section'>Paperid: <span id='pid'>920, <a href='https://arxiv.org/pdf/2401.10360.pdf' target='_blank'>https://arxiv.org/pdf/2401.10360.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Or Zamir
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2401.10360">Excuse me, sir? Your language model is leaking (information)</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>We introduce a cryptographic method to hide an arbitrary secret payload in the response of a Large Language Model (LLM). A secret key is required to extract the payload from the model's response, and without the key it is provably impossible to distinguish between the responses of the original LLM and the LLM that hides a payload. In particular, the quality of generated text is not affected by the payload. Our approach extends a recent result of Christ, Gunn and Zamir (2023) who introduced an undetectable watermarking scheme for LLMs.
<div id='section'>Paperid: <span id='pid'>921, <a href='https://arxiv.org/pdf/2312.17295.pdf' target='_blank'>https://arxiv.org/pdf/2312.17295.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Bram Wouters
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2312.17295">Optimizing watermarks for large language models</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rise of large language models (LLMs) and concerns about potential misuse, watermarks for generative LLMs have recently attracted much attention. An important aspect of such watermarks is the trade-off between their identifiability and their impact on the quality of the generated text. This paper introduces a systematic approach to this trade-off in terms of a multi-objective optimization problem. For a large class of robust, efficient watermarks, the associated Pareto optimal solutions are identified and shown to outperform the currently default watermark.
<div id='section'>Paperid: <span id='pid'>922, <a href='https://arxiv.org/pdf/2306.12790.pdf' target='_blank'>https://arxiv.org/pdf/2306.12790.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Xinyu Li
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2306.12790">DiffWA: Diffusion Models for Watermark Attack</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>With the rapid development of deep neural networks(DNNs), many robust blind watermarking algorithms and frameworks have been proposed and achieved good results. At present, the watermark attack algorithm can not compete with the watermark addition algorithm. And many watermark attack algorithms only care about interfering with the normal extraction of the watermark, and the watermark attack will cause great visual loss to the image. To this end, we propose DiffWA, a conditional diffusion model with distance guidance for watermark attack, which can restore the image while removing the embedded watermark. The core of our method is training an image-to-image conditional diffusion model on unwatermarked images and guiding the conditional model using a distance guidance when sampling so that the model will generate unwatermarked images which is similar to original images. We conducted experiments on CIFAR-10 using our proposed models. The results shows that the model can remove the watermark with good effect and make the bit error rate of watermark extraction higher than 0.4. At the same time, the attacked image will maintain good visual effect with PSNR more than 31 and SSIM more than 0.97 compared with the original image.
<div id='section'>Paperid: <span id='pid'>923, <a href='https://arxiv.org/pdf/2304.13953.pdf' target='_blank'>https://arxiv.org/pdf/2304.13953.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Ming Liu
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2304.13953">Automatic Localization and Detection Applicable to Robust Image Watermarking Resisting against Camera Shooting</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Robust image watermarking that can resist camera shooting has become an active research topic in recent years due to the increasing demand for preventing sensitive information displayed on computer screens from being captured. However, many mainstream schemes require human assistance during the watermark detection process and cannot adapt to scenarios that require processing a large number of images. Although deep learning-based schemes enable end-to-end watermark embedding and detection, their limited generalization ability makes them vulnerable to failure in complex scenarios. In this paper, we propose a carefully crafted watermarking system that can resist camera shooting. The proposed scheme deals with two important problems: automatic watermark localization (AWL) and automatic watermark detection (AWD). AWL automatically identifies the region of interest (RoI), which contains watermark information, in the camera-shooting image by analyzing the local statistical characteristics. Meanwhile, AWD extracts the hidden watermark from the identified RoI after applying perspective correction. Compared with previous works, the proposed scheme is fully automatic, making it ideal for application scenarios. Furthermore, the proposed scheme is not limited to any specific watermark embedding strategy, allowing for improvements in the watermark embedding and extraction procedure. Extensive experimental results and analysis show that the embedded watermark can be automatically and reliably extracted from the camera-shooting image in different scenarios, demonstrating the superiority and applicability of the proposed approach.
<div id='section'>Paperid: <span id='pid'>924, <a href='https://arxiv.org/pdf/2302.11338.pdf' target='_blank'>https://arxiv.org/pdf/2302.11338.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Rongfeng Wei
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.11338">Visual Watermark Removal Based on Deep Learning</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In recent years as the internet age continues to grow, sharing images on social media has become a common occurrence. In certain cases, watermarks are used as protection for the ownership of the image, however, in more cases, one may wish to remove these watermark images to get the original image without obscuring. In this work, we proposed a deep learning method based technique for visual watermark removal. Inspired by the strong image translation performance of the U-structure, an end-to-end deep neural network model named AdvancedUnet is proposed to extract and remove the visual watermark simultaneously. On the other hand, we embed some effective RSU module instead of the common residual block used in UNet, which increases the depth of the whole architecture without significantly increasing the computational cost. The deep-supervised hybrid loss guides the network to learn the transformation between the input image and the ground truth in a multi-scale and three-level hierarchy. Comparison experiments demonstrate the effectiveness of our method.
<div id='section'>Paperid: <span id='pid'>925, <a href='https://arxiv.org/pdf/2302.01336.pdf' target='_blank'>https://arxiv.org/pdf/2302.01336.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Mesfer Mohammed Alqarni
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2302.01336">A Framework to Allow a Third Party to Watermark Numerical Data in an Encrypted Domain while Preserving its Statistical Properties</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>Watermarking data for source tracking applications by its owner can be unfair for recipients because the data owner may redistribute the same watermarked data to many users. Hence, each data recipient should know the watermark embedded in their data; however, this may enable them to remove it, which violates the watermark security. To overcome this problem, this research develops a framework that allows the cloud to watermark numerical data taking into consideration: the correctness of the results of selected statistics, data privacy, the recipient's right to know the watermark that is embedded in their data, and the security of the watermark against passive attacks. The proposed framework contains two irreversible watermarking algorithms, each can preserve the correctness of the results for certain statistical operations. To preserve data privacy, the framework allows the cloud to watermark data while it is encrypted. Furthermore, the framework robustifies the security of the chosen algorithms to nominate the cloud as the only neutral judge able to verify the data ownership even if other users know the watermark. The security is enhanced in a way that does not affect the data usability. The time complexity to find the watermark is $\mathcal{O}(\frac{n!}{r!(n-r)!})$.
<div id='section'>Paperid: <span id='pid'>926, <a href='https://arxiv.org/pdf/2009.00951.pdf' target='_blank'>https://arxiv.org/pdf/2009.00951.pdf</a></span>  </div></span></div><div id = 'author'>Authors:<span id = 'author'>Sam Blake
</span></div><div id="title">Title: <a href="https://arxiv.org/html/2009.00951">Embedded Blockchains: A Synthesis of Blockchains, Spread Spectrum Watermarking, Perceptual Hashing & Digital Signatures</a></div></span></div><div id = 'abs'>Abstract:</br> <span id = 'abs'>In this paper we introduce a scheme for detecting manipulated audio and video. The scheme is a synthesis of blockchains, encrypted spread spectrum watermarks, perceptual hashing and digital signatures, which we call an Embedded Blockchain. Within this scheme, we use the blockchain for its data structure of a cryptographically linked list, cryptographic hashing for absolute comparisons, perceptual hashing for flexible comparisons, digital signatures for proof of ownership, and encrypted spread spectrum watermarking to embed the blockchain into the background noise of the media. So each media recording has its own unique blockchain, with each block holding information describing the media segment. The problem of verifying the integrity of the media is recast to traversing the blockchain, block-by-block, and segment-by-segment of the media. If any chain is broken, the difference in the computed and extracted perceptual hash is used to estimate the level of manipulation.
